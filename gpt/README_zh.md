[ğŸ“– English ReadMe](./README.md)  
## Introduction  
åœ¨è¿™ä¸ª GPT çš„å®ç°ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå¦‚ä½•åœ¨ [BookCorpus](https://huggingface.co/datasets/bookcorpus/bookcorpus) æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åä» [huggingface](https://huggingface.co/openai-community/openai-gpt) ä¸ŠåŠ è½½å®˜æ–¹çš„é¢„è®­ç»ƒæƒé‡åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­ï¼Œåœ¨ [Stanford Sentiment Treebank (SST-2)](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf) æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶å¤ç°è®ºæ–‡ä¸­æåˆ°çš„æ•ˆæœã€‚

## Model details  
### Key differences with vanilla transformer  
1. GPT åªä½¿ç”¨äº†åŸå§‹ transformer çš„ Decoderã€‚ç”±äºä¸å†éœ€è¦ Encoderï¼Œå› æ­¤åˆ å»äº†ä¸ Encoder äº¤äº’çš„äº¤å‰è‡ªæ³¨æ„åŠ›ï¼Œåªä½¿ç”¨å› æœè‡ªæ³¨æ„åŠ›å±‚ã€‚  
2. GPT çš„ Decoder å±‚ä¸­çš„å‰é¦ˆç¥ç»ç½‘ç»œå±‚å»æ‰äº†ä¸€æ¬¡ dropoutã€‚æ­¤å¤–ï¼Œè¿˜å°†æ¿€æ´»å‡½æ•°ä» ReLU æ›¿æ¢ä¸ºäº† GeLUã€‚  
3. GPT çš„æ¨¡å‹è§„æ¨¡ç•¥å¾®æ‰©å¤§äº†ä¸€äº›ï¼Œè¿˜å›ºå®šäº†è¯è¡¨çš„å¤§å°ï¼ˆå› ä¸º GPT æ˜¯é¢„è®­ç»ƒæ¨¡å‹ï¼‰ã€‚  
4. GPT ä½¿ç”¨äº† [Weight Tying](https://arxiv.org/abs/1608.05859)ï¼Œä»¤ language modelling head ä¸ token embedding å±‚å…±äº«æƒé‡ã€‚  
5. GPT ä½¿ç”¨ learnable positional embedding è€Œä¸æ˜¯æ­£ä½™å¼¦ä½ç½®ç¼–ç ã€‚

æ±‡æ€»å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š  
<table>  
  <thead>  
    <tr style="font-weight: bold; border-bottom: 2px solid">  
      <th></th>  
      <th style="text-align: center">GPT</th>  
      <th style="text-align: center">Transformer</th>  
    </tr>  
  </thead>  
  <tbody style="text-align:center">  
    <tr>  
      <td>Positonal encoding</td>  
      <td> learnable </td>  
      <td> sinusoidal(mainstream) </td>  
    </tr>  
    <tr>  
      <td>num_attention_heads</td>  
      <td>12</td>  
      <td>8</td>  
    </tr>  
    <tr>  
      <td>num_hidden_layers</td>  
      <td>12</td>  
      <td>6 encoder layers, 6 decoder layers</td>  
    </tr>  
    <tr>  
      <td>hidden_size</td>  
      <td>768</td>  
      <td>512</td>  
    </tr>  
    <tr>  
      <td>vocab_size</td>  
      <td>40478</td>  
      <td>depends on dataset</td>  
    </tr>  
    <tr>  
      <td>FFN path</td>  
      <td style="text-align:left">  
      <pre>  
      <code>  
      mlpf = lambda x: dropout(fc2(gelu(fc1(x))))  
      x = x + layer_norm(mlpf(x))  
      </code>  
      </pre>  
      </td>  
      <td style="text-align:left">  
      <pre>  
      <code>  
      mlpf = lambda x: dropout(fc2(dropout(relu(fc1(x)))))  
      x = x + layer_norm(mlpf(x))  
      </code>  
      </pre>  
      </td>  
    </tr>  
  </tbody>  
</table>

### [Byte-pair encoding (BPE)](./modules/bpe.py)  
BPE æ˜¯ä¸€ç§ tokenize çš„æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡åˆå¹¶æœ€é¢‘ç¹å‡ºç°çš„å­—ç¬¦å¯¹æ¥æ„å»ºæ›´å¤§çš„å­è¯å•å…ƒï¼Œä»è€Œå‡å°‘è¯æ±‡è¡¨çš„å¤§å°ï¼Œå¤„ç†ç¨€æœ‰è¯é—®é¢˜ã€‚å®ƒéœ€è¦å…ˆåœ¨ä¸€ä¸ªè¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°è¯è¡¨åæ‰èƒ½è¿›è¡Œç¼–ç å’Œè§£ç ã€‚

ç”±äº Huggingface æä¾›çš„ [BookCorpus](https://huggingface.co/datasets/bookcorpus/bookcorpus) æ•°æ®é›†å·²ç»ç»è¿‡äº†ç»†è‡´çš„åå¤„ç†ï¼Œå› æ­¤æˆ‘ä»¬æ— æ³•å®Œå…¨å¤ç°å‡º [åŸå§‹ GPT ä»£ç ](https://github.com/openai/finetune-transformer-lm) çš„ç»“æœã€‚æˆ‘ä»…åŸºäºåŸå§‹å®ç° [text_utils.py](https://github.com/openai/finetune-transformer-lm/blob/master/text_utils.py) å®Œæˆäº†ç¼–ç å’Œè§£ç éƒ¨åˆ†çš„å·¥ä½œã€‚å¦‚æœä½ å¯¹ BPE çš„è®­ç»ƒæµç¨‹æ„Ÿå…´è¶£ï¼Œå¯ä»¥å‚è€ƒ Karpathy çš„ [minbpe](https://github.com/karpathy/minbpe)ã€‚

#### Training  
1. é¢„åˆ†è¯ï¼šç”¨ [`ftfy`](https://github.com/rspeer/python-ftfy) è§„èŒƒåŒ– Unicode å­—ç¬¦ï¼ŒæŠŠéæ ‡å‡†æ ‡ç‚¹ç»Ÿä¸€ï¼Œå¹¶æ›¿æ¢æ‰€æœ‰çš„ç©ºç™½å­—ç¬¦ä¸º `\n`ï¼Œç„¶åä½¿ç”¨ spacy çš„ [en_core_web_sm](https://spacy.io/models/en#en_core_web_sm) æ¨¡å‹è¿›è¡Œåˆ†è¯ï¼ˆè§ [bpe.py](./modules/bpe.py)ï¼‰ã€‚  
2. åˆå§‹åŒ–è¯æ±‡è¡¨ï¼šå°†æ•´ä¸ªæ–‡æœ¬è¯­æ–™åº“æ‹†åˆ†æˆå•å­—ç¬¦çš„å­è¯å•å…ƒï¼Œæœ€åä¸€ä¸ªå­—ç¬¦æ·»åŠ  `</w>`ã€‚åœ¨è®­ç»ƒåçš„è¯è¡¨ [encoder_bpe_40000.json](./datasets/bookcorpus/encoder_bpe_40000.json) ä¸­å¯ä»¥çœ‹å‡ºï¼Œid ä» 1-238 éƒ½ä¸ºå•ä¸ªå­—ç¬¦ï¼Œ239-476 ä¸ºå•ä¸ªå­—ç¬¦ + `</w>` çš„å½¢å¼ã€‚è¿™é‡Œçš„ `</w>` ä»£è¡¨ä¸€ä¸ª token çš„ç»“å°¾ã€‚ä¾‹å¦‚åœ¨å•è¯ `bamboo` ä¸­ï¼Œæœ€åä¸€ä¸ª `o` ä¼šè¢«è§†ä½œ `o</w>` ä»¥ä¸å€’æ•°ç¬¬äºŒä¸ª `o` åŒºåˆ†ã€‚  
3. ç»Ÿè®¡ bi-gram å­—ç¬¦å¯¹çš„é¢‘ç‡ã€‚  
4. åˆå¹¶æœ€é¢‘ç¹å‡ºç°çš„å­—ç¬¦å¯¹ï¼Œå¹¶å½¢æˆä¸€ä¸ªæ–°çš„å­è¯å•å…ƒã€‚æ›´æ–°è¯­æ–™åº“ä¸­çš„è¯æ±‡è¡¨ï¼Œå¹¶è®°å½•è¯¥åˆå¹¶æ“ä½œã€‚  
5. é‡å¤æ­¥éª¤ 3-4 40000 æ¬¡ï¼Œäºæ˜¯åœ¨ 476 ä¸ªå•ä¸ªè¯å…ƒçš„åŸºç¡€ä¸Šè·å¾—äº† 40000 ä¸ªæ–°çš„å­è¯å•å…ƒã€‚å†åŠ ä¸Š `<unk>` å’Œ `\n</w>` å…±è®¡ 40478 ä¸ªè¯å…ƒã€‚

#### Encoding  
0. åŠ è½½è®­ç»ƒå¥½çš„è¯è¡¨ã€‚  
1. é¢„åˆ†è¯ï¼šå¯¹è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œé¢„åˆ†è¯ï¼ŒåŒè®­ç»ƒé˜¶æ®µã€‚  
2. å°†æ¯ä¸ªå­è¯æ‹†åˆ†æˆå•å­—ç¬¦çš„å­è¯å•å…ƒï¼Œæœ€åä¸€ä¸ªå­—ç¬¦æ·»åŠ  `</w>`ã€‚  
3. ç»Ÿè®¡ bi-gram å­—ç¬¦å¯¹çš„é¢‘ç‡ã€‚  
4. é€‰æ‹©åœ¨è¯è¡¨ä¸­æœ€æ—©è¢«åˆå¹¶çš„å­—ç¬¦å¯¹ï¼Œå¹¶å½¢æˆä¸€ä¸ªæ–°çš„å­è¯å•å…ƒã€‚å°†ç›®å‰æ–‡æœ¬ä¸­å‡ºç°çš„å­—ç¬¦å¯¹ä»¥æ–°å­è¯å•å…ƒè¿›è¡Œæ›¿æ¢ã€‚  
5. é‡å¤æ­¥éª¤ 3-4 ç›´åˆ°æ²¡æœ‰æ›´å¤šçš„æœ‰æ•ˆ bigram æˆ–è€…åªå‰©ä¸€ä¸ªå­—ç¬¦å•å…ƒã€‚  
6. ç¼“å­˜ç»“æœï¼Œå°†å­è¯å•å…ƒæ˜ å°„åˆ°è¯è¡¨ä¸­å¯¹åº” token çš„ idã€‚

#### Decoding  
0. åŠ è½½è®­ç»ƒå¥½çš„è¯è¡¨ã€‚  
1. æ ¹æ®è¯è¡¨å»ºç«‹åå‘æ˜ å°„ï¼Œå°†ç»™å®š token id æ˜ å°„å›åŸå­è¯å³å¯ã€‚
  
## [Pre-training](./pretrain.ipynb)  
æ ¹æ® [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) Sec. 4.1 çš„è®¾ç½®ï¼Œåœ¨ BooksCorpus æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ AdamW ($w = 0.01, \text{max-lr} = 2.4\times 10^{-4}$) ä½œä¸ºä¼˜åŒ–å™¨ï¼Œæ³¨æ„ï¼šæ‰€æœ‰ **åç½®å’Œç¼©æ”¾å±‚ï¼ˆ`LayerNorm`ï¼‰çš„å‚æ•°æƒé‡ä¸ä¼šåº”ç”¨æƒé‡è¡°å‡**ã€‚ä½¿ç”¨å…ˆä½¿å­¦ä¹ ç‡ä» 0 åˆ† 2000 æ­¥çº¿æ€§å¢åŠ åˆ° $\text{max-lr}$ï¼Œè€Œåä½™å¼¦é€€ç«çš„å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥ï¼Œå¯¹éšæœºé‡‡æ ·çš„ 64 ä¸ªè¿ç»­ 512 ä¸ª token sequences çš„ minibatch è¿›è¡Œ 100 ä¸ª epoch çš„è®­ç»ƒã€‚

ä¸ºäº†é‡‡æ ·è¿ç»­æŒ‡å®šæ•°é‡çš„ token sequenceï¼Œæˆ‘ä»¬éœ€è¦å…ˆç”¨ bpe æŠŠåŸæ•°æ®é›†çš„æ–‡æœ¬è½¬æ¢ä¸º token idã€‚è€Œæˆ‘ä»¬è‡ªå·±å®ç°çš„ tokenize é€Ÿåº¦éå¸¸æ…¢ï¼Œå› æ­¤åªèƒ½åœ¨å°éƒ¨åˆ†æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒã€‚å¦‚æœä½ æƒ³è¦å°è¯•æ›´å¤šçš„æ•°æ®ï¼Œå¯ä»¥ä¿®æ”¹ [pretrain.ipynb](./pretrain.ipynb) ä¸­ `load_data` çš„ `loading_ratio` å‚æ•°ã€‚

## [Fine-tuning](./finetune.ipynb)  
åœ¨ BookCorpus æ•°æ®é›†ä¸Šä¸è®­ç»ƒåï¼ŒGPT å·²ç»è·å¾—äº†ä¸€å®šçš„è¯­è¨€èƒ½åŠ›ï¼Œè¦å°†å…¶è¿ç”¨åˆ°æ–°çš„æ•°æ®é›†ä¸Šï¼Œåªéœ€è¦ç•¥å¾®è°ƒæ•´ä¸€ä¸‹æ¨¡å‹ç»“æ„å’Œè¾“å…¥å³å¯ã€‚

<div>  
  <img src="./images/gpt-train.png" alt="GPT architecture and training objectives used in other works" style="width: 100%; height: auto;">  
</div>

åŸè®ºæ–‡ä¸­ Sec. 3.2 æåˆ°ï¼Œæ·»åŠ  language modelling loss ä½œä¸ºå¾®è°ƒçš„è¾…åŠ©ç›®æ ‡æœ‰åŠ©äºå­¦ä¹ ï¼Œå› ä¸º (a) å¯ä»¥æé«˜ç›‘ç£æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œ(b) å¯ä»¥åŠ é€Ÿæ”¶æ•›ã€‚å› æ­¤é™¤äº†è¦å‘è¯è¡¨ä¸­æ·»åŠ ä¸€äº›æ–°çš„ tokensï¼ˆ`<start>` å’Œ `<extract>`ï¼‰ä¹‹å¤–ï¼Œè¿˜éœ€æŠŠ decoder éª¨å¹²çš„è¾“å‡ºè¾“å…¥åˆ°ä¸€ä¸ªæ–°å¢çš„åˆ†ç±»å¤´ä¸­ã€‚

å¾®è°ƒåŸºæœ¬é‡ç”¨é¢„è®­ç»ƒä¸­çš„è¶…å‚æ•°è®¾ç½®ã€‚åˆ†ç±»å™¨ä¹‹å‰æ·»åŠ  dropout å±‚ï¼ˆ$p = 0.1$ï¼‰ã€‚å­¦ä¹ ç‡ç”¨ $6.25e^{-5}$ï¼Œæ‰¹å¤§å°ç”¨ 32ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè®­ç»ƒ 3 ä¸ª epoch å°±è¶³å¤Ÿäº†ã€‚æ­¤å¤–ï¼Œè¿˜ä½¿ç”¨å¸¦ warmup çš„çº¿æ€§å­¦ä¹ ç‡è¡°å‡ç­–ç•¥ï¼Œé¢„çƒ­è®­ç»ƒæ€»è½®æ•°çš„ $0.2\%$ã€‚åˆ†ç±»æŸå¤±çš„æƒé‡è®¾ç½®ä¸º 0.5ã€‚

## Appendix  
### How to download pretrained GPT?  
åœ¨å‘½ä»¤è¡Œè¿è¡Œä»¥ä¸‹æŒ‡ä»¤  
```bash  
pip install -U huggingface-cli  
export HF_ENDPOINT=https://hf-mirror.com  
huggingface-cli download openai-community/openai-gpt --local-dir path/to/pretrained_dir  
```