{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accroding to original paper <a href=\"https://arxiv.org/abs/1810.04805\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, Bert is pretrained on bookcorpus and wikipedia. More info about bookcorpus and wikipedia can be found at huggingface webpages of [bookcorpus](https://huggingface.co/datasets/bookcorpus/bookcorpus) and [wikipedia](https://huggingface.co/datasets/wikimedia/wikipedia).\n",
    "\n",
    "\n",
    "BERT is pretrained on  Masked Language Model (Mask LM) and Next Sentence Prediction tasks during training (original paper, Section 3.1):\n",
    "\n",
    "1. Masked LM: The training data generator randomly selects 15% of token positions for prediction. If the i-th token is selected, it is replaced with: (1) the [MASK] token 80% of the time, (2) a random token 10% of the time, or (3) the original i-th token 10% of the time. The model then predicts the original token using cross-entropy loss.\n",
    "\n",
    "2. Next Sentence Prediction: This is a binary classification task. When selecting sentences A and B for each pre-training example, B is the actual subsequent sentence following A 50% of the time, and the other 50% of the time, B is a randomly chosen sentence from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:48:40.828659Z",
     "start_time": "2025-02-25T11:48:25.470033Z"
    }
   },
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "from data import load_data\n",
    "import config\n",
    "\n",
    "tokenizer, bookcorpus_dl = load_data(\"bookcorpus\", loading_ratio=0.1)\n",
    "_, wikipedia_dl = load_data(\"wikipedia\", loading_ratio=1/41)\n",
    "\n",
    "\n",
    "dataloader_size = len(bookcorpus_dl) + len(wikipedia_dl)\n",
    "print(\"bookcorpus_dl size:\", len(bookcorpus_dl))\n",
    "print(\"wikipedia_dl size:\", len(wikipedia_dl))\n",
    "print(\"Total Dataloader Size:\", dataloader_size)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded at  [/home/bks/.cache/huggingface/datasets/downloads/4ae3271c145b7cf080a16f6a2e0fa1d6410e16d8e50c32966328a2b721cf86a4.cfcd7c0ef3521898b77bd4829f8599ad4bc97b70065998fbda8f675af882a783 (origin=https://hf-mirror.com/datasets/bookcorpus/bookcorpus/resolve/refs%2Fconvert%2Fparquet/plain_text/train/0000.parquet?download=true)]\n",
      "Downloaded at  [/home/bks/.cache/huggingface/datasets/downloads/09e1288f58ee4cdc9195bfc8ffcfa2cb28160e6201045fc17901c4b14ac61f08.d220069fc80398db604d42d5a6b1718d7a4699b18cfa1209a922e783f7cccb71 (origin=https://huggingface.co/datasets/wikimedia/wikipedia/resolve/refs%2Fconvert%2Fparquet/20231101.en/train/0000.parquet)]\n",
      "bookcorpus_dl size: 3902000\n",
      "wikipedia_dl size: 2438800\n",
      "Total Dataloader Size: 6340800\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Build Model\n",
    "The key structural difference between Bert and GPT is that Bert does not use a causal mask in its layers. This allows Bert to leverage bidirectional attention, enabling it to capture global dependencies directly. Consequently, Bert's pre-training tasks are fundamentally different from GPT's next-token prediction task."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:48:49.739059Z",
     "start_time": "2025-02-25T11:48:44.400464Z"
    }
   },
   "source": [
    "import torch\n",
    "from modules.bert import BertForPreTraining\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "bert = BertForPreTraining(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    type_vocab_size=2,\n",
    "    hidden_size=config.hidden_size,\n",
    "    max_len=config.max_len,\n",
    "    num_hidden_layers=config.num_layers,\n",
    "    num_attention_heads=config.attention_heads,\n",
    "    intermediate_size=config.intermediate_size,\n",
    "    dropout=config.dropout,\n",
    "    pad_token_idx=tokenizer.pad_token_id\n",
    ").to(device)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pretrain Model\n",
    "Here I define a BERTTrainer class for pretraining settings."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:48:54.300396Z",
     "start_time": "2025-02-25T11:48:54.155875Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from modules import BertForPreTraining\n",
    "\n",
    "class BERTTrainer:\n",
    "    \"\"\"\n",
    "    BERTTrainer make the pretrained BERT model with two LM training method.\n",
    "\n",
    "        1. Masked Language Model : 3.3.1 Task #1: Masked LM\n",
    "        2. Next Sentence prediction : 3.3.2 Task #2: Next Sentence Prediction\n",
    "\n",
    "    please check the details on README.md with simple example.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            bert: BertForPreTraining,\n",
    "            lr: float = 1e-4, \n",
    "            weight_decay: float = 0.01, \n",
    "            warmup_steps=10000\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which you want to train\n",
    "        :param lr: learning rate of optimizer\n",
    "        :param weight_decay: Adam optimizer weight decay param\n",
    "        :param warmup_steps: warmup steps of learning rate scheduler\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup cuda device for BERT training, argument -c, --cuda should be true\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.bert = bert\n",
    "        self.model = bert.to(self.device)\n",
    "\n",
    "        # Distributed GPU training if CUDA can detect more than 1 GPU\n",
    "        # if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        #     cuda_visible_devices = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\")\n",
    "        #     device_ids = list(map(int, cuda_visible_devices.split(','))) if cuda_visible_devices else []\n",
    "        #     print(\"Using %d GPUS for BERT\" % torch.cuda.device_count())\n",
    "        #     self.model = nn.DataParallel(self.model, device_ids=device_ids )\n",
    "\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=dataloader_size * config.PretrainingConfig.n_epoch,\n",
    "        )\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = nn.NLLLoss(ignore_index=0)\n",
    "\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "\n",
    "    def train(self, epoch, train_dataloader):\n",
    "        self.iteration(epoch, train_dataloader)\n",
    "\n",
    "    def iteration(self, epoch, data_loader):\n",
    "        \"\"\"\n",
    "        loop over the data_loader for training \n",
    "        if on train status, backward operation is activated\n",
    "        and also auto save the model every epoch\n",
    "        :param epoch: current epoch index\n",
    "        :param data_loader: torch.utils.data.DataLoader for iteration\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "        # Setting the tqdm progress bar\n",
    "        data_iter = tqdm.tqdm(enumerate(data_loader),\n",
    "                              desc=\"EP_train:%d\" % (epoch),\n",
    "                              total=dataloader_size,\n",
    "                              bar_format=\"{l_bar}{r_bar}\")\n",
    "\n",
    "        avg_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_element = 0\n",
    "\n",
    "        for i, data in data_iter:\n",
    "            # get batch data\n",
    "            input_ids = data.input_ids.to(self.device)\n",
    "            attention_mask = data.attention_mask.to(self.device)\n",
    "            token_type_ids = data.token_type_ids.to(self.device)\n",
    "            labels = data.labels.to(self.device)\n",
    "            is_next = data.is_next.to(self.device)\n",
    "            \n",
    "            outputs = self.model.forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels,\n",
    "                next_sentence_label=is_next\n",
    "            )\n",
    "            total_loss, prediction_scores, seq_relationship_score = outputs\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "\n",
    "            # next sentence prediction accuracy\n",
    "            correct = seq_relationship_score.argmax(dim=-1).eq(is_next.squeeze(-1)).sum().item()\n",
    "            \n",
    "            avg_loss += total_loss.item()\n",
    "            total_correct += correct\n",
    "            total_element += is_next.nelement()\n",
    "\n",
    "        print(\"EP%d_train, avg_loss=\" % (epoch), avg_loss / len(data_iter), \"NSP_acc=\",\n",
    "              total_correct * 100.0 / total_element)\n",
    "\n",
    "    def save(self, epoch, file_path):\n",
    "        \"\"\"\n",
    "        Saving the current BERT model on file_path\n",
    "\n",
    "        :param epoch: current epoch number\n",
    "        :param file_path: model output path which gonna be file_path+\"ep%d\" % epoch\n",
    "        :return: final_output_path\n",
    "        \"\"\"\n",
    "        output_path = str(file_path) + \"_ep%d.pth\" % epoch\n",
    "        torch.save(self.bert.cpu(), output_path)\n",
    "        self.bert.to(self.device)\n",
    "        print(\"EP:%d Model Saved on:\" % epoch, output_path)\n",
    "        return output_path\n",
    "    \n",
    "trainer = BERTTrainer(\n",
    "    bert,\n",
    "    lr=config.PretrainingConfig.lr,\n",
    "    weight_decay=config.PretrainingConfig.weight_decay,\n",
    "    warmup_steps=config.PretrainingConfig.warmup_steps,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 110106428\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train loop\n",
    "Simple test of pretraining process."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Training Start\")\n",
    "for epoch in range(config.PretrainingConfig.n_epoch):\n",
    "    train_dl = itertools.chain(bookcorpus_dl, wikipedia_dl)\n",
    "    trainer.train(epoch, train_dl)\n",
    "    if epoch % config.PretrainingConfig.checkpoint_freq == 0:\n",
    "        trainer.save(epoch, config.trained_path)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
