{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script processes a corpus to prepare data for BERT-style training. The dataset is implemented in the **BERTDataset** class, which reads and tokenizes text, generates masked tokens for training, and creates segment labels. The vocabulary is built using the WordVocab class, which counts word frequencies and assigns token indices.\n",
    "\n",
    "The corpus is first loaded from a text file, where each line consists of two tab-separated sentences. Here I load the first parquet file of datasets **Bookcorpus** and **Wikipedia(20231101.en)** to create a txt file named corpus.txt.\n",
    "\n",
    "The dataset supports both memory-based and file-streaming modes. The **random_word** function randomly replaces words with mask tokens for masked language modeling(MLM), while **random_sent** generates paired sentences for next-sentence prediction(NSP).\n",
    "The vocabulary is built using a word counter and saved to disk for reuse. In **build()**, the text is preprocessed, and a vocabulary is created based on word frequency thresholds. Finally, the vocabulary is saved for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab already exists! If you want to rebuild it, please delete the existing vocab file.\n",
      "loading train dataset done\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "from torch.utils.data import DataLoader\n",
    "import config\n",
    "# load bookcorpus and english wikipedia\n",
    "data.load_bookcorpus_wikipedia(0.1, 1/41)\n",
    "\n",
    "data.build()\n",
    "print(\"loading train dataset done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vocab /root/autodl-tmp/bert/dataset/vocab\n",
      "Vocab Size:  30522\n",
      "Loading Train Dataset /root/autodl-tmp/bert/dataset/corpus.txt\n",
      "num_load_lines: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset: 100%|██████████| 11/11 [00:00<00:00, 82241.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of self.lines: 11\n",
      "Train Dataset Size:  11\n",
      "Loading Test Dataset None\n",
      "Creating Dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from data import WordVocab, BERTDataset\n",
    "print(\"Loading Vocab\", config.vocab_path)\n",
    "vocab = WordVocab.load_vocab(config.vocab_path)\n",
    "print(\"Vocab Size: \", len(vocab))\n",
    "\n",
    "print(\"Loading Train Dataset\", config.train_dataset)\n",
    "train_dataset = BERTDataset(config.train_dataset, vocab, seq_len=config.sequence_length,\n",
    "                            corpus_lines=config.corpus_lines, on_memory=config.on_memory, loading_ratio=0.000001)\n",
    "print(\"Train Dataset Size: \", len(train_dataset))\n",
    "\n",
    "print(\"Loading Test Dataset\", config.test_dataset)\n",
    "test_dataset = BERTDataset(config.test_dataset, vocab, seq_len=config.sequence_length, on_memory=config.on_memory, loading_ratio=0.05) \\\n",
    "    if config.test_dataset is not None else None\n",
    "\n",
    "print(\"Creating Dataloader\")\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=config.num_workers)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=config.batch_size, num_workers=config.num_workers) \\\n",
    "    if test_dataset is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Build Model\n",
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BERT model\n",
      "BERT(\n",
      "  (embedding): BERTEmbedding(\n",
      "    (token): TokenEmbedding(30522, 768, padding_idx=0)\n",
      "    (position): PositionalEmbedding(512, 768)\n",
      "    (segment): SegmentEmbedding(2, 768, padding_idx=0)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_blocks): ModuleList(\n",
      "    (0): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from modules.bert import BERT\n",
    "print(\"Building BERT model\")\n",
    "bert = BERT(\n",
    "    len(vocab), \n",
    "    hidden=config.hidden_size, \n",
    "    n_layers=config.num_layers, \n",
    "    attn_heads=config.attention_heads\n",
    ")\n",
    "print(bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model\n",
    "Here I define a BERTTrainer class for pretraing settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 132363068\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from torch.optim import Adam\n",
    "from modules.bertLM import BERTLM\n",
    "from modules.bert import BERT, ScheduledOptim\n",
    "\n",
    "\n",
    "class BERTTrainer:\n",
    "    \"\"\"\n",
    "    BERTTrainer make the pretrained BERT model with two LM training method.\n",
    "\n",
    "        1. Masked Language Model : 3.3.1 Task #1: Masked LM\n",
    "        2. Next Sentence prediction : 3.3.2 Task #2: Next Sentence Prediction\n",
    "\n",
    "    please check the details on README.md with simple example.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size: int,\n",
    "                 train_dataloader: DataLoader, test_dataloader: DataLoader = None,\n",
    "                 lr: float = 1e-4, betas=(0.9, 0.999), weight_decay: float = 0.01, warmup_steps=10000, total_steps=1000000,\n",
    "                 with_cuda: bool = True, cuda_devices=None, log_freq: int = 10):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which you want to train\n",
    "        :param vocab_size: total word vocab size\n",
    "        :param train_dataloader: train dataset data loader\n",
    "        :param test_dataloader: test dataset data loader [can be None]\n",
    "        :param lr: learning rate of optimizer\n",
    "        :param betas: Adam optimizer betas\n",
    "        :param weight_decay: Adam optimizer weight decay param\n",
    "        :param with_cuda: traning with cuda\n",
    "        :param log_freq: logging frequency of the batch iteration\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup cuda device for BERT training, argument -c, --cuda should be true\n",
    "        cuda_condition = torch.cuda.is_available() and with_cuda\n",
    "        self.device = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")\n",
    "\n",
    "        # This BERT model will be saved every epoch\n",
    "        self.bert = bert\n",
    "        # Initialize the BERT Language Model, with BERT model\n",
    "        self.model = BERTLM(bert, vocab_size).to(self.device)\n",
    "\n",
    "        # Distributed GPU training if CUDA can detect more than 1 GPU\n",
    "        if with_cuda and torch.cuda.device_count() > 1:\n",
    "            print(\"Using %d GPUS for BERT\" % torch.cuda.device_count())\n",
    "            self.model = nn.DataParallel(self.model, device_ids=cuda_devices)\n",
    "\n",
    "        # Setting the train and test data loader\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        # Setting the Adam optimizer with hyper-param\n",
    "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(self.optim, n_warmup_steps=warmup_steps, total_steps=total_steps)\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = nn.NLLLoss(ignore_index=0)\n",
    "\n",
    "        self.log_freq = log_freq\n",
    "\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        \"\"\"\n",
    "        loop over the data_loader for training or testing\n",
    "        if on train status, backward operation is activated\n",
    "        and also auto save the model every peoch\n",
    "\n",
    "        :param epoch: current epoch index\n",
    "        :param data_loader: torch.utils.data.DataLoader for iteration\n",
    "        :param train: boolean value of is train or test\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        str_code = \"train\" if train else \"test\"\n",
    "\n",
    "        # Setting the tqdm progress bar\n",
    "        data_iter = tqdm.tqdm(enumerate(data_loader),\n",
    "                              desc=\"EP_%s:%d\" % (str_code, epoch),\n",
    "                              total=len(data_loader),\n",
    "                              bar_format=\"{l_bar}{r_bar}\")\n",
    "\n",
    "        avg_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_element = 0\n",
    "\n",
    "        for i, data in data_iter:\n",
    "            # 0. batch_data will be sent into the device(GPU or cpu)\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            # 1. forward the next_sentence_prediction and masked_lm model\n",
    "            next_sent_output, mask_lm_output = self.model.forward(data[\"bert_input\"], data[\"segment_label\"])\n",
    "\n",
    "            # 2-1. NLL(negative log likelihood) loss of is_next classification result\n",
    "            next_loss = self.criterion(next_sent_output, data[\"is_next\"])\n",
    "\n",
    "            # 2-2. NLLLoss of predicting masked token word\n",
    "            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"])\n",
    "\n",
    "            # 2-3. Adding next_loss and mask_loss : 3.4 Pre-training Procedure\n",
    "            loss = next_loss + mask_loss\n",
    "\n",
    "            # 3. backward and optimization only in train\n",
    "            if train:\n",
    "                self.optim_schedule.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim_schedule.step_and_update_lr()\n",
    "\n",
    "            # next sentence prediction accuracy\n",
    "            correct = next_sent_output.argmax(dim=-1).eq(data[\"is_next\"]).sum().item()\n",
    "            avg_loss += loss.item()\n",
    "            total_correct += correct\n",
    "            total_element += data[\"is_next\"].nelement()\n",
    "\n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"avg_loss\": avg_loss / (i + 1),\n",
    "                \"avg_acc\": total_correct / total_element * 100,\n",
    "                \"loss\": loss.item()\n",
    "            }\n",
    "\n",
    "            if i % self.log_freq == 0:\n",
    "                data_iter.write(str(post_fix))\n",
    "\n",
    "        print(\"EP%d_%s, avg_loss=\" % (epoch, str_code), avg_loss / len(data_iter), \"total_acc=\",\n",
    "              total_correct * 100.0 / total_element)\n",
    "\n",
    "    def save(self, epoch, file_path):\n",
    "        \"\"\"\n",
    "        Saving the current BERT model on file_path\n",
    "\n",
    "        :param epoch: current epoch number\n",
    "        :param file_path: model output path which gonna be file_path+\"ep%d\" % epoch\n",
    "        :return: final_output_path\n",
    "        \"\"\"\n",
    "        output_path = str(file_path) + \"_ep%d.pth\" % epoch\n",
    "        torch.save(self.bert.cpu(), output_path)\n",
    "        self.bert.to(self.device)\n",
    "        print(\"EP:%d Model Saved on:\" % epoch, output_path)\n",
    "        return output_path\n",
    "\n",
    "    \n",
    "trainer = BERTTrainer(\n",
    "    bert, len(vocab), \n",
    "    train_dataloader=train_data_loader, \n",
    "    test_dataloader=test_data_loader,       \n",
    "    lr=config.learning_rate,     \n",
    "    betas=(config.adam_beta1, config.adam_beta2),\n",
    "    weight_decay=config.adam_weight_decay,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    total_steps = config.total_steps,\n",
    "    with_cuda=config.with_cuda, \n",
    "    cuda_devices=config.cuda_devices, \n",
    "    log_freq=config.log_freq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Optimizer and Scheduler\n",
    "ScheduledOptim class is a wrapper for an optimizer that implements a learning rate scheduling strategy inspired by the Transformer paper (Attention Is All You Need). It adjusts the learning rate using a warm-up and decay mechanism to stabilize training. \n",
    "\n",
    "Original paper shows that they use Adam with learning rate of 1e-4, β1 = 0.9, β2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps, and linear decay of the learning rate in the pretraining\n",
    "\n",
    "Check modules/optim_schedule.py for deeper understanding.\n",
    "\n",
    "The learing rate schedule is shown as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "from modules.bert import ScheduledOptim\n",
    "import config\n",
    "\n",
    "#random create one model\n",
    "test_model = torch.nn.Linear(10, 1)\n",
    "\n",
    "\n",
    "optimizer = Adam(test_model.parameters(), lr=config.learning_rate, betas=(config.adam_beta1, config.adam_beta2), weight_decay=config.adam_weight_decay)\n",
    "\n",
    "lr_values = []\n",
    "\n",
    "scheduler = ScheduledOptim(optimizer,config.warmup_steps,config.total_steps)\n",
    "\n",
    "for _ in range(config.total_steps):\n",
    "    \n",
    "    # update lr\n",
    "    scheduler.step_and_update_lr()\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    lr_values.append(lr)\n",
    "    \n",
    "\n",
    "# 绘制学习率曲线\n",
    "plt.plot(lr_values)\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train loop\n",
    "Simple test of pretraining process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0: 100%|| 1/1 [00:00<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'avg_loss': 11.230925559997559, 'avg_acc': 72.72727272727273, 'loss': 11.230925559997559}\n",
      "EP0_train, avg_loss= 11.230925559997559 total_acc= 72.72727272727273\n",
      "EP:0 Model Saved on: /root/autodl-tmp/bert/checkpoints/bert_self_trained_ep0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1: 100%|| 1/1 [00:00<00:00,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 0, 'avg_loss': 11.146394729614258, 'avg_acc': 63.63636363636363, 'loss': 11.146394729614258}\n",
      "EP1_train, avg_loss= 11.146394729614258 total_acc= 63.63636363636363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Start\")\n",
    "for epoch in range(config.epochs):\n",
    "    trainer.train(epoch)\n",
    "    if epoch % config.log_freq == 0:\n",
    "        trainer.save(epoch, config.trained_path)\n",
    "    if test_data_loader is not None:\n",
    "        trainer.test(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
