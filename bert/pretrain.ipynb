{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script processes a corpus to prepare data for BERT-style training. The dataset is implemented in the **BERTDataset** class, which reads and tokenizes text, generates masked tokens for training, and creates segment labels. The vocabulary is built using the WordVocab class, which counts word frequencies and assigns token indices.\n",
    "The corpus is first loaded from a text file, where each line consists of two tab-separated sentences. The dataset supports both memory-based and file-streaming modes. The **random_word** function randomly replaces words with mask tokens for masked language modeling(MLM), while **random_sent** generates paired sentences for next-sentence prediction(NSP).\n",
    "The vocabulary is built using a word counter and saved to disk for reuse. In **build()**, the text is preprocessed, and a vocabulary is created based on word frequency thresholds. Finally, the vocabulary is saved for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 80993.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE: 77\n",
      "VOCAB CONTENT: ['<pad>', '<unk>', '<eos>', '<sos>', '<mask>', 'the', 'The', 'bright', 'by', 'book.', 'lazy', 'was', 'a', 'student', 'bird', 'quickly', 'teacher', 'In', 'day,', 'eagerly', 'nervously', 'observed', 'taught', 'teacher.', 'A', 'Bright', 'beautiful', 'calm', 'calmly', 'car', 'cat', 'city', 'city.', 'climbed', 'exciting', 'explore', 'explored', 'mountain.', 'quick', 'read', 'short', 'student.', 'sunny', 'their', 'to', 'After', 'Dark', 'car.', 'cat.', 'dark', 'dog', 'dog.', 'drove', 'jumped', 'learned', 'mountain', 'nervous', 'over', 'slowly', 'tall', 'Beautiful', 'Calm', 'Lazy', 'Quick', 'Quickly', 'cat,', 'driven', 'from', 'happily', 'in', 'learning', 'mountain,', 'quickly.', 'ran', 'ran.', 'teaching', 'they']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "from torch.utils.data import DataLoader\n",
    "import config\n",
    "data.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vocab /root/autodl-tmp/bert/output/vocab\n",
      "Vocab Size:  77\n",
      "Loading Train Dataset /root/autodl-tmp/bert/dataset/corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset: 14it [00:00, 111423.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Size:  14\n",
      "Loading Test Dataset None\n",
      "Creating Dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from data import WordVocab, BERTDataset\n",
    "print(\"Loading Vocab\", config.vocab_path)\n",
    "vocab = WordVocab.load_vocab(config.vocab_path)\n",
    "print(\"Vocab Size: \", len(vocab))\n",
    "\n",
    "print(\"Loading Train Dataset\", config.train_dataset)\n",
    "train_dataset = BERTDataset(config.train_dataset, vocab, seq_len=config.sequence_length,\n",
    "                            corpus_lines=config.corpus_lines, on_memory=config.on_memory)\n",
    "print(\"Train Dataset Size: \", len(train_dataset))\n",
    "\n",
    "print(\"Loading Test Dataset\", config.test_dataset)\n",
    "test_dataset = BERTDataset(config.test_dataset, vocab, seq_len=config.sequence_length, on_memory=config.on_memory) \\\n",
    "    if config.test_dataset is not None else None\n",
    "\n",
    "print(\"Creating Dataloader\")\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=config.num_workers)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=config.batch_size, num_workers=config.num_workers) \\\n",
    "    if test_dataset is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BERT model\n",
      "BERT(\n",
      "  (embedding): BERTEmbedding(\n",
      "    (token): TokenEmbedding(77, 768, padding_idx=0)\n",
      "    (position): PositionalEmbedding()\n",
      "    (segment): SegmentEmbedding(2, 768, padding_idx=0)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_blocks): ModuleList(\n",
      "    (0): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from modules.bert import BERT\n",
    "from modules.pretrain import BERTTrainer\n",
    "print(\"Building BERT model\")\n",
    "bert = BERT(\n",
    "    len(vocab), \n",
    "    hidden=config.hidden_size, \n",
    "    n_layers=config.num_layers, \n",
    "    attn_heads=config.attention_heads\n",
    ")\n",
    "print(bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating BERT Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 85175887\n"
     ]
    }
   ],
   "source": [
    "trainer = BERTTrainer(\n",
    "    bert, len(vocab), \n",
    "    train_dataloader=train_data_loader, \n",
    "    test_dataloader=test_data_loader,       \n",
    "    lr=config.learning_rate,     \n",
    "    betas=(config.adam_beta1, config.adam_beta2),\n",
    "    weight_decay=config.adam_weight_decay,\n",
    "    with_cuda=config.with_cuda, \n",
    "    cuda_devices=config.cuda_devices, \n",
    "    log_freq=config.log_freq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ScheduledOptim class is a wrapper for an optimizer that implements a learning rate scheduling strategy inspired by the Transformer paper (Attention Is All You Need). It adjusts the learning rate using a warm-up and decay mechanism to stabilize training. Check modules/optim_schedule.py for deep understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  4, 38,  ...,  0,  0,  0],\n",
      "        [ 3,  4, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[ 0,  6,  0,  ...,  0,  0,  0],\n",
      "        [ 0, 25,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1])}\n",
      "Next sentence output: tensor([[-0.8945, -0.5256],\n",
      "        [-0.8142, -0.5851],\n",
      "        [-1.0168, -0.4491],\n",
      "        [-1.1730, -0.3702],\n",
      "        [-0.7902, -0.6047],\n",
      "        [-0.8978, -0.5233],\n",
      "        [-1.0245, -0.4447],\n",
      "        [-0.8274, -0.5748],\n",
      "        [-0.9165, -0.5107],\n",
      "        [-1.2540, -0.3360],\n",
      "        [-1.0984, -0.4056],\n",
      "        [-0.9390, -0.4960],\n",
      "        [-1.8348, -0.1739],\n",
      "        [-0.7509, -0.6386]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-3.2774, -4.4999, -4.6183,  ..., -4.2047, -4.6991, -3.9766],\n",
      "         [-4.4804, -5.0470, -4.5892,  ..., -4.0776, -3.6598, -5.3186],\n",
      "         [-4.1333, -4.8934, -4.3948,  ..., -3.7986, -4.4395, -5.2309],\n",
      "         ...,\n",
      "         [-5.1937, -4.8178, -4.4624,  ..., -3.4072, -3.9207, -5.6051],\n",
      "         [-4.9818, -5.1180, -4.3055,  ..., -3.7710, -3.8314, -5.3957],\n",
      "         [-5.1630, -5.2253, -4.5324,  ..., -3.6331, -3.8719, -4.5226]],\n",
      "\n",
      "        [[-4.0164, -5.2356, -4.2532,  ..., -3.8453, -4.3176, -4.1291],\n",
      "         [-4.5321, -5.3653, -4.2720,  ..., -3.7056, -3.9823, -5.5095],\n",
      "         [-5.2408, -4.3693, -3.6816,  ..., -3.8848, -4.9894, -4.6894],\n",
      "         ...,\n",
      "         [-5.8190, -5.6524, -4.2817,  ..., -3.3537, -4.1802, -5.0858],\n",
      "         [-5.0496, -5.8273, -4.3212,  ..., -3.4088, -4.3041, -5.0572],\n",
      "         [-5.1922, -5.1990, -4.3502,  ..., -3.0216, -3.8247, -4.5305]],\n",
      "\n",
      "        [[-3.5535, -4.8715, -4.3354,  ..., -4.5018, -4.1138, -3.9177],\n",
      "         [-3.8381, -5.6845, -4.5917,  ..., -4.5552, -4.3513, -4.5308],\n",
      "         [-4.1606, -5.0707, -3.8071,  ..., -3.9650, -4.5044, -4.6081],\n",
      "         ...,\n",
      "         [-5.1178, -5.4056, -4.7852,  ..., -3.7224, -4.0474, -5.1192],\n",
      "         [-5.0490, -5.3531, -4.4705,  ..., -4.0021, -3.9106, -4.9622],\n",
      "         [-5.0599, -5.5501, -4.3516,  ..., -3.9644, -4.1595, -4.8225]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.7075, -4.7317, -4.2378,  ..., -4.2418, -4.5656, -3.7250],\n",
      "         [-4.3628, -5.0042, -4.3931,  ..., -3.9864, -4.9751, -3.8910],\n",
      "         [-3.7676, -4.9656, -5.2521,  ..., -4.1346, -4.7097, -5.5647],\n",
      "         ...,\n",
      "         [-5.5000, -5.5483, -4.5573,  ..., -3.9027, -4.1691, -5.4188],\n",
      "         [-5.4477, -4.5802, -4.5693,  ..., -3.9523, -4.0659, -4.7577],\n",
      "         [-4.9504, -5.2812, -4.3849,  ..., -4.1662, -3.8840, -4.5022]],\n",
      "\n",
      "        [[-3.4465, -4.6904, -4.5323,  ..., -3.8567, -4.6344, -4.5205],\n",
      "         [-3.7075, -5.0401, -5.0075,  ..., -4.2164, -4.6384, -5.2234],\n",
      "         [-4.0892, -4.6994, -4.7283,  ..., -3.5133, -4.4301, -4.7215],\n",
      "         ...,\n",
      "         [-4.9668, -5.4461, -4.6418,  ..., -3.6679, -3.9170, -5.5615],\n",
      "         [-4.9927, -5.0167, -4.2121,  ..., -3.4157, -4.2692, -5.0885],\n",
      "         [-4.9358, -4.7319, -4.3335,  ..., -3.6648, -4.0560, -4.8195]],\n",
      "\n",
      "        [[-3.7436, -4.8067, -4.2191,  ..., -3.9607, -4.0382, -4.4088],\n",
      "         [-4.1966, -4.7985, -4.9509,  ..., -4.3294, -4.4600, -5.0521],\n",
      "         [-4.5127, -5.0562, -4.7391,  ..., -3.9551, -4.2832, -4.7134],\n",
      "         ...,\n",
      "         [-5.1371, -5.0336, -4.3988,  ..., -3.7518, -3.7045, -5.6140],\n",
      "         [-5.0294, -5.0425, -4.5176,  ..., -3.5100, -3.8349, -5.2060],\n",
      "         [-4.9757, -4.8776, -4.6196,  ..., -3.2975, -3.7072, -4.3288]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.5050290822982788\n",
      "Masked LM loss: 4.393974781036377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0: 100%|| 1/1 [00:00<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'avg_loss': 4.899003982543945, 'avg_acc': 42.857142857142854, 'loss': 4.899003982543945}\n",
      "EP0_train, avg_loss= 4.899003982543945 total_acc= 42.857142857142854\n",
      "EP:0 Model Saved on: /root/autodl-tmp/bert/checkpoints/bert_self_trained_ep0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6,  4,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  6,  0,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0, 56,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1])}\n",
      "Next sentence output: tensor([[-0.4819, -0.9614],\n",
      "        [-1.4141, -0.2786],\n",
      "        [-1.0736, -0.4182],\n",
      "        [-1.2029, -0.3571],\n",
      "        [-1.3528, -0.2991],\n",
      "        [-1.0876, -0.4110],\n",
      "        [-1.1126, -0.3985],\n",
      "        [-1.3762, -0.2911],\n",
      "        [-1.2836, -0.3244],\n",
      "        [-0.8452, -0.5612],\n",
      "        [-0.8698, -0.5431],\n",
      "        [-0.5781, -0.8232],\n",
      "        [-1.0715, -0.4193],\n",
      "        [-1.0167, -0.4491]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-3.7331, -4.8306, -4.3710,  ..., -4.1410, -4.3011, -4.1278],\n",
      "         [-3.9063, -4.9431, -5.0492,  ..., -4.0954, -4.8773, -4.9721],\n",
      "         [-4.1574, -4.9284, -4.0548,  ..., -3.9942, -5.1895, -4.9490],\n",
      "         ...,\n",
      "         [-5.0671, -5.0341, -3.7872,  ..., -3.7622, -4.3719, -4.9224],\n",
      "         [-4.8927, -5.0060, -4.2362,  ..., -3.6905, -3.8714, -4.7034],\n",
      "         [-5.0849, -4.6493, -4.5490,  ..., -3.6611, -4.0147, -4.6310]],\n",
      "\n",
      "        [[-3.7598, -4.7810, -4.2655,  ..., -4.1452, -4.5615, -3.4386],\n",
      "         [-3.8760, -5.4947, -5.2303,  ..., -3.7985, -5.0883, -6.2772],\n",
      "         [-4.9652, -4.9792, -3.9441,  ..., -4.2248, -5.0943, -4.5973],\n",
      "         ...,\n",
      "         [-5.0615, -6.0019, -4.5533,  ..., -3.4226, -4.1031, -4.9485],\n",
      "         [-5.1282, -5.7194, -4.3898,  ..., -3.6652, -3.5733, -5.1862],\n",
      "         [-5.2940, -5.4926, -4.3260,  ..., -3.2582, -3.5715, -4.5736]],\n",
      "\n",
      "        [[-3.5120, -5.2739, -4.1351,  ..., -4.3665, -4.0894, -3.8832],\n",
      "         [-3.7246, -5.0490, -5.5217,  ..., -4.4118, -4.6101, -5.5346],\n",
      "         [-4.4816, -4.5956, -4.2474,  ..., -4.2446, -4.6690, -5.0821],\n",
      "         ...,\n",
      "         [-4.9526, -5.2271, -4.4180,  ..., -3.4227, -4.0219, -5.1677],\n",
      "         [-5.4966, -5.1879, -4.2457,  ..., -3.8053, -3.6496, -4.6787],\n",
      "         [-4.9175, -5.2150, -4.4072,  ..., -4.0485, -3.4686, -4.6499]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.6677, -4.9692, -4.3268,  ..., -4.3988, -4.5664, -4.1145],\n",
      "         [-4.3567, -4.4804, -4.5187,  ..., -3.9896, -4.8341, -4.3616],\n",
      "         [-4.0768, -4.9241, -5.2532,  ..., -4.1376, -5.1818, -4.8031],\n",
      "         ...,\n",
      "         [-4.9743, -5.3848, -4.6004,  ..., -3.5738, -3.9952, -4.6397],\n",
      "         [-5.4915, -5.4526, -4.8594,  ..., -4.0030, -4.4040, -5.0283],\n",
      "         [-4.9217, -5.0695, -4.4071,  ..., -4.0667, -4.1322, -4.4719]],\n",
      "\n",
      "        [[-3.2767, -4.5309, -4.4871,  ..., -3.9420, -3.8778, -4.3659],\n",
      "         [-4.0554, -4.8984, -4.8924,  ..., -4.1947, -4.4290, -5.5915],\n",
      "         [-4.2255, -5.2205, -4.9022,  ..., -4.0234, -3.6534, -5.5431],\n",
      "         ...,\n",
      "         [-5.1388, -4.8357, -4.3570,  ..., -3.5180, -4.0188, -5.3376],\n",
      "         [-5.5159, -5.2877, -4.5131,  ..., -3.5258, -3.6873, -4.8850],\n",
      "         [-5.4611, -5.0714, -4.7685,  ..., -3.8101, -3.6791, -5.0956]],\n",
      "\n",
      "        [[-4.0493, -4.9303, -4.1943,  ..., -4.6823, -4.5609, -4.0987],\n",
      "         [-3.7887, -4.8091, -4.8055,  ..., -3.9053, -4.4589, -5.0257],\n",
      "         [-4.8440, -5.4748, -5.1921,  ..., -3.9424, -4.4724, -4.6329],\n",
      "         ...,\n",
      "         [-5.4968, -5.3452, -4.3052,  ..., -3.7846, -3.6574, -5.2548],\n",
      "         [-4.8175, -5.2648, -4.4776,  ..., -3.0171, -3.8355, -4.5324],\n",
      "         [-5.3342, -4.9602, -4.4564,  ..., -3.5271, -3.8780, -4.4185]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.443993479013443\n",
      "Masked LM loss: 4.467946529388428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1: 100%|| 1/1 [00:00<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 0, 'avg_loss': 4.911940097808838, 'avg_acc': 42.857142857142854, 'loss': 4.911940097808838}\n",
      "EP1_train, avg_loss= 4.911940097808838 total_acc= 42.857142857142854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:2:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[0, 6, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1])}\n",
      "Next sentence output: tensor([[-1.0381, -0.4372],\n",
      "        [-1.2583, -0.3343],\n",
      "        [-1.0699, -0.4201],\n",
      "        [-0.9634, -0.4806],\n",
      "        [-1.2084, -0.3548],\n",
      "        [-1.0283, -0.4426],\n",
      "        [-0.8886, -0.5297],\n",
      "        [-0.9878, -0.4658],\n",
      "        [-1.5560, -0.2370],\n",
      "        [-1.4139, -0.2786],\n",
      "        [-1.1367, -0.3870],\n",
      "        [-0.5853, -0.8141],\n",
      "        [-1.0149, -0.4501],\n",
      "        [-1.0525, -0.4294]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-3.7147, -4.7435, -3.8935,  ..., -4.0812, -4.0205, -4.2383],\n",
      "         [-3.8621, -4.8683, -5.0167,  ..., -3.8148, -4.6480, -5.2095],\n",
      "         [-4.2784, -4.8255, -4.3325,  ..., -3.7511, -4.5940, -4.9447],\n",
      "         ...,\n",
      "         [-5.1735, -5.2461, -3.9202,  ..., -3.4991, -3.8972, -5.1041],\n",
      "         [-4.7135, -4.9543, -4.2019,  ..., -3.2957, -3.8909, -4.6583],\n",
      "         [-4.9211, -5.2443, -4.3193,  ..., -3.8195, -3.8685, -4.3658]],\n",
      "\n",
      "        [[-3.7526, -4.6554, -4.2150,  ..., -4.0618, -4.5022, -4.3179],\n",
      "         [-3.9595, -4.6838, -5.4481,  ..., -3.8192, -4.7042, -5.6907],\n",
      "         [-4.4999, -4.5142, -4.1394,  ..., -3.8755, -5.1923, -4.5010],\n",
      "         ...,\n",
      "         [-5.0464, -5.2366, -4.7611,  ..., -3.6271, -3.7962, -5.6884],\n",
      "         [-4.8502, -5.0508, -4.4885,  ..., -3.5425, -3.9844, -5.2505],\n",
      "         [-5.2136, -5.0364, -4.3629,  ..., -3.4627, -4.1008, -4.6717]],\n",
      "\n",
      "        [[-3.2699, -4.1227, -4.3768,  ..., -4.1182, -4.9131, -4.0190],\n",
      "         [-4.0281, -5.2123, -4.9033,  ..., -3.9687, -4.4958, -5.6530],\n",
      "         [-4.3463, -4.7583, -3.8387,  ..., -3.5373, -4.7734, -4.3797],\n",
      "         ...,\n",
      "         [-4.8140, -4.9345, -4.3615,  ..., -3.4174, -3.5650, -5.3059],\n",
      "         [-5.0907, -5.2932, -4.2308,  ..., -3.7279, -3.7976, -5.2139],\n",
      "         [-5.0643, -5.4675, -4.6652,  ..., -3.5689, -3.3411, -4.1834]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.4528, -4.6702, -4.1144,  ..., -4.1391, -4.6872, -4.5055],\n",
      "         [-4.4132, -4.3939, -4.7336,  ..., -4.0938, -4.7239, -4.7551],\n",
      "         [-3.9044, -4.8091, -5.1494,  ..., -3.7641, -4.9640, -5.3542],\n",
      "         ...,\n",
      "         [-5.4484, -5.2150, -4.9039,  ..., -3.4851, -3.7964, -5.3636],\n",
      "         [-5.0312, -5.3481, -4.4093,  ..., -3.7898, -3.8122, -5.0958],\n",
      "         [-5.0464, -5.3701, -4.3623,  ..., -3.5438, -3.5821, -4.4746]],\n",
      "\n",
      "        [[-3.5226, -4.6001, -4.6052,  ..., -3.4796, -4.3198, -4.3455],\n",
      "         [-4.4097, -4.9294, -5.3489,  ..., -3.7927, -4.9268, -5.5496],\n",
      "         [-4.6547, -4.6903, -5.4017,  ..., -3.6132, -4.5042, -4.3318],\n",
      "         ...,\n",
      "         [-5.3090, -4.8791, -4.4435,  ..., -2.9620, -3.7595, -5.5107],\n",
      "         [-5.2604, -5.4423, -4.3351,  ..., -3.3144, -3.9765, -4.8436],\n",
      "         [-5.0660, -5.0617, -4.2430,  ..., -3.3179, -3.9353, -4.4437]],\n",
      "\n",
      "        [[-4.1385, -4.1323, -4.2740,  ..., -4.1544, -4.5922, -4.0004],\n",
      "         [-3.7338, -4.6103, -4.9551,  ..., -4.3793, -4.3167, -5.2073],\n",
      "         [-4.7814, -5.2054, -5.0954,  ..., -4.1490, -4.3896, -4.4790],\n",
      "         ...,\n",
      "         [-5.2167, -5.2040, -4.5321,  ..., -3.6052, -3.6823, -5.2824],\n",
      "         [-4.6327, -4.6389, -4.3960,  ..., -3.8663, -3.9602, -5.2035],\n",
      "         [-4.8205, -5.2637, -4.2018,  ..., -3.6096, -3.7438, -4.2571]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.4178631901741028\n",
      "Masked LM loss: 4.42774772644043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2: 100%|| 1/1 [00:00<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 0, 'avg_loss': 4.845611095428467, 'avg_acc': 50.0, 'loss': 4.845611095428467}\n",
      "EP2_train, avg_loss= 4.845611095428467 total_acc= 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:3:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1])}\n",
      "Next sentence output: tensor([[-1.1593, -0.3764],\n",
      "        [-0.5797, -0.8212],\n",
      "        [-1.2384, -0.3423],\n",
      "        [-1.3506, -0.2999],\n",
      "        [-1.0409, -0.4356],\n",
      "        [-0.9701, -0.4765],\n",
      "        [-1.2556, -0.3353],\n",
      "        [-1.1428, -0.3841],\n",
      "        [-1.0017, -0.4577],\n",
      "        [-1.1931, -0.3614],\n",
      "        [-1.0059, -0.4553],\n",
      "        [-1.2485, -0.3382],\n",
      "        [-1.0062, -0.4551],\n",
      "        [-1.1214, -0.3942]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-3.4477, -5.4563, -4.3404,  ..., -4.3109, -4.6805, -3.8532],\n",
      "         [-4.0329, -5.3247, -4.8405,  ..., -4.1691, -4.3169, -4.7624],\n",
      "         [-3.8536, -4.9707, -3.7894,  ..., -3.8112, -5.1680, -4.8092],\n",
      "         ...,\n",
      "         [-4.6654, -5.7226, -4.0753,  ..., -3.5879, -3.8951, -4.5029],\n",
      "         [-4.7100, -4.9605, -3.8338,  ..., -3.7660, -4.3020, -4.5058],\n",
      "         [-5.1518, -5.7669, -3.9910,  ..., -3.2697, -3.8194, -4.5637]],\n",
      "\n",
      "        [[-3.4527, -5.0658, -4.4353,  ..., -4.3774, -4.5736, -3.9714],\n",
      "         [-3.8652, -4.9040, -5.3215,  ..., -3.9363, -4.8947, -5.8442],\n",
      "         [-4.5613, -4.5890, -4.4182,  ..., -4.2464, -4.6283, -4.4186],\n",
      "         ...,\n",
      "         [-5.6202, -5.5389, -4.2531,  ..., -3.6539, -4.1257, -5.2953],\n",
      "         [-5.3625, -5.4031, -4.3320,  ..., -3.7806, -3.9514, -4.9611],\n",
      "         [-5.3029, -5.0669, -4.5827,  ..., -3.5156, -3.6832, -4.6399]],\n",
      "\n",
      "        [[-3.3988, -4.7000, -4.4675,  ..., -4.5236, -4.5772, -4.1152],\n",
      "         [-3.8008, -5.1673, -4.8057,  ..., -4.3168, -4.7194, -5.2258],\n",
      "         [-4.4557, -3.8955, -3.5245,  ..., -3.7888, -4.8713, -5.1407],\n",
      "         ...,\n",
      "         [-4.9506, -5.4725, -3.9617,  ..., -3.7016, -3.8605, -5.4069],\n",
      "         [-5.2436, -5.4550, -4.3682,  ..., -3.6932, -3.6588, -4.9329],\n",
      "         [-5.1156, -5.0521, -4.5980,  ..., -3.5454, -3.7868, -4.7731]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.8536, -4.5420, -3.8884,  ..., -4.1854, -4.7669, -4.2245],\n",
      "         [-4.0520, -4.5952, -4.0712,  ..., -4.6371, -4.6171, -4.0587],\n",
      "         [-3.5565, -4.7791, -5.0049,  ..., -4.0407, -4.5167, -5.2164],\n",
      "         ...,\n",
      "         [-4.8249, -5.0559, -4.4801,  ..., -3.8859, -4.0193, -5.5433],\n",
      "         [-5.3847, -5.1902, -3.9897,  ..., -3.8761, -3.5842, -4.9118],\n",
      "         [-5.8239, -5.1684, -4.2812,  ..., -3.6946, -3.3413, -4.5816]],\n",
      "\n",
      "        [[-3.3203, -4.5094, -4.3667,  ..., -4.0388, -4.5131, -4.7656],\n",
      "         [-3.4892, -5.1722, -5.2564,  ..., -4.5525, -4.9203, -5.3408],\n",
      "         [-4.6566, -5.3281, -5.2721,  ..., -3.6301, -4.8675, -3.8237],\n",
      "         ...,\n",
      "         [-5.1637, -5.5703, -4.3634,  ..., -3.6572, -4.4449, -5.2001],\n",
      "         [-5.3804, -5.5525, -4.8185,  ..., -3.2814, -3.9461, -5.3665],\n",
      "         [-5.1584, -5.4870, -4.5364,  ..., -3.5719, -4.3792, -4.6634]],\n",
      "\n",
      "        [[-3.4699, -5.1456, -4.6550,  ..., -3.7669, -4.7143, -4.1077],\n",
      "         [-4.0637, -5.2833, -4.6820,  ..., -4.0469, -4.3757, -4.7361],\n",
      "         [-4.1633, -5.1066, -4.6434,  ..., -3.9854, -4.6336, -5.0517],\n",
      "         ...,\n",
      "         [-5.4582, -5.0298, -3.8958,  ..., -3.6932, -4.1074, -5.0743],\n",
      "         [-5.4454, -4.8595, -4.2645,  ..., -3.4640, -3.4187, -5.1088],\n",
      "         [-5.0500, -5.1148, -4.1852,  ..., -3.6286, -3.8598, -5.0448]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.388225793838501\n",
      "Masked LM loss: 4.446285247802734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3: 100%|| 1/1 [00:00<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 0, 'avg_loss': 4.834510803222656, 'avg_acc': 50.0, 'loss': 4.834510803222656}\n",
      "EP3_train, avg_loss= 4.834510803222656 total_acc= 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:4:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1])}\n",
      "Next sentence output: tensor([[-1.1949, -0.3606],\n",
      "        [-1.6552, -0.2120],\n",
      "        [-1.4321, -0.2729],\n",
      "        [-0.9400, -0.4953],\n",
      "        [-1.2853, -0.3238],\n",
      "        [-0.8972, -0.5237],\n",
      "        [-1.4680, -0.2619],\n",
      "        [-0.9656, -0.4792],\n",
      "        [-1.4665, -0.2623],\n",
      "        [-1.4521, -0.2667],\n",
      "        [-1.1540, -0.3789],\n",
      "        [-0.6581, -0.7295],\n",
      "        [-0.9202, -0.5082],\n",
      "        [-1.6567, -0.2117]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-3.6764, -5.1482, -4.0391,  ..., -4.0908, -3.4845, -4.7281],\n",
      "         [-3.6883, -4.8201, -5.1228,  ..., -4.3865, -4.6087, -4.8944],\n",
      "         [-3.8237, -4.6272, -4.4658,  ..., -4.0989, -5.1915, -5.1346],\n",
      "         ...,\n",
      "         [-5.1378, -5.2820, -4.4619,  ..., -3.3120, -3.9654, -5.3848],\n",
      "         [-5.0810, -5.3676, -4.7312,  ..., -3.4181, -3.8904, -4.6434],\n",
      "         [-5.2029, -4.9885, -3.9830,  ..., -3.7951, -3.8069, -4.7388]],\n",
      "\n",
      "        [[-3.7907, -4.7114, -4.1551,  ..., -4.4541, -4.9617, -4.0593],\n",
      "         [-4.1620, -4.9054, -5.7208,  ..., -3.9194, -4.7987, -5.4868],\n",
      "         [-4.8290, -4.4637, -4.3667,  ..., -4.4663, -5.0122, -5.2319],\n",
      "         ...,\n",
      "         [-5.4935, -5.1552, -4.5170,  ..., -3.8068, -4.1211, -5.1501],\n",
      "         [-5.4739, -5.4816, -4.4652,  ..., -3.8147, -3.7522, -5.1984],\n",
      "         [-4.9114, -5.6400, -4.4340,  ..., -3.6550, -4.0065, -4.6540]],\n",
      "\n",
      "        [[-3.1513, -5.0130, -4.4916,  ..., -3.9595, -4.3989, -4.2132],\n",
      "         [-3.9048, -5.2861, -5.1031,  ..., -4.8429, -4.4544, -5.1801],\n",
      "         [-4.3081, -4.4857, -4.2961,  ..., -3.7114, -4.3576, -5.1200],\n",
      "         ...,\n",
      "         [-5.4514, -5.8346, -4.3446,  ..., -3.4813, -3.7112, -4.5754],\n",
      "         [-5.2363, -5.4704, -4.4388,  ..., -3.5261, -3.8704, -4.4766],\n",
      "         [-5.0638, -5.3664, -4.6073,  ..., -3.6858, -3.8115, -4.4239]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.7020, -4.8567, -4.2571,  ..., -4.6898, -4.3798, -4.0146],\n",
      "         [-4.4764, -4.3886, -4.3910,  ..., -3.8306, -4.7072, -4.7292],\n",
      "         [-3.5680, -5.0334, -5.1751,  ..., -3.7916, -4.8359, -5.3925],\n",
      "         ...,\n",
      "         [-5.0535, -5.1387, -4.3001,  ..., -3.4584, -4.1442, -5.2844],\n",
      "         [-5.4255, -5.4391, -4.5892,  ..., -3.2609, -3.7660, -4.7609],\n",
      "         [-5.4918, -5.1424, -4.3720,  ..., -4.2485, -3.7028, -4.3601]],\n",
      "\n",
      "        [[-3.3890, -4.5786, -4.0221,  ..., -3.8062, -4.6480, -4.2976],\n",
      "         [-3.7735, -5.3679, -4.9846,  ..., -4.0396, -4.7439, -5.4582],\n",
      "         [-4.2542, -4.7382, -5.2788,  ..., -3.5172, -4.9587, -4.3961],\n",
      "         ...,\n",
      "         [-4.9938, -5.1101, -4.2997,  ..., -3.2715, -4.1223, -5.2720],\n",
      "         [-5.3641, -5.2725, -4.0953,  ..., -3.3126, -3.9701, -4.8106],\n",
      "         [-5.2789, -5.1982, -4.5561,  ..., -3.9533, -3.8634, -4.4359]],\n",
      "\n",
      "        [[-3.4628, -4.4515, -4.3615,  ..., -3.8872, -4.2188, -3.6722],\n",
      "         [-3.8145, -5.3008, -4.8581,  ..., -3.9470, -4.6189, -4.9876],\n",
      "         [-4.6188, -5.2007, -5.3108,  ..., -4.4049, -4.4240, -4.7210],\n",
      "         ...,\n",
      "         [-4.9549, -4.9806, -4.4938,  ..., -3.6013, -3.7053, -4.9486],\n",
      "         [-5.3567, -5.1273, -4.4289,  ..., -3.2766, -3.9124, -5.1119],\n",
      "         [-5.3820, -5.1536, -4.3947,  ..., -3.7573, -3.7375, -4.7344]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.31128978729248047\n",
      "Masked LM loss: 4.4406938552856445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4: 100%|| 1/1 [00:00<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 0, 'avg_loss': 4.751983642578125, 'avg_acc': 64.28571428571429, 'loss': 4.751983642578125}\n",
      "EP4_train, avg_loss= 4.751983642578125 total_acc= 64.28571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:5:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  4, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 6, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1])}\n",
      "Next sentence output: tensor([[-1.3103, -0.3144],\n",
      "        [-1.4302, -0.2735],\n",
      "        [-1.2517, -0.3369],\n",
      "        [-1.2532, -0.3363],\n",
      "        [-1.1129, -0.3984],\n",
      "        [-0.9281, -0.5031],\n",
      "        [-1.1047, -0.4024],\n",
      "        [-1.1168, -0.3965],\n",
      "        [-0.8892, -0.5293],\n",
      "        [-0.9902, -0.4644],\n",
      "        [-0.7589, -0.6315],\n",
      "        [-1.0740, -0.4180],\n",
      "        [-1.1656, -0.3736],\n",
      "        [-1.0798, -0.4150]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-3.8183, -4.9319, -4.8045,  ..., -3.8361, -3.6385, -3.9946],\n",
      "         [-3.8065, -5.2378, -4.9564,  ..., -4.1583, -4.2373, -5.2303],\n",
      "         [-4.3197, -4.6734, -4.4008,  ..., -3.5832, -4.7088, -5.4865],\n",
      "         ...,\n",
      "         [-5.2448, -5.0592, -4.7061,  ..., -3.4097, -3.9310, -5.5244],\n",
      "         [-5.1534, -5.4729, -4.9541,  ..., -3.4890, -3.6375, -5.0192],\n",
      "         [-4.8565, -5.1578, -4.6578,  ..., -3.0621, -3.9328, -4.7045]],\n",
      "\n",
      "        [[-3.7197, -5.0119, -4.3069,  ..., -3.7796, -4.6224, -4.2824],\n",
      "         [-3.8395, -5.0046, -5.5518,  ..., -4.0120, -4.8039, -5.6733],\n",
      "         [-4.6446, -4.3450, -4.0049,  ..., -4.1492, -5.2567, -4.8185],\n",
      "         ...,\n",
      "         [-5.5032, -5.3061, -4.2777,  ..., -3.4344, -3.9780, -5.2101],\n",
      "         [-5.0963, -5.4059, -4.4970,  ..., -3.5080, -4.3183, -4.8355],\n",
      "         [-5.3048, -5.3931, -4.8866,  ..., -3.1345, -3.7831, -5.1035]],\n",
      "\n",
      "        [[-3.4158, -4.7377, -4.4658,  ..., -4.3194, -4.0026, -4.1757],\n",
      "         [-4.1161, -5.3310, -4.8273,  ..., -4.4344, -3.5942, -5.5876],\n",
      "         [-4.6399, -5.1131, -3.9695,  ..., -3.3248, -4.1170, -4.9906],\n",
      "         ...,\n",
      "         [-4.6750, -5.1306, -4.5561,  ..., -3.8746, -3.8314, -5.3158],\n",
      "         [-5.1300, -5.2566, -4.0725,  ..., -3.6797, -3.3316, -5.2669],\n",
      "         [-5.0242, -5.5801, -4.4610,  ..., -3.4874, -3.6616, -4.5360]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.4359, -4.5601, -4.2699,  ..., -4.2025, -4.6849, -4.4155],\n",
      "         [-4.2540, -4.6435, -4.3291,  ..., -4.6433, -4.7156, -4.2204],\n",
      "         [-4.0392, -4.8719, -4.6798,  ..., -4.2128, -4.6265, -5.4880],\n",
      "         ...,\n",
      "         [-5.2338, -5.4128, -4.2801,  ..., -3.5883, -4.3179, -5.2801],\n",
      "         [-5.3879, -5.0399, -4.0932,  ..., -3.8414, -4.6674, -5.0385],\n",
      "         [-5.4409, -5.3136, -4.6785,  ..., -3.5441, -3.7814, -4.6110]],\n",
      "\n",
      "        [[-3.4627, -4.3659, -4.2987,  ..., -3.6794, -4.4722, -4.2368],\n",
      "         [-3.5222, -5.7456, -4.9475,  ..., -4.1703, -4.4449, -5.0228],\n",
      "         [-4.4077, -5.5744, -4.7516,  ..., -3.6605, -4.6286, -3.7942],\n",
      "         ...,\n",
      "         [-5.1662, -5.4687, -4.5115,  ..., -3.5064, -3.9368, -5.0876],\n",
      "         [-5.0158, -5.2382, -4.1553,  ..., -3.7000, -3.8845, -4.7725],\n",
      "         [-4.9669, -5.2332, -4.5389,  ..., -3.1418, -3.7263, -4.3905]],\n",
      "\n",
      "        [[-4.0394, -4.3430, -4.2982,  ..., -4.0824, -4.5890, -3.9322],\n",
      "         [-4.3180, -4.4550, -5.0548,  ..., -4.4245, -4.4588, -4.8717],\n",
      "         [-4.8349, -5.2165, -4.8619,  ..., -4.2926, -4.7223, -4.4021],\n",
      "         ...,\n",
      "         [-5.1050, -5.3161, -4.5121,  ..., -3.4610, -4.0769, -4.8589],\n",
      "         [-5.0314, -4.7632, -4.3108,  ..., -3.8678, -3.8689, -4.7612],\n",
      "         [-5.2991, -5.2141, -4.5394,  ..., -3.8338, -3.9134, -4.5838]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.3951490819454193\n",
      "Masked LM loss: 4.535582542419434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:5: 100%|| 1/1 [00:00<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'iter': 0, 'avg_loss': 4.930731773376465, 'avg_acc': 50.0, 'loss': 4.930731773376465}\n",
      "EP5_train, avg_loss= 4.930731773376465 total_acc= 50.0\n",
      "EP:5 Model Saved on: /root/autodl-tmp/bert/checkpoints/bert_self_trained_ep5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:6:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6,  4,  ...,  0,  0,  0],\n",
      "        [ 3, 75, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[ 0,  0, 38,  ...,  0,  0,  0],\n",
      "        [ 0, 25,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  6,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1])}\n",
      "Next sentence output: tensor([[-1.0484, -0.4316],\n",
      "        [-1.5488, -0.2389],\n",
      "        [-1.1736, -0.3700],\n",
      "        [-0.9048, -0.5186],\n",
      "        [-1.3335, -0.3059],\n",
      "        [-1.4302, -0.2735],\n",
      "        [-1.5826, -0.2300],\n",
      "        [-1.5486, -0.2390],\n",
      "        [-1.2239, -0.3482],\n",
      "        [-1.6135, -0.2221],\n",
      "        [-1.0835, -0.4131],\n",
      "        [-1.2264, -0.3472],\n",
      "        [-1.1461, -0.3825],\n",
      "        [-0.9167, -0.5106]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-3.3421, -4.4138, -3.9794,  ..., -4.1366, -4.6336, -4.2288],\n",
      "         [-4.2081, -4.9182, -5.0998,  ..., -4.1567, -4.6025, -4.8037],\n",
      "         [-4.2081, -4.8269, -4.1086,  ..., -4.0017, -3.7069, -5.0379],\n",
      "         ...,\n",
      "         [-4.9569, -4.8809, -4.2712,  ..., -3.7507, -4.5229, -4.9855],\n",
      "         [-5.0345, -5.3372, -4.1781,  ..., -3.6045, -4.4444, -4.4471],\n",
      "         [-5.1279, -4.9286, -3.8464,  ..., -3.9429, -4.2748, -4.5567]],\n",
      "\n",
      "        [[-3.7144, -4.7706, -4.0386,  ..., -3.9114, -4.7416, -4.3703],\n",
      "         [-4.5983, -5.2095, -4.5733,  ..., -4.5903, -4.7614, -4.3481],\n",
      "         [-4.8853, -4.6125, -3.9389,  ..., -3.6783, -4.5544, -4.0725],\n",
      "         ...,\n",
      "         [-5.4534, -5.3858, -4.2282,  ..., -3.3850, -4.3479, -4.6673],\n",
      "         [-5.1497, -5.2729, -3.7978,  ..., -3.4674, -3.7895, -4.9172],\n",
      "         [-5.2397, -5.3061, -4.2187,  ..., -3.5894, -3.9668, -4.9029]],\n",
      "\n",
      "        [[-3.2300, -4.7602, -4.6473,  ..., -4.3909, -4.4190, -4.2182],\n",
      "         [-3.5698, -5.1193, -5.6554,  ..., -4.0976, -4.4017, -5.4006],\n",
      "         [-4.4815, -4.0855, -4.2880,  ..., -3.6639, -4.4992, -5.2244],\n",
      "         ...,\n",
      "         [-5.2368, -5.4299, -4.4653,  ..., -3.7317, -3.7799, -4.6887],\n",
      "         [-4.9519, -5.1554, -4.2023,  ..., -3.4106, -4.0723, -5.4016],\n",
      "         [-4.8743, -4.8882, -4.0811,  ..., -3.9077, -3.9307, -4.7466]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.2428, -4.7861, -4.2921,  ..., -3.9738, -4.9778, -4.0052],\n",
      "         [-4.2508, -4.7278, -4.7113,  ..., -3.4275, -5.0740, -4.3939],\n",
      "         [-4.4443, -4.3471, -5.3244,  ..., -4.0030, -4.8894, -5.1166],\n",
      "         ...,\n",
      "         [-4.9924, -5.4250, -4.2269,  ..., -3.8187, -4.2688, -5.0743],\n",
      "         [-5.9726, -5.3492, -4.1995,  ..., -3.3754, -4.0557, -5.0024],\n",
      "         [-5.0510, -4.8885, -4.3342,  ..., -3.7856, -3.8510, -4.5350]],\n",
      "\n",
      "        [[-3.3254, -4.5070, -4.5969,  ..., -4.2398, -4.1899, -4.4924],\n",
      "         [-3.7912, -5.1780, -5.0480,  ..., -4.3685, -4.5668, -5.3469],\n",
      "         [-4.6077, -5.2268, -5.3785,  ..., -3.9100, -4.3984, -4.1880],\n",
      "         ...,\n",
      "         [-4.8706, -4.8731, -4.2658,  ..., -3.7352, -4.0491, -5.5245],\n",
      "         [-5.2724, -5.0524, -4.3111,  ..., -3.7110, -4.2019, -4.9887],\n",
      "         [-5.0747, -4.9883, -4.5215,  ..., -3.5477, -3.9160, -4.7424]],\n",
      "\n",
      "        [[-3.4689, -4.6309, -4.5952,  ..., -3.8661, -4.5038, -4.2633],\n",
      "         [-3.6870, -4.9610, -5.3818,  ..., -3.8105, -4.5098, -4.9956],\n",
      "         [-4.4150, -4.9729, -5.1310,  ..., -3.9862, -4.4303, -4.4736],\n",
      "         ...,\n",
      "         [-4.7013, -4.5957, -4.4664,  ..., -3.5034, -3.9762, -5.5799],\n",
      "         [-5.3469, -4.9986, -4.4492,  ..., -3.5544, -4.0618, -4.7699],\n",
      "         [-4.8931, -4.7821, -4.6015,  ..., -3.2671, -4.3731, -4.6882]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.33493027091026306\n",
      "Masked LM loss: 4.335129737854004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:6: 100%|| 1/1 [00:00<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'iter': 0, 'avg_loss': 4.670060157775879, 'avg_acc': 50.0, 'loss': 4.670060157775879}\n",
      "EP6_train, avg_loss= 4.670060157775879 total_acc= 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:7:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3,  4, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 0, 60,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1])}\n",
      "Next sentence output: tensor([[-1.0760, -0.4170],\n",
      "        [-1.1449, -0.3831],\n",
      "        [-1.6117, -0.2226],\n",
      "        [-1.1207, -0.3946],\n",
      "        [-1.1472, -0.3820],\n",
      "        [-1.0442, -0.4338],\n",
      "        [-1.5109, -0.2494],\n",
      "        [-1.4063, -0.2811],\n",
      "        [-1.1940, -0.3610],\n",
      "        [-1.4414, -0.2700],\n",
      "        [-2.1761, -0.1204],\n",
      "        [-1.3065, -0.3157],\n",
      "        [-1.3807, -0.2895],\n",
      "        [-1.0637, -0.4234]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-3.9450, -4.5576, -4.2685,  ..., -3.8304, -4.7251, -4.1965],\n",
      "         [-4.1541, -4.5625, -4.8606,  ..., -4.1285, -4.4148, -5.0046],\n",
      "         [-4.4793, -4.8446, -4.4679,  ..., -3.9579, -4.6979, -5.3582],\n",
      "         ...,\n",
      "         [-5.0703, -5.1018, -4.3651,  ..., -3.4587, -3.8103, -5.2419],\n",
      "         [-4.9865, -4.8161, -4.1908,  ..., -3.9252, -3.9567, -4.8616],\n",
      "         [-4.8970, -5.2610, -4.3763,  ..., -3.9212, -3.7413, -4.5357]],\n",
      "\n",
      "        [[-3.9741, -5.0641, -3.9479,  ..., -3.9928, -4.5131, -4.4267],\n",
      "         [-3.9382, -5.2260, -5.1268,  ..., -3.5837, -4.6047, -5.1941],\n",
      "         [-4.7038, -3.9538, -4.0107,  ..., -3.9410, -5.0504, -5.0337],\n",
      "         ...,\n",
      "         [-5.3724, -5.4052, -4.2653,  ..., -3.5267, -4.0204, -5.5442],\n",
      "         [-5.2064, -5.2650, -4.3890,  ..., -3.7394, -4.0081, -4.5950],\n",
      "         [-5.3516, -5.1856, -4.0085,  ..., -3.3677, -3.6005, -4.8485]],\n",
      "\n",
      "        [[-3.8203, -5.1270, -4.2770,  ..., -4.0931, -4.7902, -3.9398],\n",
      "         [-3.8386, -5.0639, -5.0067,  ..., -4.2757, -4.3178, -5.4414],\n",
      "         [-4.8148, -4.4358, -4.0021,  ..., -3.5946, -4.2140, -4.9268],\n",
      "         ...,\n",
      "         [-5.1585, -5.1435, -4.0525,  ..., -3.8647, -4.2730, -4.9910],\n",
      "         [-5.4572, -5.5717, -4.4974,  ..., -3.4911, -3.7873, -4.9886],\n",
      "         [-5.0146, -5.0265, -3.9851,  ..., -3.6748, -3.8781, -4.6487]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.6644, -4.8151, -4.2016,  ..., -4.5140, -4.3852, -4.1764],\n",
      "         [-4.4654, -5.1236, -4.8297,  ..., -4.2671, -3.8660, -5.2749],\n",
      "         [-4.0094, -4.8348, -5.1319,  ..., -4.3113, -4.8527, -5.7906],\n",
      "         ...,\n",
      "         [-5.8646, -5.2399, -3.9865,  ..., -3.5911, -4.1482, -5.1935],\n",
      "         [-5.0725, -4.9940, -4.3364,  ..., -4.2595, -4.3856, -5.0133],\n",
      "         [-5.0089, -5.1084, -4.0954,  ..., -3.9930, -4.1558, -4.6288]],\n",
      "\n",
      "        [[-3.3821, -4.2180, -4.0739,  ..., -4.5740, -4.2517, -4.3419],\n",
      "         [-4.0073, -4.9303, -4.7258,  ..., -4.2036, -4.6860, -5.1180],\n",
      "         [-4.5849, -5.0191, -4.8477,  ..., -3.7679, -4.4612, -3.9487],\n",
      "         ...,\n",
      "         [-5.0524, -5.2860, -4.1707,  ..., -3.6693, -3.9019, -5.2978],\n",
      "         [-4.8927, -5.2221, -4.4854,  ..., -3.8480, -3.8341, -5.0801],\n",
      "         [-5.3209, -5.7145, -4.0928,  ..., -3.6235, -3.4627, -4.9390]],\n",
      "\n",
      "        [[-3.8801, -4.6280, -4.5870,  ..., -3.8999, -4.5287, -3.8516],\n",
      "         [-4.2673, -5.0816, -4.9218,  ..., -4.3109, -4.5516, -4.7459],\n",
      "         [-5.0089, -5.3732, -5.0322,  ..., -4.2617, -3.9715, -5.0239],\n",
      "         ...,\n",
      "         [-5.6600, -5.3187, -4.5737,  ..., -3.5980, -3.6058, -5.2119],\n",
      "         [-5.2625, -5.1654, -4.1637,  ..., -3.7923, -3.5623, -4.4880],\n",
      "         [-5.3351, -5.0575, -4.4074,  ..., -3.6505, -3.5594, -4.4347]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.30665427446365356\n",
      "Masked LM loss: 4.3262505531311035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:7: 100%|| 1/1 [00:00<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 7, 'iter': 0, 'avg_loss': 4.632905006408691, 'avg_acc': 57.14285714285714, 'loss': 4.632905006408691}\n",
      "EP7_train, avg_loss= 4.632905006408691 total_acc= 57.142857142857146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:8:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1])}\n",
      "Next sentence output: tensor([[-1.1359, -0.3873],\n",
      "        [-1.0604, -0.4251],\n",
      "        [-1.2852, -0.3238],\n",
      "        [-1.6520, -0.2128],\n",
      "        [-1.4925, -0.2547],\n",
      "        [-1.5142, -0.2484],\n",
      "        [-1.2498, -0.3377],\n",
      "        [-1.1125, -0.3986],\n",
      "        [-1.2005, -0.3582],\n",
      "        [-1.5867, -0.2289],\n",
      "        [-1.1318, -0.3893],\n",
      "        [-1.0713, -0.4194],\n",
      "        [-1.2831, -0.3246],\n",
      "        [-0.9693, -0.4770]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-4.2790, -5.0223, -4.7958,  ..., -4.4277, -4.2827, -4.0956],\n",
      "         [-4.0851, -4.7889, -4.9776,  ..., -3.9160, -4.6259, -5.1646],\n",
      "         [-4.3146, -4.7762, -4.3568,  ..., -3.6769, -5.2172, -4.8848],\n",
      "         ...,\n",
      "         [-5.2463, -5.2005, -4.6929,  ..., -3.2482, -4.1661, -4.8073],\n",
      "         [-5.0732, -5.4579, -4.4119,  ..., -3.1491, -4.1719, -4.7554],\n",
      "         [-4.9428, -5.0887, -4.7360,  ..., -3.0453, -3.9719, -4.4919]],\n",
      "\n",
      "        [[-3.2759, -4.8262, -4.5014,  ..., -3.9963, -4.9226, -4.2575],\n",
      "         [-3.6657, -5.1478, -5.6349,  ..., -4.0127, -5.0575, -5.7051],\n",
      "         [-4.9340, -4.5298, -4.4679,  ..., -4.3290, -4.8717, -4.3797],\n",
      "         ...,\n",
      "         [-5.2828, -5.3631, -4.1666,  ..., -3.4300, -4.1183, -5.1268],\n",
      "         [-5.2667, -5.2027, -4.4643,  ..., -3.5049, -3.5993, -4.7743],\n",
      "         [-5.1687, -5.4463, -5.3055,  ..., -3.8704, -4.0596, -4.4940]],\n",
      "\n",
      "        [[-3.5979, -4.8044, -4.3039,  ..., -4.3054, -4.4980, -4.1420],\n",
      "         [-3.7852, -4.9933, -4.9060,  ..., -4.2158, -4.7566, -5.0808],\n",
      "         [-4.8638, -4.4484, -3.7602,  ..., -3.8442, -4.5528, -4.5110],\n",
      "         ...,\n",
      "         [-5.2194, -5.1810, -4.0325,  ..., -3.6068, -4.1641, -5.1992],\n",
      "         [-5.0980, -5.2877, -4.1904,  ..., -3.7200, -4.1632, -5.0680],\n",
      "         [-5.2310, -5.1001, -3.9757,  ..., -3.6133, -3.9793, -4.4705]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.5157, -5.0800, -4.5181,  ..., -4.7195, -4.4453, -3.9848],\n",
      "         [-3.8252, -4.3164, -4.3531,  ..., -4.7848, -4.9840, -4.2379],\n",
      "         [-3.6314, -5.2621, -4.6910,  ..., -4.2351, -4.5444, -5.2478],\n",
      "         ...,\n",
      "         [-5.3541, -5.0630, -4.4264,  ..., -3.8585, -4.2228, -5.5027],\n",
      "         [-5.3992, -5.2120, -4.5632,  ..., -3.9619, -3.8085, -5.1327],\n",
      "         [-5.3560, -5.1329, -4.6713,  ..., -3.9632, -4.0067, -4.3844]],\n",
      "\n",
      "        [[-3.9755, -4.6424, -4.2219,  ..., -3.8531, -4.4475, -4.4351],\n",
      "         [-4.3673, -4.8941, -4.8073,  ..., -3.9841, -4.4667, -5.2575],\n",
      "         [-4.2734, -5.1344, -4.6996,  ..., -3.7562, -4.6886, -4.1364],\n",
      "         ...,\n",
      "         [-5.6487, -5.0709, -4.3836,  ..., -3.3604, -4.0749, -5.1132],\n",
      "         [-5.0210, -5.3546, -3.9196,  ..., -3.6905, -4.1091, -4.5996],\n",
      "         [-4.9676, -4.5374, -3.8993,  ..., -3.6303, -3.8470, -4.6943]],\n",
      "\n",
      "        [[-3.6436, -4.7851, -4.0532,  ..., -4.1923, -4.4971, -4.2464],\n",
      "         [-3.7759, -4.5217, -5.1373,  ..., -4.3010, -4.2735, -4.9250],\n",
      "         [-4.6518, -5.2991, -4.7879,  ..., -3.9128, -4.5734, -4.4208],\n",
      "         ...,\n",
      "         [-4.9083, -5.4856, -4.1716,  ..., -3.2782, -3.8980, -5.0114],\n",
      "         [-5.3062, -4.9028, -4.0074,  ..., -3.6239, -3.8109, -4.6242],\n",
      "         [-5.1224, -4.8639, -4.5245,  ..., -3.6430, -3.4818, -5.0878]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.33302009105682373\n",
      "Masked LM loss: 4.46098518371582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:8: 100%|| 1/1 [00:00<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 8, 'iter': 0, 'avg_loss': 4.794005393981934, 'avg_acc': 57.14285714285714, 'loss': 4.794005393981934}\n",
      "EP8_train, avg_loss= 4.794005393981934 total_acc= 57.142857142857146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:9:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3,  4, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6,  4,  ...,  0,  0,  0],\n",
      "        [ 3, 34,  4,  ...,  0,  0,  0]]), 'bert_label': tensor([[ 0,  0, 38,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  6,  0,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 0, 60,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0, 56,  ...,  0,  0,  0],\n",
      "        [ 0,  6, 27,  ...,  0,  0,  0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0])}\n",
      "Next sentence output: tensor([[-1.0151, -0.4500],\n",
      "        [-1.4506, -0.2671],\n",
      "        [-1.5028, -0.2517],\n",
      "        [-1.6317, -0.2177],\n",
      "        [-1.3388, -0.3040],\n",
      "        [-1.2209, -0.3495],\n",
      "        [-1.6150, -0.2218],\n",
      "        [-1.2454, -0.3394],\n",
      "        [-1.1888, -0.3632],\n",
      "        [-1.3522, -0.2993],\n",
      "        [-1.0493, -0.4311],\n",
      "        [-0.9656, -0.4792],\n",
      "        [-1.6165, -0.2214],\n",
      "        [-1.5034, -0.2515]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-3.5929, -4.9912, -4.1416,  ..., -3.6330, -4.3400, -4.1692],\n",
      "         [-4.1094, -4.8620, -5.0190,  ..., -4.2118, -4.8986, -4.9245],\n",
      "         [-3.7089, -4.9453, -3.7754,  ..., -3.8698, -5.1889, -4.9260],\n",
      "         ...,\n",
      "         [-4.9610, -5.2631, -4.2647,  ..., -3.7448, -4.3505, -5.0868],\n",
      "         [-4.9242, -5.0395, -3.6400,  ..., -3.6291, -4.2197, -4.9372],\n",
      "         [-5.2667, -5.2743, -4.0209,  ..., -3.6106, -4.1546, -4.3492]],\n",
      "\n",
      "        [[-3.8998, -4.9320, -4.3548,  ..., -3.8659, -4.3858, -4.0861],\n",
      "         [-3.9848, -5.0575, -5.5446,  ..., -3.7818, -4.8221, -5.2555],\n",
      "         [-5.0840, -4.7610, -4.1796,  ..., -4.1288, -5.4405, -4.2702],\n",
      "         ...,\n",
      "         [-5.1428, -5.2978, -4.5304,  ..., -3.5718, -3.7624, -5.2918],\n",
      "         [-5.6994, -5.5503, -4.7086,  ..., -3.2881, -3.5591, -4.7903],\n",
      "         [-4.8935, -5.5900, -3.9585,  ..., -3.7849, -4.2972, -4.7662]],\n",
      "\n",
      "        [[-3.0081, -4.9272, -4.1912,  ..., -3.8161, -4.6424, -4.2317],\n",
      "         [-3.7589, -4.7557, -5.3823,  ..., -4.4999, -4.3853, -5.0634],\n",
      "         [-4.8437, -4.3335, -3.6843,  ..., -3.7322, -4.7928, -4.4540],\n",
      "         ...,\n",
      "         [-5.0387, -5.1013, -3.9130,  ..., -3.6616, -4.0647, -4.8688],\n",
      "         [-5.6380, -5.2753, -4.1649,  ..., -3.7169, -3.8621, -4.8793],\n",
      "         [-4.8459, -5.2846, -4.1667,  ..., -3.5262, -3.4236, -4.4430]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.3193, -4.7101, -3.8296,  ..., -4.0671, -3.9348, -3.9243],\n",
      "         [-4.1896, -5.1758, -4.5568,  ..., -4.1183, -3.5266, -5.3955],\n",
      "         [-3.8688, -4.7364, -4.9430,  ..., -3.7988, -5.0781, -5.1554],\n",
      "         ...,\n",
      "         [-4.9827, -5.4639, -4.3830,  ..., -3.9059, -4.0642, -4.7591],\n",
      "         [-5.1726, -5.4489, -3.9726,  ..., -3.6809, -3.6719, -4.9210],\n",
      "         [-5.1016, -5.1143, -4.7975,  ..., -3.2422, -3.8268, -4.6593]],\n",
      "\n",
      "        [[-3.3245, -4.4485, -3.8816,  ..., -3.9411, -4.6158, -4.3255],\n",
      "         [-4.0059, -5.1542, -5.0532,  ..., -4.2432, -4.2725, -4.7693],\n",
      "         [-4.6361, -5.0429, -4.3390,  ..., -4.2243, -4.2231, -5.3457],\n",
      "         ...,\n",
      "         [-5.4405, -5.1856, -4.0434,  ..., -3.6299, -4.1551, -5.3843],\n",
      "         [-5.0840, -5.4129, -4.3292,  ..., -3.7064, -3.4887, -5.2116],\n",
      "         [-5.1352, -5.1155, -3.8370,  ..., -3.7189, -3.4946, -4.8467]],\n",
      "\n",
      "        [[-3.1928, -5.0859, -4.3909,  ..., -3.9808, -4.2535, -4.2445],\n",
      "         [-4.2663, -4.4645, -4.8142,  ..., -4.4338, -3.4328, -5.1653],\n",
      "         [-4.4070, -4.9586, -4.5753,  ..., -3.7836, -3.9072, -5.6708],\n",
      "         ...,\n",
      "         [-5.6288, -5.2533, -4.2379,  ..., -3.7234, -3.8361, -5.1573],\n",
      "         [-5.1447, -5.0846, -4.7058,  ..., -3.2994, -3.5844, -4.9423],\n",
      "         [-5.3285, -5.0577, -4.8162,  ..., -3.3227, -3.8731, -4.7532]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.2661038935184479\n",
      "Masked LM loss: 4.682394027709961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:9: 100%|| 1/1 [00:00<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 9, 'iter': 0, 'avg_loss': 4.948497772216797, 'avg_acc': 42.857142857142854, 'loss': 4.948497772216797}\n",
      "EP9_train, avg_loss= 4.948497772216797 total_acc= 42.857142857142854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:10:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3, 23, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 6, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0])}\n",
      "Next sentence output: tensor([[-1.3035, -0.3169],\n",
      "        [-1.0779, -0.4160],\n",
      "        [-0.9325, -0.5002],\n",
      "        [-1.2843, -0.3241],\n",
      "        [-1.5312, -0.2437],\n",
      "        [-1.5478, -0.2392],\n",
      "        [-1.6515, -0.2129],\n",
      "        [-1.4579, -0.2649],\n",
      "        [-1.2256, -0.3475],\n",
      "        [-1.2908, -0.3217],\n",
      "        [-1.2805, -0.3256],\n",
      "        [-1.1488, -0.3813],\n",
      "        [-1.7703, -0.1867],\n",
      "        [-1.5332, -0.2431]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-3.5994, -4.9894, -4.4005,  ..., -4.4234, -4.6252, -4.2629],\n",
      "         [-3.6673, -5.0730, -5.0782,  ..., -4.1118, -4.2172, -4.9552],\n",
      "         [-4.3293, -4.3767, -4.0979,  ..., -3.7060, -5.2590, -5.1355],\n",
      "         ...,\n",
      "         [-5.2667, -5.1741, -4.5535,  ..., -3.1656, -4.0878, -5.6969],\n",
      "         [-4.9667, -5.3750, -4.1716,  ..., -3.6134, -4.1530, -5.1342],\n",
      "         [-5.2270, -5.1592, -4.2071,  ..., -3.5273, -4.0927, -4.5441]],\n",
      "\n",
      "        [[-3.7977, -5.1209, -4.1261,  ..., -4.0372, -4.5958, -3.9476],\n",
      "         [-4.1217, -5.1547, -5.6437,  ..., -4.2748, -4.5052, -5.6172],\n",
      "         [-4.9231, -4.6947, -3.7725,  ..., -4.3013, -4.7782, -5.0776],\n",
      "         ...,\n",
      "         [-5.6192, -6.1698, -4.3573,  ..., -3.4675, -4.2271, -5.3593],\n",
      "         [-5.2801, -5.7950, -4.4848,  ..., -3.6896, -3.8772, -4.7980],\n",
      "         [-5.4698, -5.4881, -4.1604,  ..., -3.5980, -3.9900, -4.7934]],\n",
      "\n",
      "        [[-3.6062, -4.9794, -4.5938,  ..., -4.4044, -4.6998, -4.0948],\n",
      "         [-4.4459, -5.0655, -4.8881,  ..., -4.3671, -3.6613, -5.7521],\n",
      "         [-4.4356, -4.5374, -4.0967,  ..., -3.5613, -4.2715, -4.5541],\n",
      "         ...,\n",
      "         [-4.9530, -5.0602, -4.5080,  ..., -4.2167, -3.5272, -5.1117],\n",
      "         [-4.9578, -5.2337, -4.2269,  ..., -3.8354, -3.8464, -4.5020],\n",
      "         [-4.8808, -5.3256, -4.5384,  ..., -4.1342, -3.4794, -4.5398]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.6961, -4.8119, -4.2946,  ..., -3.8146, -5.1522, -4.1988],\n",
      "         [-4.2103, -4.7949, -4.2902,  ..., -4.3894, -5.2995, -3.7466],\n",
      "         [-3.9545, -5.0791, -5.0180,  ..., -3.7688, -4.6542, -4.6171],\n",
      "         ...,\n",
      "         [-4.7014, -5.1158, -4.2180,  ..., -3.7544, -3.9386, -5.2151],\n",
      "         [-5.1141, -5.1031, -4.1616,  ..., -3.5348, -4.0587, -5.2262],\n",
      "         [-5.4828, -5.2166, -4.2096,  ..., -3.9265, -3.7970, -4.6710]],\n",
      "\n",
      "        [[-3.6310, -5.0251, -4.1577,  ..., -4.3506, -5.0325, -4.4095],\n",
      "         [-3.9848, -5.0745, -5.1013,  ..., -4.1157, -4.3098, -4.9693],\n",
      "         [-4.3800, -5.0857, -4.6783,  ..., -4.2552, -4.9007, -4.0124],\n",
      "         ...,\n",
      "         [-4.7799, -5.2876, -4.6634,  ..., -3.5989, -3.9307, -5.1820],\n",
      "         [-5.0517, -5.2489, -4.1024,  ..., -3.2603, -4.4586, -5.0510],\n",
      "         [-5.4173, -5.3480, -4.3466,  ..., -3.5797, -3.9952, -4.9173]],\n",
      "\n",
      "        [[-3.5021, -4.3190, -4.3531,  ..., -3.9827, -4.7157, -4.0602],\n",
      "         [-3.7045, -4.9650, -4.8751,  ..., -4.3325, -4.7995, -4.8435],\n",
      "         [-4.3355, -5.1228, -5.2214,  ..., -4.2166, -4.4594, -5.0252],\n",
      "         ...,\n",
      "         [-4.9417, -5.3206, -4.4256,  ..., -3.4111, -3.7794, -5.2770],\n",
      "         [-4.7641, -4.9617, -4.3876,  ..., -3.3736, -4.0608, -5.3095],\n",
      "         [-4.8582, -5.0937, -4.6760,  ..., -3.5445, -4.0067, -4.8641]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.27435746788978577\n",
      "Masked LM loss: 4.413806915283203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:10: 100%|| 1/1 [00:00<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 10, 'iter': 0, 'avg_loss': 4.688164234161377, 'avg_acc': 50.0, 'loss': 4.688164234161377}\n",
      "EP10_train, avg_loss= 4.688164234161377 total_acc= 50.0\n",
      "EP:10 Model Saved on: /root/autodl-tmp/bert/checkpoints/bert_self_trained_ep10.pth\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Start\")\n",
    "for epoch in range(config.epochs):\n",
    "    trainer.train(epoch)\n",
    "    if epoch % config.log_freq == 0:\n",
    "        trainer.save(epoch, config.trained_path)\n",
    "    if test_data_loader is not None:\n",
    "        trainer.test(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
