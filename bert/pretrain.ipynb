{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accroding to original paper <a href=\"https://arxiv.org/abs/1810.04805\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, Bert is pretrained on bookcorpus and wikipedia. More info about bookcorpus and wikipedia can be found at huggingface webpages of [bookcorpus](https://huggingface.co/datasets/bookcorpus/bookcorpus) and [wikipedia](https://huggingface.co/datasets/wikimedia/wikipedia).\n",
    "\n",
    "\n",
    "BERT is pretrained on  Masked Language Model (Mask LM) and Next Sentence Prediction tasks during training (original paper, Section 3.1):\n",
    "\n",
    "1. Masked LM: The training data generator randomly selects 15% of token positions for prediction. If the i-th token is selected, it is replaced with: (1) the [MASK] token 80% of the time, (2) a random token 10% of the time, or (3) the original i-th token 10% of the time. The model then predicts the original token using cross-entropy loss.\n",
    "\n",
    "2. Next Sentence Prediction: This is a binary classification task. When selecting sentences A and B for each pre-training example, B is the actual subsequent sentence following A 50% of the time, and the other 50% of the time, B is a randomly chosen sentence from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T15:51:34.513390Z",
     "start_time": "2025-02-22T15:51:32.960301Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from data import load_data\n",
    "import config\n",
    "\n",
    "tokenizer, bookcorpus_dl = load_data(\"bookcorpus\", loading_ratio=0.1)\n",
    "_, wikipedia_dl = load_data(\"wikipedia\", loading_ratio=0.01)\n",
    "\n",
    "train_dl = itertools.chain(bookcorpus_dl, wikipedia_dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Build Model\n",
    "The key structural difference between Bert and GPT is that Bert does not use a causal mask in its layers. This allows Bert to leverage bidirectional attention, enabling it to capture global dependencies directly. Consequently, Bert's pre-training tasks are fundamentally different from GPT's next-token prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T15:51:51.750244Z",
     "start_time": "2025-02-22T15:51:50.192113Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from modules.bert import BertForPreTraining\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "bert = BertForPreTraining(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    type_vocab_size=2,\n",
    "    hidden_size=config.hidden_size,\n",
    "    max_len=config.max_len,\n",
    "    num_hidden_layers=config.num_layers,\n",
    "    num_attention_heads=config.attention_heads,\n",
    "    intermediate_size=config.intermediate_size,\n",
    "    dropout=config.dropout,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pretrain Model\n",
    "Here I define a BERTTrainer class for pretraing settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T15:51:58.420149Z",
     "start_time": "2025-02-22T15:51:55.218422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 110106428\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from modules import BertForPreTraining\n",
    "\n",
    "class BERTTrainer:\n",
    "    \"\"\"\n",
    "    BERTTrainer make the pretrained BERT model with two LM training method.\n",
    "\n",
    "        1. Masked Language Model : 3.3.1 Task #1: Masked LM\n",
    "        2. Next Sentence prediction : 3.3.2 Task #2: Next Sentence Prediction\n",
    "\n",
    "    please check the details on README.md with simple example.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            bert: BertForPreTraining,\n",
    "            train_dataloader: DataLoader, \n",
    "            test_dataloader: DataLoader = None,\n",
    "            lr: float = 1e-4, \n",
    "            betas=(0.9, 0.999), \n",
    "            weight_decay: float = 0.01, \n",
    "            warmup_steps=10000,\n",
    "            total_steps=1000000,\n",
    "            with_cuda: bool = True, \n",
    "            log_freq: int = 10):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which you want to train\n",
    "        :param train_dataloader: train dataset data loader\n",
    "        :param test_dataloader: test dataset data loader [can be None]\n",
    "        :param lr: learning rate of optimizer\n",
    "        :param betas: Adam optimizer betas\n",
    "        :param weight_decay: Adam optimizer weight decay param\n",
    "        :param with_cuda: traning with cuda\n",
    "        :param log_freq: logging frequency of the batch iteration\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup cuda device for BERT training, argument -c, --cuda should be true\n",
    "        cuda_condition = torch.cuda.is_available() and with_cuda\n",
    "        self.device = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")\n",
    "\n",
    "        # This BERT model will be saved every epoch\n",
    "        self.bert = bert\n",
    "        # Initialize the BERT Language Model, with BERT model\n",
    "        self.model = bert.to(self.device)\n",
    "\n",
    "        # Distributed GPU training if CUDA can detect more than 1 GPU\n",
    "        # if with_cuda and torch.cuda.device_count() > 1:\n",
    "        #     print(\"Using %d GPUS for BERT\" % torch.cuda.device_count())\n",
    "        #     self.model = nn.DataParallel(self.model, device_ids=cuda_devices)\n",
    "\n",
    "        # Setting the train and test data loader\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        # Setting the Adam optimizer with hyper-param\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = nn.NLLLoss(ignore_index=0)\n",
    "\n",
    "        self.log_freq = log_freq\n",
    "\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        \"\"\"\n",
    "        loop over the data_loader for training or testing\n",
    "        if on train status, backward operation is activated\n",
    "        and also auto save the model every epoch\n",
    "\n",
    "        :param epoch: current epoch index\n",
    "        :param data_loader: torch.utils.data.DataLoader for iteration\n",
    "        :param train: boolean value of is train or test\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        str_code = \"train\" if train else \"test\"\n",
    "\n",
    "        # Setting the tqdm progress bar\n",
    "        data_iter = tqdm.tqdm(enumerate(data_loader),\n",
    "                              desc=\"EP_%s:%d\" % (str_code, epoch),\n",
    "                              total=len(data_loader),\n",
    "                              bar_format=\"{l_bar}{r_bar}\")\n",
    "\n",
    "        avg_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_element = 0\n",
    "\n",
    "        for i, data in data_iter:\n",
    "            # get batch data\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            outputs = self.model.forward(\n",
    "                input_ids=data[\"bert_input\"],\n",
    "                attention_mask=data[\"bert_attention_mask\"],\n",
    "                token_type_ids=data[\"segment_label\"],\n",
    "                labels=data[\"bert_label\"],\n",
    "                next_sentence_label=data[\"is_next\"]\n",
    "            )\n",
    "            total_loss, prediction_scores, seq_relationship_score = outputs\n",
    "            # backward and optimization only in train\n",
    "            if train:\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "\n",
    "            # next sentence prediction accuracy\n",
    "            correct = seq_relationship_score.argmax(dim=-1).eq(data[\"is_next\"]).sum().item()\n",
    "            avg_loss += total_loss.item()\n",
    "            total_correct += correct\n",
    "            total_element += data[\"is_next\"].nelement()\n",
    "\n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"avg_loss\": avg_loss / (i + 1),\n",
    "                \"avg_acc\": total_correct / total_element * 100,\n",
    "                \"loss\": total_loss.item()\n",
    "            }\n",
    "\n",
    "            if i % self.log_freq == 0:\n",
    "                data_iter.write(str(post_fix))\n",
    "\n",
    "        print(\"EP%d_%s, avg_loss=\" % (epoch, str_code), avg_loss / len(data_iter), \"total_acc=\",\n",
    "              total_correct * 100.0 / total_element)\n",
    "\n",
    "    def save(self, epoch, file_path):\n",
    "        \"\"\"\n",
    "        Saving the current BERT model on file_path\n",
    "\n",
    "        :param epoch: current epoch number\n",
    "        :param file_path: model output path which gonna be file_path+\"ep%d\" % epoch\n",
    "        :return: final_output_path\n",
    "        \"\"\"\n",
    "        output_path = str(file_path) + \"_ep%d.pth\" % epoch\n",
    "        torch.save(self.bert.cpu(), output_path)\n",
    "        self.bert.to(self.device)\n",
    "        print(\"EP:%d Model Saved on:\" % epoch, output_path)\n",
    "        return output_path\n",
    "    \n",
    "trainer = BERTTrainer(\n",
    "    bert,\n",
    "    train_dataloader=train_data_loader, \n",
    "    test_dataloader=test_data_loader,       \n",
    "    lr=config.learning_rate,     \n",
    "    betas=(config.adam_beta1, config.adam_beta2),\n",
    "    weight_decay=config.adam_weight_decay,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    total_steps = config.total_steps,\n",
    "    with_cuda=config.with_cuda, \n",
    "    log_freq=config.log_freq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train loop\n",
    "Simple test of pretraining process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T15:52:12.199939Z",
     "start_time": "2025-02-22T15:52:02.824802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  27%|| 3/11 [00:01<00:02,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'avg_loss': 11.511968612670898, 'avg_acc': 100.0, 'loss': 11.511968612670898}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  82%|| 9/11 [00:01<00:00,  8.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 5, 'avg_loss': 11.693164825439453, 'avg_acc': 50.0, 'loss': 11.782041549682617}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0: 100%|| 11/11 [00:01<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 10, 'avg_loss': 11.638592720031738, 'avg_acc': 59.09090909090909, 'loss': 11.53420352935791}\n",
      "EP0_train, avg_loss= 11.638592720031738 total_acc= 59.09090909090909\n",
      "EP:0 Model Saved on: /home/bks/lzh/checkpoints/bert_self_trained_ep0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  27%|| 3/11 [00:00<00:00, 11.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 0, 'avg_loss': 11.633398056030273, 'avg_acc': 50.0, 'loss': 11.633398056030273}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  82%|| 9/11 [00:00<00:00, 14.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 5, 'avg_loss': 11.564441521962484, 'avg_acc': 66.66666666666666, 'loss': 11.521570205688477}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1: 100%|| 11/11 [00:00<00:00, 11.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 10, 'avg_loss': 11.480820222334428, 'avg_acc': 54.54545454545454, 'loss': 11.199502944946289}\n",
      "EP1_train, avg_loss= 11.480820222334428 total_acc= 54.54545454545455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:2:  27%|| 3/11 [00:00<00:00, 13.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 0, 'avg_loss': 11.45975399017334, 'avg_acc': 0.0, 'loss': 11.45975399017334}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  82%|| 9/11 [00:00<00:00, 15.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 5, 'avg_loss': 11.160041014353434, 'avg_acc': 50.0, 'loss': 11.001148223876953}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2: 100%|| 11/11 [00:00<00:00, 12.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 10, 'avg_loss': 11.023141774264248, 'avg_acc': 54.54545454545454, 'loss': 10.763666152954102}\n",
      "EP2_train, avg_loss= 11.023141774264248 total_acc= 54.54545454545455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:3:  18%|| 2/11 [00:00<00:00, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 0, 'avg_loss': 10.7632417678833, 'avg_acc': 50.0, 'loss': 10.7632417678833}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  73%|| 8/11 [00:00<00:00, 15.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 5, 'avg_loss': 10.52497657140096, 'avg_acc': 58.333333333333336, 'loss': 10.387877464294434}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3: 100%|| 11/11 [00:00<00:00, 11.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 10, 'avg_loss': 10.382498654452236, 'avg_acc': 45.45454545454545, 'loss': 10.135360717773438}\n",
      "EP3_train, avg_loss= 10.382498654452236 total_acc= 45.45454545454545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:4:  27%|| 3/11 [00:00<00:00, 11.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 0, 'avg_loss': 9.780165672302246, 'avg_acc': 100.0, 'loss': 9.780165672302246}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  82%|| 9/11 [00:00<00:00, 14.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 5, 'avg_loss': 9.686190605163574, 'avg_acc': 75.0, 'loss': 9.539986610412598}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4: 100%|| 11/11 [00:00<00:00, 11.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 10, 'avg_loss': 9.480205015702682, 'avg_acc': 59.09090909090909, 'loss': 9.070290565490723}\n",
      "EP4_train, avg_loss= 9.480205015702682 total_acc= 59.09090909090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Start\")\n",
    "for epoch in range(config.epochs):\n",
    "    trainer.train(epoch)\n",
    "    if epoch % config.log_freq == 0:\n",
    "        trainer.save(epoch, config.trained_path)\n",
    "    if test_data_loader is not None:\n",
    "        trainer.test(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
