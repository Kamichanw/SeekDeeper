{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accroding to original paper <a href=\"https://arxiv.org/abs/1810.04805\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, Bert is pretrained on bookcorpus and wikipedia. More info about bookcorpus and wikipedia can be found at huggingface webpages of [bookcorpus](https://huggingface.co/datasets/bookcorpus/bookcorpus) and [wikipedia](https://huggingface.co/datasets/wikimedia/wikipedia).\n",
    "\n",
    "\n",
    "BERT is pretrained on  Masked Language Model (Mask LM) and Next Sentence Prediction tasks during training (original paper, Section 3.1):\n",
    "\n",
    "1. Masked LM: The training data generator randomly selects 15% of token positions for prediction. If the i-th token is selected, it is replaced with: (1) the [MASK] token 80% of the time, (2) a random token 10% of the time, or (3) the original i-th token 10% of the time. The model then predicts the original token using cross-entropy loss.\n",
    "\n",
    "2. Next Sentence Prediction: This is a binary classification task. When selecting sentences A and B for each pre-training example, B is the actual subsequent sentence following A 50% of the time, and the other 50% of the time, B is a randomly chosen sentence from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:48:40.828659Z",
     "start_time": "2025-02-25T11:48:25.470033Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "from data import load_data\n",
    "import config\n",
    "\n",
    "tokenizer, bookcorpus_dl = load_data(\"bookcorpus\", loading_ratio=0.1)\n",
    "_, wikipedia_dl = load_data(\"wikipedia\", loading_ratio=1 / 41)\n",
    "\n",
    "\n",
    "dataloader_size = len(bookcorpus_dl) + len(wikipedia_dl)\n",
    "print(\"bookcorpus_dl size:\", len(bookcorpus_dl))\n",
    "print(\"wikipedia_dl size:\", len(wikipedia_dl))\n",
    "print(\"Total Dataloader Size:\", dataloader_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Build Model\n",
    "The key structural difference between Bert and GPT is that Bert does not use a causal mask in its layers. This allows Bert to leverage bidirectional attention, enabling it to capture global dependencies directly. Consequently, Bert's pre-training tasks are fundamentally different from GPT's next-token prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:48:49.739059Z",
     "start_time": "2025-02-25T11:48:44.400464Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from modules.bert import BertForPreTraining\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "bert = BertForPreTraining(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    type_vocab_size=2,\n",
    "    hidden_size=config.hidden_size,\n",
    "    max_len=config.max_len,\n",
    "    num_hidden_layers=config.num_layers,\n",
    "    num_attention_heads=config.attention_heads,\n",
    "    intermediate_size=config.intermediate_size,\n",
    "    dropout=config.dropout,\n",
    "    pad_token_idx=tokenizer.pad_token_id,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pretrain Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T11:48:54.300396Z",
     "start_time": "2025-02-25T11:48:54.155875Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def train(epoch, model, optimizer, scheduler):\n",
    "    model.train()\n",
    "    avg_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_element = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    dataloader = itertools.chain(bookcorpus_dl, wikipedia_dl)\n",
    "\n",
    "    for batch in tqdm(\n",
    "        dataloader, desc=f\"Training Epoch {epoch}\", total=dataloader_size\n",
    "    ):\n",
    "        input_ids = batch.input_ids.to(device)\n",
    "        attention_mask = (input_ids != tokenizer.pad_token_id).bool().to(device)\n",
    "        token_type_ids = batch.token_type_ids.to(device)\n",
    "        labels = batch.labels.to(device)\n",
    "        is_next = batch.is_next.to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels,\n",
    "            next_sentence_label=is_next,\n",
    "        )\n",
    "        total_loss, prediction_scores, seq_relationship_score = outputs\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # next sentence prediction accuracy\n",
    "        correct = (\n",
    "            seq_relationship_score.argmax(dim=-1).eq(is_next.squeeze(-1)).sum().item()\n",
    "        )\n",
    "\n",
    "        avg_loss += total_loss.item()\n",
    "        total_correct += correct\n",
    "        total_element += is_next.nelement()\n",
    "\n",
    "    print(\n",
    "        \"EP%d_train, avg_loss=\" % (epoch),\n",
    "        avg_loss / len(dataloader),\n",
    "        \"NSP_acc=\",\n",
    "        total_correct * 100.0 / total_element,\n",
    "    )\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train loop\n",
    "Simple test of pretraining process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "print(\"Training Start\")\n",
    "\n",
    "def training_loop(restore_epoch=-1):\n",
    "    optimizer = AdamW(\n",
    "        bert.parameters(),\n",
    "        lr=config.PretrainConfig.lr,\n",
    "        weight_decay=config.PretrainConfig.weight_decay,\n",
    "    )\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=config.PretrainConfig.warmup_steps,\n",
    "        num_training_steps=dataloader_size * config.PretrainConfig.n_epoch,\n",
    "    )\n",
    "\n",
    "    restore_ckpt_path = config.checkpoint_dir / f\"gpt_{restore_epoch}.pth\"\n",
    "    if restore_epoch != -1 and os.path.exists(restore_ckpt_path):\n",
    "        ckpt = torch.load(restore_ckpt_path)\n",
    "        assert ckpt[\"epoch\"] == restore_epoch\n",
    "        bert.load_state_dict(ckpt[\"model\"])\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
    "    else:\n",
    "        restore_epoch = 0\n",
    "\n",
    "    for epoch in range(restore_epoch, config.PretrainConfig.n_epoch):\n",
    "        avg_train_loss = train(epoch + 1, bert, optimizer, scheduler)\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{config.PretrainConfig.n_epoch}, Training Loss: {avg_train_loss: .4f}\"\n",
    "        )\n",
    "\n",
    "        checkpoint_path = config.checkpoint_dir / f\"bert_{epoch + 1}.pth\"\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model\": bert.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "            },\n",
    "            checkpoint_path,\n",
    "        )\n",
    "\n",
    "\n",
    "training_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
