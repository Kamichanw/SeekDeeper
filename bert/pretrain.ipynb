{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script processes a corpus to prepare data for BERT-style training. The dataset is implemented in the **BERTDataset** class, which reads and tokenizes text, generates masked tokens for training, and creates segment labels. The vocabulary is built using the WordVocab class, which counts word frequencies and assigns token indices.\n",
    "The corpus is first loaded from a text file, where each line consists of two tab-separated sentences. The dataset supports both memory-based and file-streaming modes. The **random_word** function randomly replaces words with mask tokens for masked language modeling(MLM), while **random_sent** generates paired sentences for next-sentence prediction(NSP).\n",
    "The vocabulary is built using a word counter and saved to disk for reuse. In **build()**, the text is preprocessed, and a vocabulary is created based on word frequency thresholds. Finally, the vocabulary is saved for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 90061.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE: 77\n",
      "VOCAB CONTENT: ['<pad>', '<unk>', '<eos>', '<sos>', '<mask>', 'the', 'The', 'bright', 'by', 'book.', 'lazy', 'was', 'a', 'student', 'bird', 'quickly', 'teacher', 'In', 'day,', 'eagerly', 'nervously', 'observed', 'taught', 'teacher.', 'A', 'Bright', 'beautiful', 'calm', 'calmly', 'car', 'cat', 'city', 'city.', 'climbed', 'exciting', 'explore', 'explored', 'mountain.', 'quick', 'read', 'short', 'student.', 'sunny', 'their', 'to', 'After', 'Dark', 'car.', 'cat.', 'dark', 'dog', 'dog.', 'drove', 'jumped', 'learned', 'mountain', 'nervous', 'over', 'slowly', 'tall', 'Beautiful', 'Calm', 'Lazy', 'Quick', 'Quickly', 'cat,', 'driven', 'from', 'happily', 'in', 'learning', 'mountain,', 'quickly.', 'ran', 'ran.', 'teaching', 'they']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "from torch.utils.data import DataLoader\n",
    "import config\n",
    "data.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vocab /root/autodl-tmp/bert/output/vocab\n",
      "Vocab Size:  77\n",
      "Loading Train Dataset /root/autodl-tmp/bert/dataset/corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset: 14it [00:00, 98855.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Size:  14\n",
      "Loading Test Dataset None\n",
      "Creating Dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from data import WordVocab, BERTDataset\n",
    "print(\"Loading Vocab\", config.vocab_path)\n",
    "vocab = WordVocab.load_vocab(config.vocab_path)\n",
    "print(\"Vocab Size: \", len(vocab))\n",
    "\n",
    "print(\"Loading Train Dataset\", config.train_dataset)\n",
    "train_dataset = BERTDataset(config.train_dataset, vocab, seq_len=config.sequence_length,\n",
    "                            corpus_lines=config.corpus_lines, on_memory=config.on_memory)\n",
    "print(\"Train Dataset Size: \", len(train_dataset))\n",
    "\n",
    "print(\"Loading Test Dataset\", config.test_dataset)\n",
    "test_dataset = BERTDataset(config.test_dataset, vocab, seq_len=config.sequence_length, on_memory=config.on_memory) \\\n",
    "    if config.test_dataset is not None else None\n",
    "\n",
    "print(\"Creating Dataloader\")\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=config.num_workers)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=config.batch_size, num_workers=config.num_workers) \\\n",
    "    if test_dataset is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BERT model\n",
      "BERT(\n",
      "  (embedding): BERTEmbedding(\n",
      "    (token): TokenEmbedding(77, 768, padding_idx=0)\n",
      "    (position): PositionalEmbedding(512, 768)\n",
      "    (segment): SegmentEmbedding(2, 768, padding_idx=0)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_blocks): ModuleList(\n",
      "    (0): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from modules.bert import BERT\n",
    "from modules.pretrain import BERTTrainer\n",
    "print(\"Building BERT model\")\n",
    "bert = BERT(\n",
    "    len(vocab), \n",
    "    hidden=config.hidden_size, \n",
    "    n_layers=config.num_layers, \n",
    "    attn_heads=config.attention_heads\n",
    ")\n",
    "print(bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model\n",
    "Creating BERT Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 85569103\n"
     ]
    }
   ],
   "source": [
    "trainer = BERTTrainer(\n",
    "    bert, len(vocab), \n",
    "    train_dataloader=train_data_loader, \n",
    "    test_dataloader=test_data_loader,       \n",
    "    lr=config.learning_rate,     \n",
    "    betas=(config.adam_beta1, config.adam_beta2),\n",
    "    weight_decay=config.adam_weight_decay,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    total_steps = config.total_steps,\n",
    "    with_cuda=config.with_cuda, \n",
    "    cuda_devices=config.cuda_devices, \n",
    "    log_freq=config.log_freq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Optimizer and Scheduler\n",
    "ScheduledOptim class is a wrapper for an optimizer that implements a learning rate scheduling strategy inspired by the Transformer paper (Attention Is All You Need). It adjusts the learning rate using a warm-up and decay mechanism to stabilize training. \n",
    "\n",
    "Original paper shows that they use Adam with learning rate of 1e-4, β1 = 0.9, β2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps, and linear decay of the learning rate in the pretraining\n",
    "\n",
    "Check modules/optim_schedule.py for deeper understanding.\n",
    "\n",
    "The learing rate schedule is shown as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "from modules.optim_schedule import ScheduledOptim\n",
    "import config\n",
    "\n",
    "#random create one model\n",
    "test_model = torch.nn.Linear(10, 1)\n",
    "\n",
    "\n",
    "optimizer = Adam(test_model.parameters(), lr=config.learning_rate, betas=(config.adam_beta1, config.adam_beta2), weight_decay=config.adam_weight_decay)\n",
    "\n",
    "lr_values = []\n",
    "\n",
    "scheduler = ScheduledOptim(optimizer,config.warmup_steps,config.total_steps)\n",
    "\n",
    "for _ in range(config.total_steps):\n",
    "    \n",
    "    # update lr\n",
    "    scheduler.step_and_update_lr()\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    lr_values.append(lr)\n",
    "    \n",
    "\n",
    "# 绘制学习率曲线\n",
    "plt.plot(lr_values)\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train loop\n",
    "Simple test of pretraining process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6,  4,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[ 0,  0, 38,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0])}\n",
      "Next sentence output: tensor([[-0.8047, -0.5928],\n",
      "        [-0.5485, -0.8623],\n",
      "        [-0.7184, -0.6685],\n",
      "        [-0.5492, -0.8613],\n",
      "        [-0.4566, -1.0036],\n",
      "        [-0.8399, -0.5652],\n",
      "        [-0.4441, -1.0255],\n",
      "        [-0.3626, -1.1903],\n",
      "        [-0.4876, -0.9521],\n",
      "        [-0.8234, -0.5779],\n",
      "        [-0.4733, -0.9754],\n",
      "        [-0.7558, -0.6342],\n",
      "        [-0.3741, -1.1644],\n",
      "        [-0.8960, -0.5246]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-4.7536, -3.6578, -4.6697,  ..., -3.6961, -3.1597, -3.8831],\n",
      "         [-4.4195, -4.9557, -4.7841,  ..., -5.0809, -3.8363, -4.5228],\n",
      "         [-3.8929, -5.4961, -4.5773,  ..., -4.8173, -4.1710, -4.2231],\n",
      "         ...,\n",
      "         [-5.1112, -4.4475, -4.2557,  ..., -4.8230, -3.3429, -2.6713],\n",
      "         [-5.3184, -4.1449, -4.1446,  ..., -4.7268, -3.2765, -2.8583],\n",
      "         [-5.3924, -4.0302, -3.9697,  ..., -4.7067, -3.3787, -2.8305]],\n",
      "\n",
      "        [[-4.2699, -3.8114, -4.3795,  ..., -3.8882, -3.5408, -4.2038],\n",
      "         [-4.6190, -3.9934, -3.9465,  ..., -5.4436, -4.9104, -4.1864],\n",
      "         [-4.2814, -4.2959, -4.4904,  ..., -4.6762, -3.2656, -3.5293],\n",
      "         ...,\n",
      "         [-5.1914, -3.8235, -3.6318,  ..., -5.3855, -3.5144, -2.9881],\n",
      "         [-5.3490, -4.2159, -4.0727,  ..., -5.0779, -3.5108, -3.0137],\n",
      "         [-5.1201, -4.2937, -4.0491,  ..., -4.8900, -3.3513, -3.1612]],\n",
      "\n",
      "        [[-4.8678, -4.2669, -4.4102,  ..., -3.9149, -2.8366, -4.3717],\n",
      "         [-4.7005, -4.6847, -4.7463,  ..., -4.9313, -3.4761, -4.0593],\n",
      "         [-4.9622, -4.8236, -3.9646,  ..., -4.8231, -3.5383, -3.5555],\n",
      "         ...,\n",
      "         [-4.9172, -4.6101, -4.2433,  ..., -5.2286, -3.5006, -3.5684],\n",
      "         [-4.5994, -4.4561, -4.2586,  ..., -4.5637, -3.7007, -3.0333],\n",
      "         [-4.8371, -4.7138, -3.8069,  ..., -4.4886, -3.6667, -3.0668]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.0153, -3.5714, -4.5593,  ..., -4.3330, -3.4309, -3.9955],\n",
      "         [-3.6114, -4.3137, -4.4541,  ..., -5.1938, -3.7451, -3.7601],\n",
      "         [-4.4419, -4.3512, -3.3740,  ..., -5.5139, -3.6335, -3.0925],\n",
      "         ...,\n",
      "         [-4.8337, -4.5016, -4.1364,  ..., -4.6267, -3.4070, -3.3250],\n",
      "         [-4.7633, -4.8923, -4.4297,  ..., -4.6573, -3.5199, -3.1423],\n",
      "         [-5.1057, -4.2335, -3.9328,  ..., -4.7345, -3.6092, -3.1915]],\n",
      "\n",
      "        [[-4.6819, -3.7306, -4.6839,  ..., -4.1775, -2.9576, -3.9764],\n",
      "         [-4.5799, -5.0332, -4.6660,  ..., -5.0322, -4.0710, -4.2252],\n",
      "         [-4.7283, -4.5538, -4.2956,  ..., -4.3212, -3.0832, -2.6913],\n",
      "         ...,\n",
      "         [-5.2703, -4.4289, -3.5293,  ..., -5.3657, -3.4621, -3.4120],\n",
      "         [-5.1636, -4.1203, -3.9497,  ..., -4.7034, -3.4178, -3.1636],\n",
      "         [-4.8618, -4.5584, -3.6600,  ..., -5.0145, -3.7739, -3.0387]],\n",
      "\n",
      "        [[-4.2435, -4.0970, -4.5105,  ..., -4.4437, -3.4093, -4.4880],\n",
      "         [-4.7855, -4.8761, -4.4524,  ..., -4.9092, -3.5780, -3.9888],\n",
      "         [-3.6930, -4.5652, -5.4737,  ..., -4.7646, -3.7451, -4.0152],\n",
      "         ...,\n",
      "         [-5.1905, -3.9160, -3.9263,  ..., -5.2212, -3.4115, -3.2095],\n",
      "         [-5.3101, -4.3264, -3.8468,  ..., -5.0823, -3.1559, -2.8912],\n",
      "         [-4.9596, -4.0565, -4.3633,  ..., -5.1084, -3.2727, -2.9475]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.8691140413284302\n",
      "Masked LM loss: 4.390117168426514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0: 100%|| 1/1 [00:00<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'avg_loss': 5.259231090545654, 'avg_acc': 35.714285714285715, 'loss': 5.259231090545654}\n",
      "EP0_train, avg_loss= 5.259231090545654 total_acc= 35.714285714285715\n",
      "EP:0 Model Saved on: /root/autodl-tmp/bert/checkpoints/bert_self_trained_ep0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  4, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 6, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0])}\n",
      "Next sentence output: tensor([[-0.7325, -0.6553],\n",
      "        [-0.5293, -0.8892],\n",
      "        [-0.7964, -0.5996],\n",
      "        [-0.6930, -0.6933],\n",
      "        [-0.4150, -1.0798],\n",
      "        [-0.6929, -0.6934],\n",
      "        [-0.8062, -0.5916],\n",
      "        [-0.5448, -0.8674],\n",
      "        [-0.4384, -1.0357],\n",
      "        [-0.8901, -0.5287],\n",
      "        [-0.7846, -0.6094],\n",
      "        [-0.7590, -0.6313],\n",
      "        [-0.2821, -1.4032],\n",
      "        [-0.8983, -0.5230]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-4.5556, -3.9984, -4.9396,  ..., -4.1081, -3.5727, -4.1445],\n",
      "         [-4.7600, -5.1643, -4.9716,  ..., -4.6250, -3.8825, -3.9442],\n",
      "         [-4.0421, -5.3833, -4.6440,  ..., -4.6723, -4.3577, -4.9615],\n",
      "         ...,\n",
      "         [-5.0231, -4.3395, -4.2456,  ..., -5.3072, -3.2583, -3.4450],\n",
      "         [-4.9530, -4.3146, -4.0531,  ..., -4.7495, -3.7496, -2.9809],\n",
      "         [-5.0401, -4.2326, -4.2196,  ..., -4.8642, -3.2691, -2.5692]],\n",
      "\n",
      "        [[-4.4608, -4.2634, -4.5967,  ..., -3.8648, -3.8148, -3.8363],\n",
      "         [-5.0621, -4.5655, -3.7176,  ..., -5.3181, -4.7949, -4.4365],\n",
      "         [-4.8960, -4.4223, -3.7802,  ..., -4.8275, -3.3157, -3.5852],\n",
      "         ...,\n",
      "         [-5.2503, -4.0576, -4.0028,  ..., -4.6930, -3.5755, -3.2650],\n",
      "         [-5.0313, -4.3920, -3.9755,  ..., -4.5189, -3.6596, -3.6867],\n",
      "         [-5.3010, -4.3824, -3.7354,  ..., -4.4600, -3.4450, -3.3372]],\n",
      "\n",
      "        [[-4.2853, -3.9027, -4.8160,  ..., -4.2937, -3.4123, -3.8625],\n",
      "         [-4.7550, -4.9741, -4.6383,  ..., -4.9510, -3.7844, -4.1826],\n",
      "         [-4.6328, -4.1225, -3.5073,  ..., -4.5721, -3.7198, -3.7134],\n",
      "         ...,\n",
      "         [-5.1210, -3.8409, -3.9334,  ..., -5.0679, -3.5717, -3.2859],\n",
      "         [-5.3280, -4.3119, -4.2266,  ..., -5.1884, -3.5839, -2.8094],\n",
      "         [-4.8078, -4.1851, -3.8768,  ..., -4.7663, -3.5157, -3.2332]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.2952, -4.1580, -4.4716,  ..., -3.3294, -3.3948, -4.3891],\n",
      "         [-3.7074, -4.2585, -3.8242,  ..., -4.7887, -3.7552, -3.4433],\n",
      "         [-4.3394, -4.2591, -3.4606,  ..., -4.9800, -3.8358, -3.5944],\n",
      "         ...,\n",
      "         [-4.7834, -4.5905, -4.4199,  ..., -4.7854, -3.2292, -3.3745],\n",
      "         [-5.3512, -4.3981, -3.8936,  ..., -4.6551, -3.1240, -3.4721],\n",
      "         [-4.8417, -4.3184, -3.9145,  ..., -4.8690, -3.3893, -3.5461]],\n",
      "\n",
      "        [[-4.2643, -3.9514, -4.7542,  ..., -4.1952, -3.7704, -3.7928],\n",
      "         [-5.0105, -4.8245, -4.2149,  ..., -4.8353, -3.5928, -3.6842],\n",
      "         [-4.9036, -4.3415, -4.5284,  ..., -4.5233, -3.2661, -2.9912],\n",
      "         ...,\n",
      "         [-5.0319, -4.4848, -3.8721,  ..., -5.1365, -3.3596, -3.6674],\n",
      "         [-5.0044, -4.3965, -3.3563,  ..., -4.8357, -3.8806, -3.0311],\n",
      "         [-4.7593, -4.2803, -3.8158,  ..., -4.9289, -3.4329, -3.2524]],\n",
      "\n",
      "        [[-4.6086, -3.7630, -4.6656,  ..., -3.7833, -3.9699, -4.2365],\n",
      "         [-5.2230, -4.6167, -4.5495,  ..., -4.7765, -4.0020, -4.2296],\n",
      "         [-4.2715, -4.7565, -5.8275,  ..., -4.3030, -4.0817, -3.8299],\n",
      "         ...,\n",
      "         [-5.5684, -4.0865, -4.1378,  ..., -4.5726, -3.9376, -2.8836],\n",
      "         [-5.1421, -4.7231, -4.5368,  ..., -4.7990, -3.5917, -3.4618],\n",
      "         [-5.3377, -4.7657, -4.5156,  ..., -5.0724, -3.8416, -2.8912]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.8406245708465576\n",
      "Masked LM loss: 4.5682759284973145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1: 100%|| 1/1 [00:00<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 0, 'avg_loss': 5.408900260925293, 'avg_acc': 50.0, 'loss': 5.408900260925293}\n",
      "EP1_train, avg_loss= 5.408900260925293 total_acc= 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:2:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  4, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  4, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60,  4,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  4,  4,  ...,  0,  0,  0]]), 'bert_label': tensor([[ 0,  6,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  6,  0,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 0,  0, 31,  ...,  0,  0,  0],\n",
      "        [ 0,  6,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  6, 27,  ...,  0,  0,  0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1])}\n",
      "Next sentence output: tensor([[-1.3514, -0.2996],\n",
      "        [-0.5257, -0.8944],\n",
      "        [-1.0414, -0.4354],\n",
      "        [-0.4894, -0.9492],\n",
      "        [-0.6447, -0.7440],\n",
      "        [-0.7471, -0.6420],\n",
      "        [-0.6322, -0.7580],\n",
      "        [-0.6780, -0.7085],\n",
      "        [-0.8886, -0.5297],\n",
      "        [-0.8440, -0.5621],\n",
      "        [-0.4173, -1.0753],\n",
      "        [-0.4529, -1.0099],\n",
      "        [-0.3614, -1.1931],\n",
      "        [-0.8400, -0.5651]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-4.4622, -3.7102, -4.5621,  ..., -4.0785, -3.4441, -3.7057],\n",
      "         [-5.2108, -4.5996, -4.7359,  ..., -5.1013, -3.9517, -3.6330],\n",
      "         [-4.2854, -5.0934, -4.3912,  ..., -5.0161, -4.1231, -4.5288],\n",
      "         ...,\n",
      "         [-5.2616, -4.6444, -4.2111,  ..., -4.8190, -3.6352, -3.0807],\n",
      "         [-4.8794, -4.5826, -4.5655,  ..., -4.4261, -3.4716, -3.1111],\n",
      "         [-4.7410, -4.2401, -4.7149,  ..., -4.9304, -3.6466, -3.4087]],\n",
      "\n",
      "        [[-4.1759, -3.7548, -4.3510,  ..., -3.8872, -3.0694, -4.2014],\n",
      "         [-4.7265, -4.4598, -3.7732,  ..., -5.2662, -4.6652, -4.3146],\n",
      "         [-4.4658, -4.4353, -4.1879,  ..., -5.0366, -3.3610, -3.8376],\n",
      "         ...,\n",
      "         [-5.1386, -4.4566, -4.1800,  ..., -4.9336, -3.8262, -2.8728],\n",
      "         [-5.3145, -4.5083, -3.8408,  ..., -5.2767, -4.0361, -3.5061],\n",
      "         [-5.3375, -4.1866, -3.7709,  ..., -5.2246, -3.5577, -3.3981]],\n",
      "\n",
      "        [[-4.5579, -3.9793, -4.6860,  ..., -3.7583, -3.1149, -4.2541],\n",
      "         [-5.0446, -5.4310, -4.3137,  ..., -4.7874, -3.9700, -4.1390],\n",
      "         [-4.7772, -4.1110, -4.0426,  ..., -4.5762, -3.7124, -3.9230],\n",
      "         ...,\n",
      "         [-4.9105, -4.8884, -3.9058,  ..., -4.9428, -3.8278, -3.3483],\n",
      "         [-5.2223, -4.4408, -4.0922,  ..., -4.7364, -3.5737, -3.8104],\n",
      "         [-4.9928, -4.2453, -4.3046,  ..., -5.1088, -3.4126, -4.0795]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.6444, -3.9645, -4.6136,  ..., -3.7471, -3.2155, -4.5276],\n",
      "         [-4.2424, -4.3454, -4.0674,  ..., -4.9789, -3.3993, -4.0994],\n",
      "         [-5.2657, -4.8820, -3.9436,  ..., -5.3565, -3.0249, -3.8197],\n",
      "         ...,\n",
      "         [-5.1314, -4.2202, -4.4625,  ..., -4.6534, -2.7749, -3.1406],\n",
      "         [-5.2173, -4.3056, -3.9609,  ..., -4.7254, -2.9457, -3.2553],\n",
      "         [-5.0942, -4.3537, -4.2450,  ..., -4.8022, -3.4002, -3.3872]],\n",
      "\n",
      "        [[-4.8545, -4.0544, -4.5505,  ..., -4.2095, -3.0240, -4.1017],\n",
      "         [-5.0701, -4.7688, -4.3022,  ..., -4.9664, -4.3774, -4.3654],\n",
      "         [-4.6197, -4.3562, -5.0880,  ..., -4.5651, -3.6056, -2.8111],\n",
      "         ...,\n",
      "         [-5.4568, -4.1353, -3.9774,  ..., -4.6545, -3.7475, -3.2817],\n",
      "         [-5.5415, -4.0114, -4.0730,  ..., -4.8404, -3.5552, -3.2393],\n",
      "         [-5.5287, -4.1876, -3.8961,  ..., -4.1440, -3.3143, -3.4776]],\n",
      "\n",
      "        [[-4.8547, -3.9338, -4.6038,  ..., -4.1865, -3.3239, -3.6976],\n",
      "         [-5.4405, -4.7804, -4.4076,  ..., -4.6440, -3.9687, -3.7732],\n",
      "         [-5.2913, -4.6835, -4.1447,  ..., -4.8966, -3.8179, -3.8845],\n",
      "         ...,\n",
      "         [-5.4418, -4.5204, -4.0672,  ..., -4.8567, -3.4293, -3.5331],\n",
      "         [-5.1074, -4.5080, -4.3961,  ..., -4.7245, -3.2234, -3.4437],\n",
      "         [-5.0853, -4.4539, -3.9671,  ..., -5.4777, -3.2709, -2.8873]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.7182696461677551\n",
      "Masked LM loss: 4.675418376922607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2: 100%|| 1/1 [00:00<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 0, 'avg_loss': 5.393688201904297, 'avg_acc': 42.857142857142854, 'loss': 5.393688201904297}\n",
      "EP2_train, avg_loss= 5.393688201904297 total_acc= 42.857142857142854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:3:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  4, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[0, 6, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0])}\n",
      "Next sentence output: tensor([[-0.6824, -0.7040],\n",
      "        [-0.7654, -0.6258],\n",
      "        [-0.7024, -0.6840],\n",
      "        [-0.9182, -0.5096],\n",
      "        [-0.8520, -0.5561],\n",
      "        [-0.5036, -0.9273],\n",
      "        [-1.1526, -0.3795],\n",
      "        [-0.6809, -0.7056],\n",
      "        [-0.9732, -0.4746],\n",
      "        [-0.4605, -0.9968],\n",
      "        [-0.8357, -0.5684],\n",
      "        [-0.8602, -0.5501],\n",
      "        [-0.6115, -0.7821],\n",
      "        [-0.9561, -0.4852]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-4.5649, -3.7945, -4.3856,  ..., -4.2073, -3.5084, -4.1389],\n",
      "         [-4.9310, -4.9586, -4.3691,  ..., -4.9825, -3.7020, -3.1727],\n",
      "         [-4.7170, -5.1355, -4.6321,  ..., -4.9245, -4.2374, -4.0549],\n",
      "         ...,\n",
      "         [-5.1143, -4.7648, -4.0762,  ..., -4.9299, -3.4020, -2.7748],\n",
      "         [-5.1167, -4.2296, -4.3031,  ..., -4.6508, -3.4646, -3.2931],\n",
      "         [-5.1462, -4.5685, -3.9430,  ..., -4.9678, -3.2530, -2.7139]],\n",
      "\n",
      "        [[-4.7157, -4.2266, -4.1830,  ..., -3.7870, -3.1162, -4.0582],\n",
      "         [-4.8410, -4.2636, -3.9154,  ..., -5.2255, -4.5479, -4.1544],\n",
      "         [-4.6568, -4.8245, -4.3009,  ..., -4.6605, -3.2730, -3.7871],\n",
      "         ...,\n",
      "         [-5.4375, -4.5598, -4.2199,  ..., -4.9576, -3.3441, -3.2901],\n",
      "         [-5.5464, -4.4217, -4.3353,  ..., -4.6471, -3.4567, -3.3875],\n",
      "         [-5.2687, -4.8155, -3.9485,  ..., -4.6642, -3.3453, -3.4034]],\n",
      "\n",
      "        [[-4.1569, -4.0743, -4.4723,  ..., -4.0109, -3.1572, -4.2447],\n",
      "         [-5.1716, -5.0207, -4.6726,  ..., -4.5169, -3.7115, -4.5361],\n",
      "         [-4.7165, -4.1625, -3.8856,  ..., -4.7369, -3.6061, -3.9535],\n",
      "         ...,\n",
      "         [-5.1378, -4.5635, -4.0485,  ..., -4.1290, -2.8263, -3.1937],\n",
      "         [-5.0242, -4.8993, -4.3604,  ..., -4.9861, -3.2969, -3.0125],\n",
      "         [-5.3200, -5.1393, -4.0393,  ..., -4.7677, -3.0398, -3.3553]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.0216, -3.9901, -4.3118,  ..., -3.9432, -3.4686, -4.2824],\n",
      "         [-4.0315, -3.7884, -4.1965,  ..., -5.0382, -3.8833, -4.0228],\n",
      "         [-4.1360, -4.5958, -3.0414,  ..., -5.6396, -3.8041, -3.8623],\n",
      "         ...,\n",
      "         [-4.4704, -4.3417, -3.9632,  ..., -4.6230, -3.4126, -3.3705],\n",
      "         [-4.8334, -4.6432, -3.9091,  ..., -4.8596, -3.3797, -3.7399],\n",
      "         [-4.6270, -4.4659, -4.0878,  ..., -4.1748, -3.1250, -3.7657]],\n",
      "\n",
      "        [[-4.0829, -4.1880, -4.5797,  ..., -4.3033, -3.0381, -4.2314],\n",
      "         [-4.5872, -5.2422, -4.9190,  ..., -5.3117, -3.7376, -4.3152],\n",
      "         [-4.3825, -4.6433, -4.9328,  ..., -4.7049, -3.5197, -2.8529],\n",
      "         ...,\n",
      "         [-5.5512, -4.1429, -3.6259,  ..., -4.9850, -3.5112, -3.3091],\n",
      "         [-5.3938, -4.2482, -3.4222,  ..., -5.0888, -3.5866, -3.7199],\n",
      "         [-5.2075, -4.0378, -3.5679,  ..., -5.2075, -3.9092, -3.7529]],\n",
      "\n",
      "        [[-4.4580, -4.0958, -4.3799,  ..., -3.7930, -3.3559, -4.0457],\n",
      "         [-4.6036, -5.0302, -4.9343,  ..., -4.6401, -3.7842, -3.9123],\n",
      "         [-4.1498, -4.7897, -5.8195,  ..., -4.4275, -3.5469, -3.6768],\n",
      "         ...,\n",
      "         [-4.8102, -3.9576, -4.7280,  ..., -4.9679, -3.6627, -3.3560],\n",
      "         [-4.8569, -4.4852, -4.3280,  ..., -4.6993, -3.4548, -3.6700],\n",
      "         [-4.8712, -4.3112, -4.4790,  ..., -4.6245, -3.4705, -3.3207]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.6609982848167419\n",
      "Masked LM loss: 4.596312046051025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3: 100%|| 1/1 [00:00<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 0, 'avg_loss': 5.257310390472412, 'avg_acc': 35.714285714285715, 'loss': 5.257310390472412}\n",
      "EP3_train, avg_loss= 5.257310390472412 total_acc= 35.714285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:4:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6,  4,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0])}\n",
      "Next sentence output: tensor([[-0.7536, -0.6362],\n",
      "        [-0.8765, -0.5383],\n",
      "        [-0.5481, -0.8628],\n",
      "        [-0.6852, -0.7012],\n",
      "        [-0.7353, -0.6527],\n",
      "        [-0.6755, -0.7111],\n",
      "        [-0.6426, -0.7463],\n",
      "        [-0.4850, -0.9563],\n",
      "        [-0.7502, -0.6392],\n",
      "        [-0.3871, -1.1363],\n",
      "        [-0.6033, -0.7918],\n",
      "        [-0.4305, -1.0503],\n",
      "        [-0.6420, -0.7471],\n",
      "        [-0.6829, -0.7035]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-4.2380, -3.8211, -4.8193,  ..., -4.2653, -3.5112, -3.7728],\n",
      "         [-4.3972, -5.2404, -4.8445,  ..., -5.0425, -3.6266, -4.0809],\n",
      "         [-4.2274, -4.9896, -4.7584,  ..., -4.8142, -4.1692, -4.6973],\n",
      "         ...,\n",
      "         [-4.9622, -4.1488, -4.0514,  ..., -4.4051, -3.6363, -3.1288],\n",
      "         [-5.2664, -4.2531, -3.7967,  ..., -4.6190, -3.3726, -3.2329],\n",
      "         [-5.1524, -4.6036, -3.8736,  ..., -4.6554, -3.4256, -3.4176]],\n",
      "\n",
      "        [[-4.8325, -4.1969, -4.5979,  ..., -4.6793, -3.7144, -4.0112],\n",
      "         [-4.7526, -4.2858, -4.0489,  ..., -5.5953, -5.0187, -4.5422],\n",
      "         [-4.6255, -4.6238, -3.9270,  ..., -5.3802, -3.7613, -3.6717],\n",
      "         ...,\n",
      "         [-5.6935, -4.7759, -3.6983,  ..., -5.2406, -3.4360, -3.1410],\n",
      "         [-5.2131, -4.6945, -4.0923,  ..., -4.8320, -3.3077, -2.8558],\n",
      "         [-5.1660, -4.6616, -3.4618,  ..., -5.0957, -3.7323, -3.0666]],\n",
      "\n",
      "        [[-4.1076, -4.0878, -4.5500,  ..., -4.6539, -3.1152, -3.9590],\n",
      "         [-4.6852, -4.6693, -4.7109,  ..., -5.1987, -4.1648, -4.0237],\n",
      "         [-4.8583, -5.0344, -4.3525,  ..., -5.1404, -3.8136, -4.0225],\n",
      "         ...,\n",
      "         [-4.3212, -4.6140, -4.1697,  ..., -4.9908, -3.6001, -3.0616],\n",
      "         [-4.6218, -4.9074, -4.3300,  ..., -4.7787, -3.6733, -3.4612],\n",
      "         [-5.0150, -4.4183, -4.1896,  ..., -4.8815, -3.3228, -3.4248]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.9681, -4.1866, -4.7132,  ..., -4.0533, -3.3175, -4.4104],\n",
      "         [-3.7447, -4.3734, -4.0224,  ..., -5.3039, -3.7447, -3.9069],\n",
      "         [-4.1602, -4.5894, -3.3274,  ..., -5.3019, -4.2149, -3.5854],\n",
      "         ...,\n",
      "         [-4.8920, -4.2426, -3.8465,  ..., -5.0845, -3.3978, -3.3837],\n",
      "         [-5.2321, -3.9897, -3.9718,  ..., -4.6403, -3.9514, -3.7776],\n",
      "         [-5.1070, -4.7127, -3.1894,  ..., -4.8486, -3.3602, -3.2318]],\n",
      "\n",
      "        [[-4.6427, -3.7597, -4.2701,  ..., -4.1005, -3.6277, -4.2700],\n",
      "         [-4.6496, -5.2756, -4.4523,  ..., -4.8887, -3.5727, -4.2048],\n",
      "         [-5.0913, -4.5611, -4.5394,  ..., -4.2562, -3.5127, -2.8691],\n",
      "         ...,\n",
      "         [-5.3018, -4.2800, -4.2428,  ..., -4.7841, -3.3015, -3.7281],\n",
      "         [-5.2506, -4.2884, -3.9158,  ..., -4.4535, -3.2776, -3.4620],\n",
      "         [-4.7898, -4.3811, -3.9023,  ..., -4.7823, -3.7661, -3.3662]],\n",
      "\n",
      "        [[-4.7389, -3.7390, -4.5133,  ..., -4.2477, -3.3290, -3.7135],\n",
      "         [-4.4044, -4.6139, -4.5286,  ..., -4.9341, -3.8309, -3.9962],\n",
      "         [-3.6918, -4.6944, -5.1017,  ..., -4.8831, -3.7777, -3.7297],\n",
      "         ...,\n",
      "         [-5.1919, -4.6828, -3.9581,  ..., -4.7669, -3.3011, -3.2047],\n",
      "         [-5.1477, -4.3713, -4.1976,  ..., -4.4930, -3.5479, -2.9739],\n",
      "         [-5.2264, -4.0820, -3.8856,  ..., -5.2248, -3.1406, -3.1238]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.8407074213027954\n",
      "Masked LM loss: 4.532493591308594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4: 100%|| 1/1 [00:00<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 0, 'avg_loss': 5.3732008934021, 'avg_acc': 35.714285714285715, 'loss': 5.3732008934021}\n",
      "EP4_train, avg_loss= 5.3732008934021 total_acc= 35.714285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:5:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3,  4, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  6,  0,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 0, 60,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1])}\n",
      "Next sentence output: tensor([[-0.7828, -0.6108],\n",
      "        [-0.7017, -0.6847],\n",
      "        [-0.7083, -0.6782],\n",
      "        [-0.6904, -0.6959],\n",
      "        [-1.1822, -0.3662],\n",
      "        [-0.9889, -0.4652],\n",
      "        [-0.9583, -0.4838],\n",
      "        [-0.6118, -0.7816],\n",
      "        [-0.7755, -0.6170],\n",
      "        [-0.3956, -1.1186],\n",
      "        [-0.8612, -0.5493],\n",
      "        [-0.8617, -0.5489],\n",
      "        [-0.4918, -0.9455],\n",
      "        [-1.0169, -0.4490]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-4.4646, -4.2110, -4.7407,  ..., -4.1360, -3.9832, -3.9318],\n",
      "         [-4.9006, -4.6811, -5.0348,  ..., -4.4870, -3.8618, -3.7642],\n",
      "         [-3.9822, -5.5207, -4.8843,  ..., -4.6392, -4.0766, -4.0711],\n",
      "         ...,\n",
      "         [-5.3400, -4.5932, -4.1081,  ..., -4.9062, -3.4958, -3.0435],\n",
      "         [-5.3112, -4.6278, -4.5315,  ..., -4.9998, -3.6030, -2.7225],\n",
      "         [-5.2023, -4.1942, -4.4791,  ..., -4.6071, -3.6524, -2.7292]],\n",
      "\n",
      "        [[-3.7482, -3.9568, -4.1646,  ..., -4.5635, -3.4132, -4.4659],\n",
      "         [-4.3384, -4.0898, -4.2581,  ..., -5.5401, -4.8541, -3.9991],\n",
      "         [-4.2885, -4.4460, -3.8722,  ..., -5.1680, -3.4953, -3.7853],\n",
      "         ...,\n",
      "         [-5.0803, -4.4230, -4.1384,  ..., -4.6845, -3.1210, -3.2619],\n",
      "         [-5.4588, -3.7651, -3.7360,  ..., -4.7450, -3.3484, -2.6752],\n",
      "         [-5.7087, -4.3398, -3.9622,  ..., -5.1978, -3.7879, -3.2055]],\n",
      "\n",
      "        [[-4.2372, -3.6944, -4.9612,  ..., -3.9518, -3.6627, -4.3774],\n",
      "         [-4.6981, -5.1424, -4.7863,  ..., -4.8013, -3.7756, -4.4162],\n",
      "         [-4.6900, -3.6796, -4.1081,  ..., -4.3875, -3.7780, -3.7632],\n",
      "         ...,\n",
      "         [-4.7939, -4.6115, -4.4437,  ..., -4.6856, -3.1297, -2.9757],\n",
      "         [-5.0397, -4.5748, -4.3025,  ..., -5.0635, -2.8255, -3.0793],\n",
      "         [-5.1974, -4.6569, -4.1116,  ..., -4.8076, -3.5655, -3.2851]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.4901, -3.6007, -4.3937,  ..., -4.3186, -3.4080, -3.9826],\n",
      "         [-4.9564, -5.0232, -4.4179,  ..., -5.1889, -4.0458, -3.6399],\n",
      "         [-4.2639, -4.4091, -3.0300,  ..., -5.2925, -3.8158, -3.6256],\n",
      "         ...,\n",
      "         [-5.3436, -4.1949, -3.6035,  ..., -4.8347, -3.3978, -3.4623],\n",
      "         [-5.3518, -4.3165, -3.7588,  ..., -4.7705, -3.2414, -3.6050],\n",
      "         [-4.8843, -4.5255, -3.5932,  ..., -4.7490, -3.2564, -3.5026]],\n",
      "\n",
      "        [[-4.3412, -4.1093, -4.6466,  ..., -3.7720, -3.3218, -4.0451],\n",
      "         [-4.3702, -4.7904, -4.5492,  ..., -4.9315, -3.9271, -4.1340],\n",
      "         [-4.5193, -4.4720, -4.6307,  ..., -4.6706, -3.3658, -3.2436],\n",
      "         ...,\n",
      "         [-5.2564, -4.2961, -4.0495,  ..., -4.5925, -3.3049, -3.2307],\n",
      "         [-5.0308, -4.1674, -4.3554,  ..., -4.8286, -3.4310, -3.8528],\n",
      "         [-4.9067, -4.0431, -3.9640,  ..., -4.7725, -3.6693, -3.6465]],\n",
      "\n",
      "        [[-4.6926, -4.1032, -4.7188,  ..., -3.6012, -3.6072, -3.9641],\n",
      "         [-4.4800, -5.0776, -4.7380,  ..., -4.3946, -3.8094, -4.2610],\n",
      "         [-4.0928, -4.5042, -5.5681,  ..., -4.4239, -3.9080, -3.7625],\n",
      "         ...,\n",
      "         [-5.2838, -4.4862, -3.9602,  ..., -4.6644, -2.9802, -3.0461],\n",
      "         [-5.2581, -4.2822, -4.0507,  ..., -4.6448, -3.4671, -3.3693],\n",
      "         [-5.1791, -4.2875, -4.2611,  ..., -4.3974, -3.4328, -3.1826]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.5872510075569153\n",
      "Masked LM loss: 4.4278178215026855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:5: 100%|| 1/1 [00:00<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'iter': 0, 'avg_loss': 5.015069007873535, 'avg_acc': 71.42857142857143, 'loss': 5.015069007873535}\n",
      "EP5_train, avg_loss= 5.015069007873535 total_acc= 71.42857142857143\n",
      "EP:5 Model Saved on: /root/autodl-tmp/bert/checkpoints/bert_self_trained_ep5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:6:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  4, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 6, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1])}\n",
      "Next sentence output: tensor([[-1.0054, -0.4555],\n",
      "        [-0.4926, -0.9443],\n",
      "        [-0.5463, -0.8654],\n",
      "        [-0.5753, -0.8267],\n",
      "        [-0.7355, -0.6525],\n",
      "        [-0.5817, -0.8186],\n",
      "        [-0.4766, -0.9700],\n",
      "        [-0.5670, -0.8376],\n",
      "        [-0.6909, -0.6954],\n",
      "        [-0.7331, -0.6548],\n",
      "        [-0.8066, -0.5912],\n",
      "        [-0.6084, -0.7857],\n",
      "        [-0.7211, -0.6659],\n",
      "        [-0.7475, -0.6416]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-4.1701, -3.7555, -4.5551,  ..., -4.2416, -3.7060, -3.8724],\n",
      "         [-4.8257, -5.1114, -4.8741,  ..., -5.1339, -3.6521, -4.0767],\n",
      "         [-4.2156, -5.4227, -4.5360,  ..., -4.4925, -3.9702, -4.5264],\n",
      "         ...,\n",
      "         [-5.5017, -4.2168, -4.4227,  ..., -4.8758, -3.4626, -2.9213],\n",
      "         [-5.3848, -4.4434, -4.3209,  ..., -4.7272, -3.4140, -2.7782],\n",
      "         [-5.1114, -4.7008, -4.7750,  ..., -4.7182, -3.0777, -3.0904]],\n",
      "\n",
      "        [[-4.5848, -3.7144, -4.4261,  ..., -4.3856, -3.7500, -3.7407],\n",
      "         [-4.7144, -3.9728, -4.0445,  ..., -5.1145, -4.7208, -4.0180],\n",
      "         [-4.4030, -4.1902, -3.8497,  ..., -4.9482, -3.6707, -3.4167],\n",
      "         ...,\n",
      "         [-5.6377, -4.3031, -3.9052,  ..., -4.8758, -3.8258, -2.7498],\n",
      "         [-5.6154, -4.1886, -3.6891,  ..., -5.0138, -3.5388, -3.3894],\n",
      "         [-5.4392, -3.8722, -3.8798,  ..., -4.6556, -3.6632, -3.4907]],\n",
      "\n",
      "        [[-4.0475, -4.1360, -4.6890,  ..., -4.0250, -3.3356, -3.9451],\n",
      "         [-4.9233, -4.4363, -4.5168,  ..., -5.3403, -3.4776, -3.9828],\n",
      "         [-4.2855, -4.0448, -4.1393,  ..., -4.5774, -3.3430, -3.6317],\n",
      "         ...,\n",
      "         [-4.7718, -4.3517, -3.9894,  ..., -5.2218, -3.6484, -3.2252],\n",
      "         [-4.9260, -4.5339, -4.0618,  ..., -4.9072, -3.4443, -3.4702],\n",
      "         [-4.7390, -4.4263, -4.1031,  ..., -4.8417, -3.5582, -3.6899]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.1815, -3.8534, -4.5463,  ..., -3.9564, -3.5340, -3.9207],\n",
      "         [-3.9120, -4.4640, -3.7049,  ..., -4.5748, -3.3715, -4.0154],\n",
      "         [-3.8753, -4.5889, -3.2789,  ..., -5.0975, -4.0706, -3.4998],\n",
      "         ...,\n",
      "         [-4.9944, -4.2194, -3.7020,  ..., -4.8600, -3.5857, -3.2607],\n",
      "         [-4.9253, -4.5851, -3.8916,  ..., -4.4621, -3.5132, -3.6582],\n",
      "         [-5.0170, -4.4887, -3.9637,  ..., -5.1512, -3.5813, -3.2629]],\n",
      "\n",
      "        [[-4.2265, -3.8664, -4.5606,  ..., -3.6984, -4.2683, -3.8959],\n",
      "         [-4.7594, -5.0037, -4.8467,  ..., -4.6162, -4.0626, -4.3586],\n",
      "         [-4.8662, -4.6906, -4.4767,  ..., -4.3066, -3.2997, -3.1472],\n",
      "         ...,\n",
      "         [-4.8307, -4.7000, -3.6862,  ..., -5.0634, -4.1197, -3.6796],\n",
      "         [-5.1905, -4.4009, -4.0381,  ..., -5.0111, -3.8340, -3.3691],\n",
      "         [-5.3528, -4.3522, -3.9863,  ..., -4.7475, -3.5873, -3.8646]],\n",
      "\n",
      "        [[-4.7085, -4.2183, -4.3573,  ..., -4.0607, -3.2168, -3.4773],\n",
      "         [-4.8975, -4.7799, -4.7572,  ..., -4.8874, -3.8343, -4.0541],\n",
      "         [-3.9212, -4.7164, -5.4601,  ..., -4.3847, -3.5585, -3.5917],\n",
      "         ...,\n",
      "         [-5.0248, -4.0562, -3.8474,  ..., -4.9517, -3.1613, -2.7643],\n",
      "         [-6.0646, -4.6291, -4.4305,  ..., -5.0390, -3.3381, -2.8545],\n",
      "         [-5.2343, -4.3582, -4.4617,  ..., -4.7756, -3.5111, -3.5398]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.7229133248329163\n",
      "Masked LM loss: 4.685662269592285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:6: 100%|| 1/1 [00:00<00:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'iter': 0, 'avg_loss': 5.408575534820557, 'avg_acc': 50.0, 'loss': 5.408575534820557}\n",
      "EP6_train, avg_loss= 5.408575534820557 total_acc= 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:7:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 34,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 0,  0, 31,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1])}\n",
      "Next sentence output: tensor([[-0.7640, -0.6270],\n",
      "        [-1.2745, -0.3279],\n",
      "        [-0.8245, -0.5771],\n",
      "        [-0.6118, -0.7818],\n",
      "        [-0.6412, -0.7479],\n",
      "        [-0.7666, -0.6247],\n",
      "        [-0.7079, -0.6786],\n",
      "        [-0.7234, -0.6637],\n",
      "        [-0.7909, -0.6041],\n",
      "        [-1.2131, -0.3528],\n",
      "        [-0.9336, -0.4995],\n",
      "        [-0.6014, -0.7942],\n",
      "        [-0.6667, -0.7203],\n",
      "        [-0.6245, -0.7669]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-3.9955, -3.8266, -4.2821,  ..., -3.9743, -3.5726, -3.6352],\n",
      "         [-4.6548, -5.0103, -4.8283,  ..., -5.1332, -3.9254, -3.7378],\n",
      "         [-4.3045, -5.0366, -4.8471,  ..., -5.1617, -4.1064, -4.5616],\n",
      "         ...,\n",
      "         [-5.4720, -4.2189, -4.3827,  ..., -4.9021, -3.4169, -3.1423],\n",
      "         [-4.7134, -4.2615, -4.0349,  ..., -4.6018, -3.4911, -3.0107],\n",
      "         [-5.2178, -4.0398, -4.2509,  ..., -5.0851, -3.9394, -3.0985]],\n",
      "\n",
      "        [[-4.4259, -4.1423, -4.6933,  ..., -3.5682, -3.7088, -3.9443],\n",
      "         [-4.9580, -4.5211, -4.1130,  ..., -4.9705, -4.7934, -4.3010],\n",
      "         [-4.6943, -4.4042, -4.3540,  ..., -4.7297, -3.3767, -3.1635],\n",
      "         ...,\n",
      "         [-5.5608, -4.8815, -3.9757,  ..., -4.7009, -3.5082, -2.7333],\n",
      "         [-5.7649, -4.3617, -4.1047,  ..., -4.6836, -3.4814, -3.2216],\n",
      "         [-5.3863, -4.5538, -3.9218,  ..., -4.6856, -2.9819, -2.9862]],\n",
      "\n",
      "        [[-4.3883, -3.9197, -4.7622,  ..., -3.9768, -3.1925, -4.3651],\n",
      "         [-4.5916, -5.3980, -4.6067,  ..., -4.9216, -3.6208, -4.2679],\n",
      "         [-4.4886, -4.1846, -3.8224,  ..., -4.2778, -3.6663, -3.6070],\n",
      "         ...,\n",
      "         [-5.3849, -4.8066, -4.3521,  ..., -4.5295, -3.5732, -3.4048],\n",
      "         [-4.9079, -4.6717, -3.9308,  ..., -4.7379, -3.2040, -3.1274],\n",
      "         [-4.4942, -5.0039, -3.9532,  ..., -4.8062, -3.3942, -3.1470]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.2483, -3.8760, -4.3688,  ..., -4.1136, -3.6216, -3.9633],\n",
      "         [-4.0405, -3.8853, -3.9882,  ..., -5.0330, -3.2825, -4.1651],\n",
      "         [-4.9946, -4.0507, -3.4440,  ..., -5.2219, -3.4028, -3.9215],\n",
      "         ...,\n",
      "         [-5.2029, -4.3069, -3.9889,  ..., -4.6453, -3.0903, -3.6623],\n",
      "         [-5.1876, -4.0765, -3.8288,  ..., -4.5812, -3.3176, -3.8131],\n",
      "         [-4.7787, -4.1291, -3.8805,  ..., -4.8502, -3.1327, -4.1875]],\n",
      "\n",
      "        [[-4.6748, -4.0858, -4.5908,  ..., -4.1095, -3.4011, -3.8192],\n",
      "         [-4.8195, -4.6157, -4.6929,  ..., -5.2947, -3.8806, -4.5418],\n",
      "         [-4.9735, -4.7117, -4.7598,  ..., -4.3288, -2.8955, -2.8069],\n",
      "         ...,\n",
      "         [-5.3639, -4.6694, -3.8426,  ..., -4.8135, -3.3847, -3.3871],\n",
      "         [-5.2994, -4.5513, -3.5985,  ..., -4.7761, -3.7272, -3.2381],\n",
      "         [-5.3185, -4.3255, -3.2428,  ..., -4.6247, -3.2889, -3.1064]],\n",
      "\n",
      "        [[-4.6110, -3.9170, -4.2549,  ..., -4.0632, -3.4316, -4.1626],\n",
      "         [-4.4645, -4.6505, -4.6874,  ..., -5.2576, -3.3974, -4.5609],\n",
      "         [-3.6114, -4.5102, -5.9191,  ..., -4.5465, -4.0062, -4.2837],\n",
      "         ...,\n",
      "         [-5.0236, -4.4569, -3.8703,  ..., -5.0894, -3.3758, -3.0999],\n",
      "         [-4.8858, -4.3138, -4.4240,  ..., -4.5882, -3.6408, -3.3778],\n",
      "         [-5.1341, -4.3462, -4.2207,  ..., -4.5289, -3.2727, -3.0674]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.6547332406044006\n",
      "Masked LM loss: 4.704166412353516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:7: 100%|| 1/1 [00:00<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 7, 'iter': 0, 'avg_loss': 5.3588995933532715, 'avg_acc': 35.714285714285715, 'loss': 5.3588995933532715}\n",
      "EP7_train, avg_loss= 5.3588995933532715 total_acc= 35.714285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:8:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6,  4,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3,  4, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[ 0,  0, 38,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 0, 60,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0])}\n",
      "Next sentence output: tensor([[-1.0011, -0.4580],\n",
      "        [-0.6380, -0.7515],\n",
      "        [-0.7660, -0.6252],\n",
      "        [-0.7145, -0.6723],\n",
      "        [-0.7402, -0.6482],\n",
      "        [-1.0985, -0.4055],\n",
      "        [-1.0396, -0.4363],\n",
      "        [-0.7415, -0.6470],\n",
      "        [-1.0092, -0.4534],\n",
      "        [-1.0033, -0.4568],\n",
      "        [-1.0064, -0.4550],\n",
      "        [-0.4689, -0.9827],\n",
      "        [-0.4335, -1.0447],\n",
      "        [-1.0702, -0.4200]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-4.5029, -3.8272, -4.2456,  ..., -3.7971, -3.5514, -3.9812],\n",
      "         [-4.8899, -4.6794, -4.4099,  ..., -4.7298, -3.9790, -4.1662],\n",
      "         [-4.5566, -4.5844, -4.5056,  ..., -5.1683, -3.4298, -3.5452],\n",
      "         ...,\n",
      "         [-4.9175, -4.4247, -4.2288,  ..., -4.9027, -3.4135, -3.1138],\n",
      "         [-4.7167, -4.3403, -4.4628,  ..., -5.3526, -3.5931, -3.3743],\n",
      "         [-5.1845, -4.2187, -4.4306,  ..., -4.9359, -3.9839, -3.3625]],\n",
      "\n",
      "        [[-4.2667, -3.8487, -4.3617,  ..., -3.9284, -3.5437, -4.1075],\n",
      "         [-4.6099, -4.4205, -4.0477,  ..., -5.2637, -4.5362, -4.2166],\n",
      "         [-4.7729, -4.7641, -3.9448,  ..., -4.9521, -3.4527, -3.6804],\n",
      "         ...,\n",
      "         [-5.4085, -4.3181, -4.4849,  ..., -5.2609, -3.7930, -3.7589],\n",
      "         [-5.3222, -4.2874, -3.9284,  ..., -4.9517, -3.2795, -3.8867],\n",
      "         [-5.4495, -4.4340, -4.0603,  ..., -4.8987, -3.5608, -3.0089]],\n",
      "\n",
      "        [[-4.4462, -3.9681, -4.5450,  ..., -3.9248, -3.4098, -3.9197],\n",
      "         [-4.6061, -4.9305, -4.7106,  ..., -4.6552, -3.5194, -4.3885],\n",
      "         [-4.3860, -3.6783, -3.6637,  ..., -4.5788, -3.2849, -4.3066],\n",
      "         ...,\n",
      "         [-4.9531, -4.4684, -3.9519,  ..., -4.6637, -3.0388, -3.4653],\n",
      "         [-5.2849, -4.4441, -4.3755,  ..., -5.0279, -3.3011, -3.5291],\n",
      "         [-5.1734, -3.9775, -3.9978,  ..., -4.6134, -2.9991, -3.5287]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.1508, -4.0901, -4.3865,  ..., -4.0575, -3.8142, -4.2897],\n",
      "         [-4.9764, -4.7122, -4.3525,  ..., -4.8459, -3.6854, -3.9842],\n",
      "         [-4.3830, -4.2658, -3.2499,  ..., -5.2034, -3.8608, -3.3594],\n",
      "         ...,\n",
      "         [-5.6735, -4.2517, -3.8629,  ..., -4.4845, -3.7240, -3.4943],\n",
      "         [-5.4259, -4.6307, -4.2939,  ..., -4.6293, -3.2683, -3.8847],\n",
      "         [-5.1835, -4.8574, -4.1828,  ..., -4.9868, -3.0230, -2.9785]],\n",
      "\n",
      "        [[-4.4807, -4.2749, -4.6766,  ..., -4.1376, -3.0758, -4.5687],\n",
      "         [-4.5132, -4.9251, -4.7307,  ..., -4.8804, -3.8732, -4.3271],\n",
      "         [-5.1148, -4.7537, -4.9111,  ..., -4.7500, -2.9644, -3.0628],\n",
      "         ...,\n",
      "         [-5.3810, -4.4057, -4.3603,  ..., -4.9800, -3.2646, -3.6313],\n",
      "         [-5.4368, -4.5191, -4.4791,  ..., -4.6915, -3.4520, -3.7012],\n",
      "         [-5.1413, -4.2675, -4.0250,  ..., -4.9351, -3.0862, -2.9771]],\n",
      "\n",
      "        [[-3.9126, -3.9564, -3.9360,  ..., -4.6397, -3.6366, -4.2105],\n",
      "         [-4.7124, -5.1848, -4.2185,  ..., -4.8908, -3.7633, -4.2150],\n",
      "         [-3.7964, -4.6860, -5.6269,  ..., -4.7922, -3.7327, -3.9353],\n",
      "         ...,\n",
      "         [-4.7085, -4.4836, -3.4989,  ..., -5.1060, -3.5970, -3.2236],\n",
      "         [-4.8436, -4.4156, -3.6417,  ..., -4.6399, -3.7619, -3.0115],\n",
      "         [-4.8642, -4.5325, -3.7079,  ..., -5.1865, -3.7168, -3.0443]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.6168834567070007\n",
      "Masked LM loss: 4.558293342590332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:8: 100%|| 1/1 [00:00<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 8, 'iter': 0, 'avg_loss': 5.175176620483398, 'avg_acc': 50.0, 'loss': 5.175176620483398}\n",
      "EP8_train, avg_loss= 5.175176620483398 total_acc= 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:9:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  6, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  4, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1])}\n",
      "Next sentence output: tensor([[-0.9367, -0.4974],\n",
      "        [-0.7442, -0.6446],\n",
      "        [-0.8506, -0.5572],\n",
      "        [-0.6113, -0.7823],\n",
      "        [-0.8888, -0.5296],\n",
      "        [-0.5897, -0.8085],\n",
      "        [-0.5818, -0.8185],\n",
      "        [-0.9439, -0.4929],\n",
      "        [-0.9632, -0.4807],\n",
      "        [-0.4707, -0.9797],\n",
      "        [-0.8592, -0.5508],\n",
      "        [-0.7060, -0.6804],\n",
      "        [-0.9587, -0.4835],\n",
      "        [-1.2809, -0.3255]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-4.0582, -3.9370, -5.0728,  ..., -3.9589, -3.8161, -3.8482],\n",
      "         [-4.7885, -4.7063, -4.4797,  ..., -4.6193, -3.7947, -4.1206],\n",
      "         [-4.2071, -4.9496, -4.7624,  ..., -4.9681, -3.9663, -4.4975],\n",
      "         ...,\n",
      "         [-5.6821, -4.3066, -4.4823,  ..., -4.5149, -3.7403, -3.1633],\n",
      "         [-5.6096, -4.3899, -4.2537,  ..., -4.6232, -3.4096, -2.8532],\n",
      "         [-5.1000, -4.3434, -4.7983,  ..., -4.7013, -3.2148, -2.9785]],\n",
      "\n",
      "        [[-4.5032, -3.9250, -4.5999,  ..., -4.4653, -3.5631, -4.2731],\n",
      "         [-4.4699, -3.9626, -3.9890,  ..., -5.4510, -4.7953, -4.5126],\n",
      "         [-4.5351, -4.3898, -4.6332,  ..., -5.1907, -3.7680, -3.9178],\n",
      "         ...,\n",
      "         [-5.1327, -4.4496, -4.0235,  ..., -4.7342, -3.5565, -3.1581],\n",
      "         [-5.2448, -4.5293, -3.9051,  ..., -4.8117, -3.1534, -3.2402],\n",
      "         [-5.3126, -4.3254, -4.2325,  ..., -5.0924, -3.2796, -3.1254]],\n",
      "\n",
      "        [[-4.1862, -3.6400, -4.6117,  ..., -3.9094, -3.3959, -3.8463],\n",
      "         [-4.6834, -4.8915, -4.5181,  ..., -5.1914, -3.9042, -4.3015],\n",
      "         [-4.9602, -3.8117, -3.7126,  ..., -4.5530, -3.7464, -4.0790],\n",
      "         ...,\n",
      "         [-5.2115, -4.3298, -4.1941,  ..., -4.6581, -3.3854, -3.2911],\n",
      "         [-5.4546, -4.3483, -4.4809,  ..., -4.7874, -3.6275, -3.2256],\n",
      "         [-5.4834, -4.3938, -4.1979,  ..., -4.3038, -3.6138, -3.1841]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.0673, -3.5291, -4.4895,  ..., -3.9010, -3.2042, -4.0867],\n",
      "         [-3.7962, -4.3525, -3.8362,  ..., -4.8747, -3.4374, -3.9525],\n",
      "         [-3.9126, -4.4727, -3.2754,  ..., -5.3667, -3.6283, -3.4001],\n",
      "         ...,\n",
      "         [-5.2502, -4.1496, -3.8014,  ..., -5.0716, -2.6683, -3.3219],\n",
      "         [-4.8477, -3.9682, -3.9437,  ..., -4.5731, -2.6570, -3.4795],\n",
      "         [-5.2660, -4.1008, -3.6923,  ..., -4.7102, -2.7097, -3.3660]],\n",
      "\n",
      "        [[-4.6190, -4.1104, -4.2758,  ..., -4.1803, -3.7137, -4.1721],\n",
      "         [-4.7661, -4.9680, -4.2553,  ..., -4.5278, -4.0519, -4.1948],\n",
      "         [-4.8320, -4.8691, -5.2099,  ..., -4.1165, -3.2487, -3.1674],\n",
      "         ...,\n",
      "         [-4.9799, -4.0392, -4.3246,  ..., -4.5456, -3.5475, -3.7356],\n",
      "         [-4.9493, -4.7927, -3.9748,  ..., -4.4477, -3.7688, -3.5695],\n",
      "         [-4.7785, -4.4651, -3.8349,  ..., -4.8088, -3.7919, -3.9750]],\n",
      "\n",
      "        [[-4.9052, -3.8543, -4.6026,  ..., -4.0231, -3.2771, -3.7140],\n",
      "         [-4.6507, -4.8865, -4.7010,  ..., -4.8655, -3.6302, -4.4000],\n",
      "         [-4.0549, -4.6000, -5.3952,  ..., -4.3525, -3.6891, -3.4001],\n",
      "         ...,\n",
      "         [-5.3911, -4.1975, -3.9797,  ..., -4.7712, -3.1301, -3.1826],\n",
      "         [-5.4300, -4.0851, -3.9265,  ..., -4.6770, -3.3405, -3.5066],\n",
      "         [-5.7509, -4.0534, -4.5451,  ..., -4.6223, -3.2464, -3.2386]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.6407034397125244\n",
      "Masked LM loss: 4.41757869720459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:9: 100%|| 1/1 [00:00<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 9, 'iter': 0, 'avg_loss': 5.058281898498535, 'avg_acc': 50.0, 'loss': 5.058281898498535}\n",
      "EP9_train, avg_loss= 5.058281898498535 total_acc= 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EP_train:10:   0%|| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_corpus_line:item: 0\n",
      "get_corpus_line:item: 1\n",
      "get_corpus_line:item: 2\n",
      "get_corpus_line:item: 3\n",
      "get_corpus_line:item: 4\n",
      "get_corpus_line:item: 5\n",
      "get_corpus_line:item: 6\n",
      "get_corpus_line:item: 7\n",
      "get_corpus_line:item: 8\n",
      "get_corpus_line:item: 9\n",
      "get_corpus_line:item: 10\n",
      "get_corpus_line:item: 11\n",
      "get_corpus_line:item: 12\n",
      "get_corpus_line:item: 13\n",
      "Batch 0 data: {'bert_input': tensor([[ 3,  4, 38,  ...,  0,  0,  0],\n",
      "        [ 3, 25, 50,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 49,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 3, 60, 31,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 56,  ...,  0,  0,  0],\n",
      "        [ 3,  6, 27,  ...,  0,  0,  0]]), 'bert_label': tensor([[0, 6, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'segment_label': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_next': tensor([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0])}\n",
      "Next sentence output: tensor([[-1.4721, -0.2607],\n",
      "        [-0.7928, -0.6025],\n",
      "        [-0.9369, -0.4973],\n",
      "        [-1.0510, -0.4301],\n",
      "        [-0.9341, -0.4992],\n",
      "        [-1.0421, -0.4350],\n",
      "        [-0.8563, -0.5529],\n",
      "        [-0.7446, -0.6442],\n",
      "        [-1.0213, -0.4465],\n",
      "        [-0.8939, -0.5260],\n",
      "        [-0.9366, -0.4975],\n",
      "        [-0.9251, -0.5050],\n",
      "        [-0.9117, -0.5139],\n",
      "        [-1.1542, -0.3788]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Masked LM output: tensor([[[-4.5090, -3.5066, -4.4209,  ..., -3.8722, -3.6830, -3.7452],\n",
      "         [-5.4724, -4.5872, -4.1532,  ..., -4.9844, -3.8137, -3.7244],\n",
      "         [-4.5771, -5.2366, -4.5286,  ..., -4.6905, -4.2207, -4.2943],\n",
      "         ...,\n",
      "         [-5.2026, -4.5633, -4.2331,  ..., -4.8294, -3.6266, -3.3092],\n",
      "         [-5.1791, -4.1649, -4.3923,  ..., -4.6496, -3.3872, -2.8698],\n",
      "         [-5.2543, -4.5072, -3.9830,  ..., -4.7466, -3.7641, -3.2360]],\n",
      "\n",
      "        [[-4.7692, -3.7302, -4.7348,  ..., -4.0822, -3.5405, -4.4436],\n",
      "         [-4.7343, -4.2966, -4.1999,  ..., -5.3246, -5.0390, -4.5382],\n",
      "         [-4.5695, -4.1852, -4.2816,  ..., -4.7751, -3.4046, -3.7378],\n",
      "         ...,\n",
      "         [-5.2912, -4.4217, -3.7508,  ..., -4.7332, -3.5030, -3.0248],\n",
      "         [-5.3859, -4.1133, -3.9415,  ..., -5.0578, -3.5425, -2.9031],\n",
      "         [-5.8587, -4.2722, -3.7645,  ..., -4.2840, -3.4157, -2.9051]],\n",
      "\n",
      "        [[-4.5825, -4.6446, -4.7351,  ..., -4.4153, -3.1119, -4.0987],\n",
      "         [-4.5085, -5.0654, -4.5958,  ..., -5.0114, -3.6615, -4.1266],\n",
      "         [-4.3898, -4.2934, -3.9545,  ..., -4.4098, -3.6196, -4.1108],\n",
      "         ...,\n",
      "         [-4.8580, -4.8167, -4.1449,  ..., -4.6650, -3.3894, -3.2839],\n",
      "         [-4.9673, -4.4910, -4.0769,  ..., -4.6201, -3.4614, -3.4630],\n",
      "         [-4.8936, -4.1828, -4.5602,  ..., -4.5652, -3.4582, -3.5057]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.2774, -3.8056, -4.6076,  ..., -3.9311, -3.4662, -3.9460],\n",
      "         [-3.7895, -4.2497, -4.1558,  ..., -4.6073, -3.6280, -3.9915],\n",
      "         [-4.4320, -4.5094, -3.0959,  ..., -5.0743, -3.6973, -3.4885],\n",
      "         ...,\n",
      "         [-5.4236, -4.4096, -3.9671,  ..., -4.7610, -3.1308, -3.5612],\n",
      "         [-5.0690, -4.4068, -3.7863,  ..., -4.6016, -2.8150, -3.2240],\n",
      "         [-5.1264, -4.6472, -4.0218,  ..., -4.8814, -2.8869, -3.2532]],\n",
      "\n",
      "        [[-4.6077, -4.1309, -4.7161,  ..., -4.1445, -3.3356, -3.7719],\n",
      "         [-5.0725, -5.2969, -4.7948,  ..., -4.5049, -3.6251, -4.5772],\n",
      "         [-5.3915, -4.0401, -4.6593,  ..., -4.3877, -3.3701, -2.9742],\n",
      "         ...,\n",
      "         [-5.5039, -4.1364, -4.1390,  ..., -4.5784, -3.5325, -3.7147],\n",
      "         [-5.1486, -4.2270, -4.1388,  ..., -4.5971, -3.1971, -3.6083],\n",
      "         [-5.3539, -4.1187, -4.2153,  ..., -5.0083, -3.0241, -3.2113]],\n",
      "\n",
      "        [[-4.0697, -3.9466, -4.4769,  ..., -4.0127, -3.5447, -3.8697],\n",
      "         [-4.5728, -4.6591, -4.6471,  ..., -4.6923, -3.8105, -4.1685],\n",
      "         [-3.7571, -4.3919, -5.8527,  ..., -4.6624, -4.0577, -4.1932],\n",
      "         ...,\n",
      "         [-5.3847, -4.5701, -3.8532,  ..., -4.2953, -3.4486, -3.4953],\n",
      "         [-4.8691, -4.0949, -3.9921,  ..., -4.7327, -3.4094, -3.3237],\n",
      "         [-5.0823, -4.5325, -3.9650,  ..., -4.5992, -3.2622, -3.3564]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "Next sentence loss: 0.5224645137786865\n",
      "Masked LM loss: 4.452347755432129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:10: 100%|| 1/1 [00:00<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 10, 'iter': 0, 'avg_loss': 4.9748125076293945, 'avg_acc': 42.857142857142854, 'loss': 4.9748125076293945}\n",
      "EP10_train, avg_loss= 4.9748125076293945 total_acc= 42.857142857142854\n",
      "EP:10 Model Saved on: /root/autodl-tmp/bert/checkpoints/bert_self_trained_ep10.pth\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Start\")\n",
    "for epoch in range(config.epochs):\n",
    "    trainer.train(epoch)\n",
    "    if epoch % config.log_freq == 0:\n",
    "        trainer.save(epoch, config.trained_path)\n",
    "    if test_data_loader is not None:\n",
    "        trainer.test(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
