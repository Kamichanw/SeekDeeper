{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules and Data\n",
    "BERT can be fine-tined on Stanford Sentiment Treebank-2(SST2) dataset for text classification task. More info about SST2 can be found [here](https://huggingface.co/datasets/stanfordnlp/sst2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, AdamW\n",
    "from data import load_data\n",
    "from modules.bert import BERTTextClassifier, ScheduledOptim\n",
    "import config \n",
    "# load sst-2\n",
    "tokenizer, train_dataloader, valid_dataloader = load_data(\n",
    "    name=\"sst2\",\n",
    "    loading_ratio=1,  # load 100% sst-2 data\n",
    "    num_proc=4,  # use 4 processes\n",
    "    splits=[\"train\", \"validation\"]  # load train and validation dataset\n",
    ")\n",
    "\n",
    "# check one batch shape\n",
    "for batch in train_dataloader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    print(input_ids.shape, attention_mask.shape, labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Model and Load from Pre-trained\n",
    "Build a BERT text classification model which inherits from the BERT class and add a binary linear classification layer at the end of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 108.89M\n",
      "BERTTextClassifier(\n",
      "  (embedding): BERTEmbedding(\n",
      "    (token): TokenEmbedding(30522, 768, padding_idx=0)\n",
      "    (position): PositionalEmbedding(512, 768)\n",
      "    (segment): SegmentEmbedding(2, 768, padding_idx=0)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_blocks): ModuleList(\n",
      "    (0): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (w_concat): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_1): LayerNorm()\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU()\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# load pretrained model\n",
    "model = BERTTextClassifier.from_pretrained(\n",
    "    config.pretrained_path,\n",
    "    num_frozen_layers=0,  # frozen layer. Here do not froze\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model\n",
    "\n",
    "### 3.1 Optimizer and Scheduler\n",
    "ScheduledOptim class is a wrapper for an optimizer that implements a learning rate scheduling strategy inspired by the Transformer paper (Attention Is All You Need). It adjusts the learning rate using a warm-up and decay mechanism to stabilize training. You can check modules/optim_schedule.py for deeper understanding.\n",
    "\n",
    "Original paper shows that they use Adam with learning rate of 1e-4, β1 = 0.9, β2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps, and linear decay of the learning rate in the pretraining.\n",
    "\n",
    "For fine-tuning, most model hyperparameters are the same as in pre-training, with the exception of the batch size, learning rate, and number of training epochs.\n",
    "The dropout probability was always kept at 0.1. The optimal hyperparameter values are task-specific.\n",
    "\n",
    "The learing rate schedule is shown as below, and initial lr is set as 2e-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from data import load_data\n",
    "from modules.bert import BERTTextClassifier, ScheduledOptim\n",
    "import config  \n",
    "\n",
    "# finetune\n",
    "pad_idx = 0\n",
    "def split_batch(batch):  \n",
    "    tokens, attention_mask, labels = batch \n",
    "    attention_mask = (tokens != pad_idx).long()  \n",
    "    input_ids = tokens.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "    return input_ids, attention_mask, labels\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "@torch.no_grad()\n",
    "def evaluate(model, clf_criterion, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        input_ids, attention_mask, gt = split_batch(batch)\n",
    "\n",
    "        clf_logits = model(input_ids)  # get classfier logits\n",
    "        \n",
    "        # print(f\"evaluate_clf_logits: {clf_logits}\")\n",
    "\n",
    "        loss = clf_criterion(clf_logits, gt)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(clf_logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(gt.cpu().numpy())\n",
    "        # get predictions and GT\n",
    "        # print(f\"Predictions: {preds.cpu().numpy()}\")\n",
    "        # print(f\"Ground Truth: {gt.cpu().numpy()}\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, scheduled_optimizer, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    scheduled_optimizer.zero_grad()  \n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Training Epoch {epoch}\"):\n",
    "        input_ids, attention_mask, labels = split_batch(batch)\n",
    "        \n",
    "        # single sentence, requiring no segment info\n",
    "        segment_info = torch.zeros(input_ids.size(0), input_ids.size(1))\n",
    "\n",
    "        input_ids = input_ids.to(device).long()  \n",
    "        attention_mask = attention_mask.to(device).long()  \n",
    "        segment_info = segment_info.to(device).long()  \n",
    "        \n",
    "        \n",
    "        loss, logits = model(input_ids, segment_info=segment_info, labels=labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        # grad-clip\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # update step and lr\n",
    "        scheduled_optimizer.step_and_update_lr() \n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e5a6aa6ff842a7b2f447d56a2eb6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/2105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a335da2bb1984f9795340e63ac3289d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Training Loss: 0.4088, Validation Loss: 0.2562, Accuracy: 89.56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edebbddc7c0d4adb8ffe4c872168b57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/2105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b8055f07a4430b9e792e3ca62ee40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Training Loss: 0.2057, Validation Loss: 0.2360, Accuracy: 91.51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a5db6a15b8452c97fab2b97641e74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/2105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17bec7378634f7daea9c9ff862c177b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Training Loss: 0.1456, Validation Loss: 0.2189, Accuracy: 91.97\n"
     ]
    }
   ],
   "source": [
    "def training_loop(model, train_dataloader, valid_dataloader, optimizer, criterion, num_epochs, warmup_steps):\n",
    "\n",
    "    scheduled_optimizer = ScheduledOptim(\n",
    "        optimizer=optimizer,\n",
    "        n_warmup_steps=warmup_steps,\n",
    "        total_steps=len(train_dataloader) * num_epochs\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train\n",
    "        avg_train_loss = train(\n",
    "            epoch + 1, model, scheduled_optimizer, train_dataloader\n",
    "        )\n",
    "\n",
    "        # valid\n",
    "        avg_valid_loss, avg_acc = evaluate(model, criterion, valid_dataloader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f},\",\n",
    "            f\"Validation Loss: {avg_valid_loss:.4f}, Accuracy: {avg_acc * 100:.2f}\",\n",
    "        )\n",
    "\n",
    "        # save checkpoint\n",
    "        checkpoint_path = config.checkpoint_dir / f\"bert_clf_{epoch + 1}.pth\"\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": scheduled_optimizer._optimizer.state_dict(),\n",
    "                \"scheduler\": scheduled_optimizer.n_current_steps,\n",
    "            },\n",
    "            checkpoint_path,\n",
    "        )\n",
    "\n",
    "\n",
    "# AdamW optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=config.FinetuningConfig.lr, weight_decay=config.FinetuningConfig.weight_decay)\n",
    "\n",
    "training_loop(model, train_dataloader, valid_dataloader, optimizer, torch.nn.CrossEntropyLoss(), config.FinetuningConfig.n_epoch, config.FinetuningConfig.warmup_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
