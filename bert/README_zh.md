[ğŸ“–English ReadMe](./README.md)

## Introduction

åœ¨è¿™ä¸ªBERT-baseçš„å®ç°ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå¦‚ä½•åœ¨ç¤ºä¾‹æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼›å°±å¾®è°ƒéƒ¨åˆ†ï¼Œå°†å±•ç¤ºä»huggingfaceä¸ŠåŠ è½½googleçš„é¢„è®­ç»ƒæ¨¡å‹æƒé‡åˆ°è‡ªå·±å®ç°çš„æ¨¡å‹ä¸­ï¼Œå¹¶åœ¨SST-2æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒè®­ç»ƒï¼›æœ€åç»™å®šæ–‡æœ¬ç”±æ¨¡å‹è¿›è¡Œå¥å­æƒ…æ„Ÿåˆ†ç±»çš„æ¨ç†ã€‚



## Model details

### Key differences with GPT

äº‹å®ä¸Šï¼ŒBERTçš„è®¸å¤šè®¾è®¡éƒ½æ˜¯æœ‰æ„ä¸ºä¹‹çš„ï¼Œä½¿å…¶å°½å¯èƒ½æ¥è¿‘åŸæœ¬çš„GPTï¼Œä»è€Œä½¿ä¸¤ç§æ–¹æ³•èƒ½å¤Ÿæœ€å°ç¨‹åº¦åœ°è¿›è¡Œæ¯”è¾ƒã€‚BERT_baseçš„æ¨¡å‹å¤§å°ã€æ³¨æ„åŠ›å¤´æ•°ã€å±‚æ•°éƒ½å’ŒGPTç›¸åŒï¼›åŒæ ·çš„ï¼ŒBERTçš„æ¿€æ´»å‡½æ•°ä» ReLU æ›¿æ¢ä¸ºäº† GeLUï¼Œä¹Ÿä½¿ç”¨ learnable positional embedding è€Œä¸æ˜¯æ­£ä½™å¼¦ä½ç½®ç¼–ç ã€‚

ä½†å…·ä½“æ¥è¯´è¿˜æ˜¯æœ‰ä¸€å®šä¸åŒï¼š

1.BERTä½¿ç”¨çš„æ˜¯åŸå§‹Transformer çš„Encoderï¼Œæ— éœ€Decoderã€‚ä¹Ÿå°±æ˜¯BERTæ˜¯å¯ä»¥çœ‹åˆ°å‰åæ–‡ä¿¡æ¯çš„ï¼Œè€ŒGPTæ˜¯åªèƒ½ä»å†å²ä¸Šä¸‹æ–‡ä¸­è·å–ä¿¡æ¯ã€‚

2.BERTé¢„è®­ç»ƒçš„ä»»åŠ¡åŒ…æ‹¬MLMå’ŒNSPï¼Œè€ŒGPTé¢„è®­ç»ƒçš„ä»»åŠ¡ä¸»è¦æ˜¯é¢„æµ‹ç»™å®šæ–‡æœ¬åºåˆ—ä¸­ä¸‹ä¸€ä¸ªè¯ã€‚

3.BERTåœ¨é¢„è®­ç»ƒçš„æ—¶å€™å°±å­¦ä¹ åˆ°[SEP]ã€[CLS]å’Œå¥å­A/Bçš„åµŒå…¥ï¼›GPTä½¿ç”¨[SEP]å’Œ[CLS]ï¼Œä½†å®ƒä»¬åªåœ¨å¾®è°ƒçš„æ—¶å€™å¼•å…¥ã€‚

4.è®­ç»ƒç»†èŠ‚ï¼ˆåŒ…æ‹¬æ•°æ®é›†ã€å­¦ä¹ ç‡çš„è®¾ç½®ï¼‰



### MLM å’Œ NSP

#### Masked Language Modelï¼ˆMLMï¼‰æ©ç è¯­è¨€æ¨¡å‹

> Original Paper : 3.3.1 Task #1: Masked LM 

```
Input Sequence  : The man went to [MASK] store with [MASK] dog
Target Sequence :                  the                his
```

##### Rules:

è¾“å…¥çš„éšæœº15%åŸºäºä»¥ä¸‹è§„åˆ™å°†ä¼šè¢«æ”¹å˜ä¸ºå…¶ä»–ä¸œè¥¿ï¼š

1. éšæœº80%çš„tokenâ€”> `[MASK]`
2. éšæœº10%çš„tokenâ€”>`[RANDOM]` tokenï¼ˆå…¶ä»–çš„ä¸ä¸€æ ·çš„è¯ï¼‰
3. éšæœº10%çš„tokenä¸å˜ï¼Œä½†ä»éœ€è¦è¢«é¢„æµ‹ã€‚

#### Next Sentence Prediction(NSP) ä¸‹ä¸€å¥å­é¢„æµ‹

> Original Paper : 3.3.2 Task #2: Next Sentence Prediction

```
Input : [CLS] the man went to the store [SEP] he bought a gallon of milk [SEP]
Label : Is Next

Input = [CLS] the man heading to the store [SEP] penguin [MASK] are flight ##less birds [SEP]
Label = NotNext
```

ç”¨äºç†è§£ä¸¤ä¸ªæ–‡æœ¬å¥å­ä¹‹é—´çš„å…³ç³»ï¼Œè¿™æ˜¯å¤§è¯­è¨€æ¨¡å‹ä¸èƒ½ç›´æ¥å­¦ä¹ åˆ°çš„ã€‚

##### Rules:

1. éšæœº50%çš„å¥å­ä¸‹ä¸€å¥æ˜¯è¿ç»­çš„
2. éšæœº50%çš„å¥å­ä¸‹ä¸€å¥æ˜¯ä¸è¿ç»­çš„



## [Pre-training](./pretrain.ipynb)

è®ºæ–‡ä¸­çš„BERTæ˜¯åœ¨BookCorpuså’ŒWikipediaæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ã€‚åœ¨æœ¬å®ç°ä¸­ï¼Œæˆ‘åŠ è½½äº†BookCorpuså’ŒWikipediaçš„ç¬¬ä¸€ä¸ªparquet æ–‡ä»¶å¹¶å°†å…¶åˆå¹¶æˆäº†ä¸€ä¸ª**corpus.txt**æ–‡ä»¶ã€‚

è¯­æ–™çš„æ ¼å¼æ˜¯åŒä¸€è¡Œä¸¤ä¸ªå¥å­ï¼Œå¥å­é—´éšå¼åœ°ç”¨ \t åˆ†éš”ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š

```bash
Welcome to the \t the jungle\n
I can stay \t here all night\n
```

BERTä½¿ç”¨WordPieceæ–¹æ³•è¿›è¡Œtokenizeï¼Œé¦–å…ˆå¯¹æ•°æ®é›†tokenizeç”Ÿæˆè¯æ±‡è¡¨ï¼›æ¥ç€å°±æ˜¯è®¾å®šæ¨¡å‹å‚æ•°æ„å»ºBERTæ¨¡å‹ï¼Œè¿›è¡Œè®­ç»ƒï¼Œå…·ä½“æµç¨‹å‚è€ƒpretrain.ipynbã€‚é¢„è®­ç»ƒä½¿ç”¨çš„ä¼˜åŒ–å™¨æ˜¯Adamï¼Œå…¶ä¸­lrä¸º1e-4ï¼ŒÎ²1å’ŒÎ²2åˆ†åˆ«ä¸º0.9å’Œ0.999ï¼ŒL2 æƒé‡è¡°å‡ä¸º0.01ã€‚

## [Fine-tuning](./finetune.ipynb) 

åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒè¿‡åï¼ŒBERTæ¨¡å‹å¯ä»¥è®¤ä¸ºæœ‰å­¦ä¹ åˆ°ä¸€å®šçš„è¯­è¨€èƒ½åŠ›ã€‚æœ¬å®ç°é€‰æ‹©åœ¨SST-2æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œç”¨äºæ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ã€‚è¿™é¦–å…ˆéœ€è¦åœ¨åŸæœ¬çš„BERTæ¨¡å‹æ¶æ„çš„æœ€ååŠ ä¸€ä¸ªçº¿æ€§äºŒåˆ†ç±»å±‚ã€‚æˆ‘ä»huggingfaceä¸ŠåŠ è½½[bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased)çš„æƒé‡åˆ°è‡ªå·±çš„æ¨¡å‹ä¸­ï¼Œç„¶ååœ¨æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå¾®è°ƒã€‚ä½¿ç”¨çš„ä¼˜åŒ–å™¨æ˜¯AdamWï¼Œå…¶ä¸­lrä¸º4e-5ï¼ŒÎ²1å’ŒÎ²2åˆ†åˆ«ä¸ºé»˜è®¤çš„0.9å’Œ0.999ï¼Œæƒé‡è¡°å‡åŒæ ·ä¸º0.01ã€‚

## [Inferencing](./inference.ipynb) 

åŠ è½½æŒ‡å®šæ¨¡å‹ï¼Œå¯¹ç»™å®šæ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡çš„æ¨ç†ã€‚



## Reference

æœ¬å®ç°å‚è€ƒå¦‚ä¸‹ï¼š

1.[google-research/bert: TensorFlow code and pre-trained models for BERT](https://github.com/google-research/bert)

2.[codertimo/BERT-pytorch: Google AI 2018 BERT pytorch implementation](https://github.com/codertimo/BERT-pytorch)

3.[transformers/src/transformers/models/bert at 0de15c988b0d27758ce360adb2627e9ea99e91b3 Â· huggingface/transformers](https://github.com/huggingface/transformers/tree/0de15c988b0d27758ce360adb2627e9ea99e91b3/src/transformers/models/bert)