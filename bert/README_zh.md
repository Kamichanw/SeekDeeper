[ğŸ“– English ReadMe](./README.md)

## Introduction
åœ¨è¿™ä¸ª BERT å®ç°ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•åœ¨ [BookCorpus](https://huggingface.co/datasets/bookcorpus/bookcorpus) å’Œ [Wikipedia](https://huggingface.co/datasets/wikimedia/wikipedia) æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååŠ è½½ Hugging Face æä¾›çš„å®˜æ–¹é¢„è®­ç»ƒæƒé‡ï¼Œå¹¶åœ¨ [Stanford Sentiment Treebank (SST-2)](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf) æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¤ç°è®ºæ–‡ä¸­çš„æ•ˆæœã€‚

## Model Details

### Key Differences with Vanilla Transformer and GPT
1. **BERT çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šBERT é€šè¿‡åŒå‘è‡ªæ³¨æ„åŠ›æ¥æ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè€Œ GPT ä»…ä½¿ç”¨å› æœè‡ªæ³¨æ„åŠ›ã€‚æ¢è€Œè¨€ä¹‹ï¼ŒBERT çš„ mask åªç”¨äº paddingã€‚
2. **BERT çš„è¯åµŒå…¥**ï¼šBERT ä½¿ç”¨çš„æ˜¯ WordPiece çš„åˆ†è¯æ–¹å¼ï¼Œè¿™ä¸ GPT çš„ Byte-Pair Encodingï¼ˆBPEï¼‰ä¸åŒã€‚
3. **è®­ç»ƒç›®æ ‡**ï¼šBERT ä½¿ç”¨äº† Masked Language Modeling (MLM) å’Œ Next Sentence Prediction (NSP) ä½œä¸ºé¢„è®­ç»ƒç›®æ ‡ï¼Œè€Œ GPT å’ŒåŸå§‹ Transformer é‡‡ç”¨çš„æ˜¯ä¼ ç»Ÿçš„è¯­è¨€å»ºæ¨¡ç›®æ ‡ã€‚

### Pre-training Tasks

#### Masked Language Modeling (MLM)

> Original Paper : 3.3.1 Task #1: Masked LM 

```
Input Sequence  : The man went to [MASK] store with [MASK] dog
Target Sequence :                  the                his
```

##### Rules:

è¾“å…¥çš„éšæœº15%åŸºäºä»¥ä¸‹è§„åˆ™å°†ä¼šè¢«æ”¹å˜ä¸ºå…¶ä»–ä¸œè¥¿ï¼š

1. éšæœº80%çš„token -> `[MASK]`
2. éšæœº10%çš„token -> `[RANDOM]` tokenï¼ˆå…¶ä»–çš„ä¸ä¸€æ ·çš„è¯ï¼‰
3. éšæœº10%çš„tokenä¸å˜ï¼Œä½†ä»éœ€è¦è¢«é¢„æµ‹ã€‚

#### Next Sentence Prediction (NSP)

> Original Paper : 3.3.2 Task #2: Next Sentence Prediction

```
Input : [CLS] the man went to the store [SEP] he bought a gallon of milk [SEP]
Label : Is Next

Input = [CLS] the man heading to the store [SEP] penguin [MASK] are flight ##less birds [SEP]
Label = NotNext
```

ç”¨äºç†è§£ä¸¤ä¸ªæ–‡æœ¬å¥å­ä¹‹é—´çš„å…³ç³»ï¼Œè¿™æ˜¯å¤§è¯­è¨€æ¨¡å‹ä¸èƒ½ç›´æ¥å­¦ä¹ åˆ°çš„ã€‚

##### Rules:

1. éšæœº50%çš„å¥å­ä¸‹ä¸€å¥æ˜¯è¿ç»­çš„
2. éšæœº50%çš„å¥å­ä¸‹ä¸€å¥æ˜¯ä¸è¿ç»­çš„

## [Pre-training](./pretrain.ipynb)
æ ¹æ® [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) è®ºæ–‡ä¸­çš„è®¾ç½®ï¼ŒBERT åœ¨ BooksCorpus æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿ç”¨ AdamW ä¼˜åŒ–å™¨ï¼ˆ$w = 0.01, \text{max-lr} = 2.4 \times 10^{-4}$ï¼‰ã€‚è®­ç»ƒæ—¶ä½¿ç”¨äº†çº¿æ€§å¢é•¿çš„å­¦ä¹ ç‡ç­–ç•¥ï¼Œå­¦ä¹ ç‡åœ¨å‰ 2000 æ­¥çº¿æ€§å¢åŠ ï¼Œä¹‹åä½¿ç”¨ä½™å¼¦é€€ç«ç­–ç•¥è°ƒæ•´å­¦ä¹ ç‡ã€‚

## [Fine-tuning](./finetune.ipynb)
é¢„è®­ç»ƒå®Œæˆåï¼ŒBERT å·²ç»è·å¾—äº†è¾ƒå¼ºçš„è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥é€šè¿‡å¾®è°ƒæ¥é€‚åº”æ–°çš„ä»»åŠ¡ã€‚åœ¨å¾®è°ƒæ—¶ï¼Œåªéœ€è¦å¯¹æ¨¡å‹ç»“æ„åšè½»å¾®è°ƒæ•´ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­æ·»åŠ é€‚å½“çš„åˆ†ç±»å¤´ã€‚

åœ¨å¾®è°ƒæ—¶ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼ˆå¦‚ $3 \times 10^{-5}$ï¼‰ï¼Œå¹¶ä½¿ç”¨åˆé€‚çš„ batch sizeã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œè®­ç»ƒ 3-4 ä¸ª epoch è¶³çŸ£ã€‚

## Appendix
### How to Download Pretrained BERT?
åœ¨å‘½ä»¤è¡Œè¿è¡Œä»¥ä¸‹æŒ‡ä»¤ï¼š
```bash
pip install -U huggingface-cli
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download bert-base-uncased --local-dir path/to/pretrained_dir
```
