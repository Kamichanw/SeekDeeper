[ğŸ“– English ReadMe](./README.md)

## Introduction
åœ¨è¿™ä¸ª BERT å®ç°ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•åœ¨ [BookCorpus](https://huggingface.co/datasets/bookcorpus/bookcorpus) å’Œ [Wikipedia](https://huggingface.co/datasets/wikimedia/wikipedia) æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååŠ è½½ Hugging Face æä¾›çš„å®˜æ–¹é¢„è®­ç»ƒæƒé‡ï¼Œå¹¶åœ¨ [Stanford Sentiment Treebank (SST-2)](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf) æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¤ç°è®ºæ–‡ä¸­çš„æ•ˆæœã€‚

## Model Details

### Key Differences with Vanilla Transformer and GPT

BERTçš„è®¸å¤šè®¾è®¡éƒ½æ˜¯æœ‰æ„ä¸ºä¹‹çš„ï¼Œä½¿å…¶å°½å¯èƒ½æ¥è¿‘åŸæœ¬çš„GPTï¼Œä»è€Œä½¿ä¸¤ç§æ–¹æ³•èƒ½å¤Ÿæœ€å°ç¨‹åº¦åœ°è¿›è¡Œæ¯”è¾ƒã€‚BERT_baseçš„æ¨¡å‹å¤§å°ã€æ³¨æ„åŠ›å¤´æ•°ã€å±‚æ•°éƒ½å’ŒGPTç›¸åŒï¼›åŒæ ·çš„ï¼ŒBERTçš„æ¿€æ´»å‡½æ•°ä» ReLU æ›¿æ¢ä¸ºäº† GeLUï¼Œä¹Ÿä½¿ç”¨ learnable positional embedding è€Œä¸æ˜¯æ­£ä½™å¼¦ä½ç½®ç¼–ç ã€‚ä½†å…·ä½“æ¥è¯´è¿˜æ˜¯æœ‰ä¸åŒä¹‹å¤„ï¼š

1. **BERT çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šBERT é€šè¿‡åŒå‘è‡ªæ³¨æ„åŠ›æ¥æ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè€Œ GPT ä»…ä½¿ç”¨å› æœè‡ªæ³¨æ„åŠ›ã€‚æ¢è€Œè¨€ä¹‹ï¼ŒBERT çš„ mask åªç”¨äº paddingã€‚
2. **BERT çš„è¯åµŒå…¥**ï¼šBERT ä½¿ç”¨çš„æ˜¯ WordPiece çš„åˆ†è¯æ–¹å¼ï¼Œè¿™ä¸ GPT çš„ Byte-Pair Encodingï¼ˆBPEï¼‰ä¸åŒã€‚
3. **è®­ç»ƒç›®æ ‡**ï¼šBERT ä½¿ç”¨äº† Masked Language Modeling (MLM) å’Œ Next Sentence Prediction (NSP) ä½œä¸ºé¢„è®­ç»ƒç›®æ ‡ï¼Œè€Œ GPT å’ŒåŸå§‹ Transformer é‡‡ç”¨çš„æ˜¯ä¼ ç»Ÿçš„è¯­è¨€å»ºæ¨¡ç›®æ ‡ã€‚
4. **tokenå¤„ç†**ï¼šBERTåœ¨é¢„è®­ç»ƒçš„æ—¶å€™å°±å­¦ä¹ åˆ°[SEP]ã€[CLS]å’Œå¥å­A/Bçš„åµŒå…¥ï¼›GPTä½¿ç”¨[SEP]å’Œ[CLS]ï¼Œä½†å®ƒä»¬åªåœ¨å¾®è°ƒçš„æ—¶å€™å¼•å…¥ã€‚

### Pre-training Tasks

#### Masked Language Modeling (MLM)

> Original Paper : 3.3.1 Task #1: Masked LM 

```
Input Sequence  : The man went to [MASK] store with [MASK] dog
Target Sequence :                  the                his
```

##### Rules:

è¾“å…¥çš„éšæœº15%åŸºäºä»¥ä¸‹è§„åˆ™å°†ä¼šè¢«æ”¹å˜ä¸ºå…¶ä»–ä¸œè¥¿ï¼š

1. éšæœº80%çš„token -> `[MASK]`
2. éšæœº10%çš„token -> `[RANDOM]` tokenï¼ˆå…¶ä»–çš„ä¸ä¸€æ ·çš„è¯ï¼‰
3. éšæœº10%çš„tokenä¸å˜ï¼Œä½†ä»éœ€è¦è¢«é¢„æµ‹ã€‚

#### Next Sentence Prediction (NSP)

> Original Paper : 3.3.2 Task #2: Next Sentence Prediction

```
Input : [CLS] the man went to the store [SEP] he bought a gallon of milk [SEP]
Label : Is Next

Input = [CLS] the man heading to the store [SEP] penguin [MASK] are flight ##less birds [SEP]
Label = NotNext
```

ç”¨äºç†è§£ä¸¤ä¸ªæ–‡æœ¬å¥å­ä¹‹é—´çš„å…³ç³»ï¼Œè¿™æ˜¯å¤§è¯­è¨€æ¨¡å‹ä¸èƒ½ç›´æ¥å­¦ä¹ åˆ°çš„ã€‚

##### Rules:

1. éšæœº50%çš„å¥å­ä¸‹ä¸€å¥æ˜¯è¿ç»­çš„
2. éšæœº50%çš„å¥å­ä¸‹ä¸€å¥æ˜¯ä¸è¿ç»­çš„

## [Pre-training](./pretrain.ipynb)
æ ¹æ® [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) è®ºæ–‡ä¸­çš„è®¾ç½®ï¼ŒBERT åœ¨ BooksCorpus å’ŒWikipedia æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿ç”¨ AdamW ä¼˜åŒ–å™¨ï¼ˆ$w = 0.01, \text{max-lr} = 1 \times 10^{-4}$ï¼‰ã€‚è®­ç»ƒæ—¶ä½¿ç”¨äº†çº¿æ€§å¢é•¿çš„å­¦ä¹ ç‡ç­–ç•¥ï¼Œå­¦ä¹ ç‡åœ¨å‰ 10000 æ­¥çº¿æ€§å¢åŠ ï¼Œä¹‹åä½¿ç”¨çº¿æ€§è¡°å‡ç­–ç•¥è°ƒæ•´å­¦ä¹ ç‡ã€‚

## [Fine-tuning](./finetune.ipynb)
é¢„è®­ç»ƒå®Œæˆåï¼ŒBERT å·²ç»è·å¾—äº†è¾ƒå¼ºçš„è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥é€šè¿‡å¾®è°ƒæ¥é€‚åº”æ–°çš„ä»»åŠ¡ã€‚åœ¨å¾®è°ƒæ—¶ï¼Œåªéœ€è¦å¯¹æ¨¡å‹ç»“æ„åšè½»å¾®è°ƒæ•´ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­æ·»åŠ é€‚å½“çš„åˆ†ç±»å¤´ã€‚

ç”±äºæœ€ä½³è¶…å‚æ•°å€¼å–å†³äºå…·ä½“ä»»åŠ¡ï¼ŒåŸè®ºæ–‡é’ˆå¯¹ä¸åŒä»»åŠ¡çš„å¾®è°ƒç»™å‡ºäº†è¶…å‚æ•°èŒƒå›´ï¼š

- **Batch size**ï¼š16ï¼Œ32
- **Learning rate **ï¼š5e-5, 3e-5, 2e-5
- **Number of epochs**: 2, 3, 4

åœ¨æœ¬å®ç°ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† AdamW ä¼˜åŒ–å™¨ï¼ˆ$w = 0.01, \text{max-lr} = 4 \times 10^{-5}$ï¼‰ï¼Œå¹¶é€‰ç”¨ batch size ä¸º 32 ï¼Œepoch æ•°ä¸º 3 ã€‚å¾®è°ƒåŒæ ·ä½¿ç”¨äº†çº¿æ€§å¢é•¿çš„å­¦ä¹ ç‡ç­–ç•¥ï¼Œå­¦ä¹ ç‡åœ¨å‰ 10000 æ­¥çº¿æ€§å¢åŠ ï¼Œä¹‹åä½¿ç”¨çº¿æ€§è¡°å‡ç­–ç•¥è°ƒæ•´å­¦ä¹ ç‡ã€‚

## Appendix
### How to Download Pretrained BERT?
åœ¨å‘½ä»¤è¡Œè¿è¡Œä»¥ä¸‹æŒ‡ä»¤ï¼š
```bash
pip install -U huggingface-cli
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download bert-base-uncased --local-dir path/to/pretrained_dir
```
